<!DOCTYPE html>
<html lang="ru" data-vue-meta="%7B%22lang%22:%7B%22ssr%22:%22ru%22%7D%7D">
<head >
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0,viewport-fit=cover">
  <title>Face Anti-Spoofing или технологично узнаём обманщика из тысячи по лицу / Хабр</title>
  <style>
    /* cyrillic-ext */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveSxf6TF0.woff2) format('woff2');
      unicode-range: U+0460-052F, U+1C80-1C88, U+20B4, U+2DE0-2DFF, U+A640-A69F, U+FE2E-FE2F;
    }

    /* cyrillic */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveQhf6TF0.woff2) format('woff2');
      unicode-range: U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;
    }

    /* latin-ext */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveSBf6TF0.woff2) format('woff2');
      unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
    }

    /* latin */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveRhf6.woff2) format('woff2');
      unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
    }
  </style>
  <link rel="preload" href="https://assets.habr.com/habr-web/css/chunk-vendors.e7843cc0.css" as="style"><link rel="preload" href="https://assets.habr.com/habr-web/js/chunk-vendors.c2c3fc9a.js" as="script"><link rel="preload" href="https://assets.habr.com/habr-web/css/app.02ee25a4.css" as="style"><link rel="preload" href="https://assets.habr.com/habr-web/js/app.c0af73e7.js" as="script">
  <link rel="stylesheet" href="https://assets.habr.com/habr-web/css/chunk-vendors.e7843cc0.css"><link rel="stylesheet" href="https://assets.habr.com/habr-web/css/app.02ee25a4.css">
  <script>window.i18nFetch = new Promise((res, rej) => {
          const xhr = new XMLHttpRequest();
          xhr.open('GET', '/js/i18n/ru-compiled.85eb77f0b17c8235e7b64b9f81ea5ec2.json');
          xhr.responseType = 'json';
          xhr.onload = function(e) {
            if (this.status === 200) {
              res({ru: xhr.response});
            } else {
              rej(e);
            }
          };
          xhr.send();
        });</script>
  
  <script data-vue-meta="ssr" src="/js/ads.js" onload="window['zhY4i4nJ9K'] = true" data-vmid="checkad"></script><script data-vue-meta="ssr" type="application/ld+json" data-vmid="ldjson-schema">{"@context":"http:\/\/schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/habr.com\/ru\/company\/ods\/blog\/452894\/"},"headline":"Face Anti-Spoofing или технологично узнаём обманщика из тысячи по лицу","datePublished":"2019-05-23T15:22:31+03:00","dateModified":"2019-10-20T23:46:21+03:00","author":{"@type":"Person","name":"evgeniimakarov"},"publisher":{"@type":"Organization","name":"Habr","logo":{"@type":"ImageObject","url":"https:\/\/habrastorage.org\/webt\/a_\/lk\/9m\/a_lk9mjkccjox-zccjrpfolmkmq.png"}},"description":"Биометрическая идентификация человека &ndash; это одна из самых старых идей для распознавания людей, которую вообще попытались технически осуществить. Пароли можно укр...","url":"https:\/\/habr.com\/ru\/company\/ods\/blog\/452894\/#post-content-body","about":["c_ods","h_infosecurity","h_machine_learning","h_popular_science","h_artificial_intelligence","f_develop","f_popsci"],"image":["https:\/\/habrastorage.org\/webt\/h4\/wd\/zr\/h4wdzrqvlnvfy8da0k3cn5589hg.jpeg","https:\/\/habrastorage.org\/webt\/pl\/vz\/67\/plvz67jfgazkigzqsdhkko62ego.png","https:\/\/habrastorage.org\/webt\/xl\/kq\/pr\/xlkqprysss0ce2arbqm0sw_xbpo.png","https:\/\/habrastorage.org\/webt\/dt\/jx\/lp\/dtjxlptnxcphiicevd6lbhlzzpa.png","https:\/\/habrastorage.org\/webt\/q2\/tr\/nl\/q2trnl3zhhuftbggl3vx-sfqoyw.png","https:\/\/habrastorage.org\/webt\/f9\/g7\/ds\/f9g7dsc81eiffu9srctasf2udcm.png","https:\/\/habrastorage.org\/webt\/de\/dd\/vj\/deddvj6xvnwbv6dpbryxphjwaa8.png","https:\/\/habrastorage.org\/webt\/1r\/n8\/-e\/1rn8-elrfhm1q0ud9q4jbaukidq.png","https:\/\/habrastorage.org\/webt\/4s\/fb\/vv\/4sfbvvforopuonls_ochr8dpzjg.png","https:\/\/habrastorage.org\/webt\/qu\/x_\/vv\/qux_vvmfk7ikww7ojknhq_mpbhu.png","https:\/\/habrastorage.org\/webt\/7j\/vy\/i7\/7jvyi7inhsl5g2ulp05v4luido8.png","https:\/\/habrastorage.org\/webt\/98\/m2\/ls\/98m2ls-svv7gxfaagripfrhzymo.png","https:\/\/habrastorage.org\/webt\/ih\/9e\/vo\/ih9evob9a5hmtjekxhx1iwsgxm8.png","https:\/\/habrastorage.org\/webt\/-i\/1w\/w2\/-i1ww2rsu0kqmxglhh-on8lnr5g.png","https:\/\/habrastorage.org\/webt\/7p\/t-\/yk\/7pt-ykbzzuyb7bzpnd9qpjodfog.png","https:\/\/habrastorage.org\/webt\/a7\/bl\/df\/a7bldfybkgzeeptdtomphvq1kwk.png","https:\/\/habrastorage.org\/webt\/ek\/8b\/rc\/ek8brc52akg2y3zfy6ejmbr-taq.png","https:\/\/habrastorage.org\/webt\/aw\/_k\/s_\/aw_ks_6-xp40fyjz97ue9mwzp8q.png","https:\/\/habrastorage.org\/webt\/bb\/7v\/73\/bb7v73awyzwhxeyyvtejgm3skh4.png","https:\/\/habrastorage.org\/webt\/jw\/b5\/-n\/jwb5-naohhjhek2tkmakbssjyik.png","https:\/\/habrastorage.org\/webt\/vf\/lz\/ol\/vflzoljiplnkj_vgm-zth9a0sdc.png","https:\/\/habrastorage.org\/webt\/ur\/yf\/t8\/uryft8082ylvw-1kbyvcxucjcem.png","https:\/\/habrastorage.org\/webt\/db\/le\/44\/dble44fc8acyqdhzead3ndbkg3a.png","https:\/\/habrastorage.org\/webt\/sy\/u7\/vu\/syu7vucfvb5inhajqoilb6rqxu8.png","https:\/\/habrastorage.org\/webt\/qd\/bu\/fn\/qdbufnt8yc9pghj4egkxqaku_3g.png","https:\/\/habrastorage.org\/webt\/x9\/lq\/kz\/x9lqkzuzmgz7gz5ejx1rsiom7wq.png","https:\/\/habrastorage.org\/webt\/c-\/ts\/w8\/c-tsw8vywrvsv3l58ow-axifkay.png"]}</script>
  <script src="//www.googletagservices.com/tag/js/gpt.js" async></script>
  <style>.grecaptcha-badge{visibility: hidden;}</style>
  <meta name="habr-version" content="2.49.0">
  
  <meta data-vue-meta="ssr" property="fb:app_id" content="444736788986613"><meta data-vue-meta="ssr" property="fb:pages" content="472597926099084"><meta data-vue-meta="ssr" name="twitter:card" content="summary_large_image"><meta data-vue-meta="ssr" name="twitter:site" content="@habr_eng"><meta data-vue-meta="ssr" property="og:title" content="Face Anti-Spoofing или технологично узнаём обманщика из тысячи по лицу" data-vmid="og:title"><meta data-vue-meta="ssr" name="twitter:title" content="Face Anti-Spoofing или технологично узнаём обманщика из тысячи по лицу" data-vmid="twitter:title"><meta data-vue-meta="ssr" name="aiturec:title" content="Face Anti-Spoofing или технологично узнаём обманщика из тысячи по лицу" data-vmid="aiturec:title"><meta data-vue-meta="ssr" name="description" content="Биометрическая идентификация человека – это одна из самых старых идей для распознавания людей, которую вообще попытались технически осуществить. Пароли можно украсть, подсмотреть, забыть, ключи –..." data-vmid="description"><meta data-vue-meta="ssr" itemprop="description" content="Биометрическая идентификация человека – это одна из самых старых идей для распознавания людей, которую вообще попытались технически осуществить. Пароли можно украсть, подсмотреть, забыть, ключи –..." data-vmid="description:itemprop"><meta data-vue-meta="ssr" property="og:description" content="Биометрическая идентификация человека – это одна из самых старых идей для распознавания людей, которую вообще попытались технически осуществить. Пароли можно украсть, подсмотреть, забыть, ключи –..." data-vmid="og:description"><meta data-vue-meta="ssr" name="twitter:description" content="Биометрическая идентификация человека – это одна из самых старых идей для распознавания людей, которую вообще попытались технически осуществить. Пароли можно украсть, подсмотреть, забыть, ключи –..." data-vmid="twitter:description"><meta data-vue-meta="ssr" property="aiturec:description" content="Биометрическая идентификация человека – это одна из самых старых идей для распознавания людей, которую вообще попытались технически осуществить. Пароли можно украсть, подсмотреть, забыть, ключи –..." data-vmid="aiturec:description"><meta data-vue-meta="ssr" itemprop="image" content="https://habr.com/share/publication/452894/ce98f9b9d7867563422bf6824b86bc45/" data-vmid="image:itemprop"><meta data-vue-meta="ssr" property="og:image" content="https://habr.com/share/publication/452894/ce98f9b9d7867563422bf6824b86bc45/" data-vmid="og:image"><meta data-vue-meta="ssr" property="aiturec:image" content="https://habr.com/share/publication/452894/ce98f9b9d7867563422bf6824b86bc45/" data-vmid="aiturec:image"><meta data-vue-meta="ssr" name="twitter:image" content="https://habr.com/share/publication/452894/ce98f9b9d7867563422bf6824b86bc45/" data-vmid="twitter:image"><meta data-vue-meta="ssr" property="vk:image" content="https://habr.com/share/publication/452894/ce98f9b9d7867563422bf6824b86bc45/" data-vmid="vk:image"><meta data-vue-meta="ssr" property="aiturec:item_id" content="452894" data-vmid="aiturec:item_id"><meta data-vue-meta="ssr" property="aiturec:datetime" content="2019-05-23T12:22:31.000Z" data-vmid="aiturec:datetime"><meta data-vue-meta="ssr" property="og:type" content="article" data-vmid="og:type"><meta data-vue-meta="ssr" property="og:locale" content="ru_RU" data-vmid="og:locale"><meta data-vue-meta="ssr" property="og:image:width" content="1200" data-vmid="og:image:width"><meta data-vue-meta="ssr" property="og:image:height" content="630" data-vmid="og:image:height">
  <link data-vue-meta="ssr" href="https://habr.com/ru/rss/post/452894/?fl=ru" type="application/rss+xml" title="" rel="alternate" name="rss"><link data-vue-meta="ssr" href="https://habr.com/ru/company/ods/blog/452894/" rel="canonical" data-vmid="canonical"><link data-vue-meta="ssr" data-vmid="hreflang"><link data-vue-meta="ssr" image_src="image" href="https://habr.com/share/publication/452894/ce98f9b9d7867563422bf6824b86bc45/" data-vmid="image:href"><link data-vue-meta="ssr" rel="amphtml" href="https://habr.com/ru/amp/post/452894/">
  <meta name="apple-mobile-web-app-status-bar-style" content="#303b44">
  <meta name="msapplication-TileColor" content="#629FBC">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="mobile-web-app-capable" content="yes">
  <link
    rel="shortcut icon"
    type="image/png"
    sizes="16x16"
    href="https://assets.habr.com/habr-web/img/favicons/favicon-16.png"
  >
  <link
    rel="shortcut icon"
    type="image/png"
    sizes="32x32"
    href="https://assets.habr.com/habr-web/img/favicons/favicon-32.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="76x76"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-76.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="120x120"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-120.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="152x152"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-152.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="180x180"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-180.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="256x256"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-256.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1136x640.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2436x1125.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1792x828.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_828x1792.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1334x750.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1242x2668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2208x1242.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1125x2436.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1242x2208.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2732x2048.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2688x1242.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2224x1668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_750x1334.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2048x2732.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2388x1668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1668x2224.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_640x1136.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1668x2388.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2048x1536.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1536x2048.png"
  >
  <link
    rel="mask-icon"
    color="#77a2b6"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-120.svg"
  >
  <link
    crossorigin="use-credentials"
    href="/manifest.webmanifest"
    rel="manifest"
  >
</head>
<body>


<div id="app" data-server-rendered="true" data-async-called="true"><div class="tm-layout__wrapper"><!----> <div></div> <!----> <header class="tm-header"><div class="tm-page-width"><div class="tm-header__container"><!----> <span class="tm-header__logo-wrap"><a href="/ru/" class="tm-header__logo tm-header__logo_ru"><svg height="16" width="16" class="tm-svg-img tm-header__icon"><title>Хабр</title> <use xlink:href="/img/habr-logo-ru.svg#logo"></use></svg></a> <span class="tm-header__beta-sign" style="display:none;">β</span></span> <div class="tm-dropdown tm-header__projects"><div class="tm-dropdown__head"><button class="tm-header__dropdown-toggle"><svg height="16" width="16" class="tm-svg-img tm-header__icon tm-header__icon_dropdown"><title>Открыть список</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#arrow-down"></use></svg></button></div> <!----></div> <a href="/ru/sandbox/start/" class="tm-header__become-author-btn">
            Как стать автором
          </a> <div class="tm-feature tm-header__feature tm-feature_variant-inline"><!----></div> <!----> <!----></div></div></header> <div class="tm-layout"><div class="tm-page-progress-bar"></div> <div data-menu-sticky="true" class="tm-base-layout__header tm-base-layout__header_is-sticky"><div class="tm-page-width"><div class="tm-base-layout__header-wrapper"><div class="tm-main-menu"><div class="tm-main-menu__section"><nav class="tm-main-menu__section-content"><!----> <a href="/ru/all/" class="tm-main-menu__item">
        Все потоки
      </a> <a href="/ru/flows/develop/" class="tm-main-menu__item">
          Разработка
        </a><a href="/ru/flows/admin/" class="tm-main-menu__item">
          Администрирование
        </a><a href="/ru/flows/design/" class="tm-main-menu__item">
          Дизайн
        </a><a href="/ru/flows/management/" class="tm-main-menu__item">
          Менеджмент
        </a><a href="/ru/flows/marketing/" class="tm-main-menu__item">
          Маркетинг
        </a><a href="/ru/flows/popsci/" class="tm-main-menu__item">
          Научпоп
        </a></nav></div></div> <div class="tm-header-user-menu tm-base-layout__user-menu"><a href="/ru/search/" class="tm-header-user-menu__item tm-header-user-menu__search"><svg height="24" width="24" class="tm-svg-img tm-header-user-menu__icon tm-header-user-menu__icon_search tm-header-user-menu__icon_dark"><title>Поиск</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#search"></use></svg></a> <!----> <!----> <!----> <div class="tm-header-user-menu__item tm-header-user-menu__user_desktop"><div class="tm-dropdown"><div class="tm-dropdown__head"><svg height="24" width="24" data-test-id="menu-toggle-guest" class="tm-svg-img tm-header-user-menu__icon"><title>Профиль</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#header-user"></use></svg> <!----></div> <!----></div> <!----></div> <!----></div></div></div></div> <!----> <div class="tm-page-width"></div> <main class="tm-layout__container"><div hl="ru" companyName="ods" data-async-called="true" class="tm-page"><div class="tm-page-width"><div class="tm-page__header"><!----></div> <div class="tm-page__wrapper"><div class="tm-page__main tm-page__main_has-sidebar"><div class="pull-down"><div class="pull-down__header" style="height:0px;"><div class="pull-down__content" style="bottom:10px;"><svg height="24" width="24" class="tm-svg-img pull-down__arrow"><title>Обновить</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#pull-arrow"></use></svg></div></div> <div class="tm-article-presenter"><div class="tm-company-card tm-company-article__company-card"><div class="tm-company-card__info"><div class="tm-company-card__header"><a href="/ru/company/ods/profile/" class="tm-company-card__avatar"><div class="tm-entity-image"><img alt="" height="48" src="//habrastorage.org/getpro/habr/company/5d0/e1e/b70/5d0e1eb7036b66d4ff4a35ad6950cc3e.png" width="48" class="tm-entity-image__pic"></div></a> <!----> <div class="tm-rating tm-company-card__rating"><div class="tm-rating__header"> <div class="tm-rating__counter">92.29</div></div> <div class="tm-rating__text">
    Рейтинг
  </div></div></div> <div class="tm-company-card__info"><a href="/ru/company/ods/profile/" class="tm-company-card__name">
        Open Data Science
      </a> <div class="tm-company-card__description">Крупнейшее русскоязычное Data Science сообщество</div></div></div> <div class="tm-company-card__buttons"><!----> <!----></div></div> <div class="tm-article-presenter__body"><div class="tm-misprint-area"><div class="tm-misprint-area__wrapper"><article class="tm-article-presenter__content tm-article-presenter__content_narrow"><div class="tm-article-presenter__header"> <div class="tm-article-snippet tm-article-presenter__snippet"><div class="tm-article-snippet__meta-container"><div class="tm-article-snippet__meta"><span class="tm-user-info tm-article-snippet__author"><a href="/ru/users/evgeniimakarov/" title="evgeniimakarov" class="tm-user-info__userpic"><div class="tm-entity-image"><svg height="24" width="24" class="tm-svg-img tm-image-placeholder tm-image-placeholder_blue"><!----> <use xlink:href="/img/megazord-v24.ce74655c.svg#placeholder-user"></use></svg></div></a> <span class="tm-user-info__user"><a href="/ru/users/evgeniimakarov/" class="tm-user-info__username">
      evgeniimakarov
    </a> </span></span> <span class="tm-article-snippet__datetime-published"><time datetime="2019-05-23T12:22:31.000Z" title="2019-05-23, 15:22">23  мая  2019 в 15:22</time></span></div> <!----></div> <h1 lang="ru" class="tm-article-snippet__title tm-article-snippet__title_h1"><span>Face Anti-Spoofing или технологично узнаём обманщика из тысячи по лицу</span></h1> <div class="tm-article-snippet__hubs"><span class="tm-article-snippet__hubs-item"><a href="/ru/company/ods/blog/" class="tm-article-snippet__hubs-item-link router-link-active"><span>Блог компании Open Data Science</span> <!----></a></span><span class="tm-article-snippet__hubs-item"><a href="/ru/hub/infosecurity/" class="tm-article-snippet__hubs-item-link"><span>Информационная безопасность</span> <span title="Профильный хаб" class="tm-article-snippet__profiled-hub">*</span></a></span><span class="tm-article-snippet__hubs-item"><a href="/ru/hub/machine_learning/" class="tm-article-snippet__hubs-item-link"><span>Машинное обучение</span> <span title="Профильный хаб" class="tm-article-snippet__profiled-hub">*</span></a></span><span class="tm-article-snippet__hubs-item"><a href="/ru/hub/popular_science/" class="tm-article-snippet__hubs-item-link"><span>Научно-популярное</span> <!----></a></span><span class="tm-article-snippet__hubs-item"><a href="/ru/hub/artificial_intelligence/" class="tm-article-snippet__hubs-item-link"><span>Искусственный интеллект</span> <!----></a></span></div> <!----> <!----> <!----></div></div> <!----> <div data-gallery-root="" lang="ru" class="tm-article-body"><div id="post-content-body" class="article-formatted-body article-formatted-body_version-1"><div xmlns="http://www.w3.org/1999/xhtml"><p>Биометрическая идентификация человека – это одна из самых старых идей для распознавания людей, которую вообще попытались технически осуществить. Пароли можно украсть, подсмотреть, забыть, ключи – подделать. А вот уникальные характеристики самого человека подделать и потерять намного труднее. Это могут быть отпечатки пальцев, голос, рисунок сосудов сетчатки глаза, походка и прочее.</p><br/>
<p><img src="https://habrastorage.org/r/w780q1/webt/h4/wd/zr/h4wdzrqvlnvfy8da0k3cn5589hg.jpeg" data-src="https://habrastorage.org/webt/h4/wd/zr/h4wdzrqvlnvfy8da0k3cn5589hg.jpeg" data-blurred="true"/></p><br/>
<p>Конечно же, системы биометрии пытаются обмануть! Вот об этом мы сегодня и поговорим. Как злоумышленники пытаются обойти системы распознавания лица, выдав себя за другого человека и каким образом это можно обнаружить.</p><a name="habracut"></a><br/>
<p>Видео-версию этого рассказа можно посмотреть <a href="https://www.youtube.com/watch?v=Y4BMi38yRLo">тут</a>, а тех, кто предпочитает чтение просмотру, приглашаю проследовать дальше</p><br/>
<p>Согласно представлениям режиссеров Голливуда и писателей-фантастов, обмануть биометрическую идентификацию довольно просто. Нужно всего лишь предъявить системе «требуемые части» настоящего пользователя, как по отдельности, так и взяв его в заложники целиком. Или же можно “надеть личину” другого человека на себя, например, с помощью <a href="https://www.kinopoisk.ru/film/bez-litsa-1997-4606">физической пересадки маски</a> или вообще, <a href="https://www.kinopoisk.ru/film/gattaka-1997-5012">предъявления фальшивых генетических признаков</a></p><br/>
<p>В реальной жизни злоумышленники тоже пытаются представиться кем-то другим. Например, ограбить банк, надев маску чернокожего мужчины, как на картинке ниже.</p><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/pl/vz/67/plvz67jfgazkigzqsdhkko62ego.png"/></p><br/>
<p>Распознавание по лицу выглядит очень перспективным направлением для использования в мобильном секторе. Если к использованию отпечатков пальцев все уже давно привыкли, а технологии работы с голосом постепенно и довольно предсказуемо развиваются, то с идентификацией по лицу ситуация сложилась довольно необычная и достойная небольшого экскурса в историю вопроса.</p><br/>
<h2 id="kak-vse-nachinalos-ili-iz-fantastiki-v-realnost">Как все начиналось или из фантастики в реальность</h2><br/>
<p>Сегодняшние системы распознавания демонстрируют огромную точность. С появлением больших наборов данных и сложных архитектур стало возможным добиться точности распознавания лица вплоть до 0,000001 (одна ошибка на миллион!) и они уже сейчас пригодны для переноса на мобильные платформы. Узким местом стала их уязвимость.</p><br/>
<p>Для того, чтобы выдать себя за другого человека в нашей технической реальности, а не в фильме, чаще всего используют маски. Компьютерную систему тоже пытаются одурачить, представив вместо своего лица чье-то еще. Маски бывают совершенно разного качества, от распечатанного на принтере фото другого человека, которое держат перед лицом, до очень сложных трехмерных масок с подогревом. Маски могут как предъявляться отдельно в виде листа или экрана, так и надеваться на голову. </p><br/>
<p>Большое внимание к теме привлекла успешная попытка обмануть систему Face ID на iPhone X с помощью довольно сложной маски из каменного порошка со специальными вставками вокруг глаз, имитирующими тепло живого лица с помощью инфракрасного излучения.</p><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/xl/kq/pr/xlkqprysss0ce2arbqm0sw_xbpo.png"/></p><br/>
<p>Утверждается, что помощью такой маски удалось обмануть Face ID на iPhone X. Видео и немного текста можно найти <a href="http://www.bkav.com/d/top-news/-/view_content/content/103968/bkav%92s-new-mask-beats-face-id-in-twin-way-severity-level-raised-do-not-use-face-id-in-business-transactions">здесь</a></p><br/>
<p>Наличие таких уязвимостей очень опасно для банковских или государственных систем аутентификации пользователя по лицу, где проникновение злоумышленника влечет за собой значительные потери.</p><br/>
<h2 id="terminologiya">Терминология</h2><br/>
<p>Область исследования face anti-spoofing довольно новая и пока еще не может похвастаться даже сложившейся терминологией.</p><br/>
<p>Условимся называть попытку обмана системы идентификации путем предъявления ей поддельного биометрического параметра (в данном случае — лица) <strong>spoofing attack</strong>.</p><br/>
<p>Соответственно, комплекс защитных мер, чтобы противостоять такому обману, будем называть <strong>anti-spoofing</strong>. Он может быть реализован в виде самых разных технологий и алгоритмов, встраиваемых в конвейер системы идентификации.</p><br/>
<p>В <a href="https://www.iso.org/obp/ui/#iso:std:iso-iec:30107:-3:ed-1:v1:en">ISO</a> предлагается несколько расширенный набор терминологии, с такими терминами, как <strong>presentation attack</strong> — попытки заставить систему неверно идентифицировать пользователя или дать ему возможность избежать идентификации, с помощью демонстрации картинки, записанного видео и так далее. <strong>Normal (Bona Fide)</strong> – соответствует обычному алгоритму работы системы, то есть всему, что НЕ является атакой. <strong>Presentation attack instrument</strong> означает средство совершения атаки, например, искусственно изготовленную часть тела. И, наконец, <strong>Presentation attack detection</strong> — автоматизированные средства обнаружения таких атак. Впрочем, сами стандарты все еще находятся в разработке, поэтому говорить о каких-либо устоявшихся понятиях нельзя. Терминология на русском языке отсутствует почти полностью.</p><br/>
<p>Для определения качества работы системы часто пользуются метрикой <strong>HTER</strong> (Half-Total Error Rate – половина полной ошибки), которую вычисляют в виде суммы коэффициентов ошибочно разрешенных идентификаций (FAR – False Acceptance Rate) и ошибочно запрещенных идентификаций (FRR – False Rejection Rate), деленной пополам.<br/>
HTER=(FAR+FRR)/2</p><br/>
<p>Стоит сказать, что в системах биометрии обычно самое большое внимание уделяют FAR, с целью сделать всё возможное, чтобы не допустить злоумышленника в систему. И добиваются в этом неплохих успехов (помните одну миллионную из начала статьи?) Обратной стороной оказывается неизбежное возрастание FRR – количества обычных пользователей, ошибочно классифицированных как злоумышленников. Если для государственных, оборонных и прочих подобных систем этим можно пожертвовать, то мобильные технологии, работающие с их огромными масштабами, разнообразием абонентских устройств и, вообще, user-perspective ориентированные, очень чувствительны к любым факторам, которые могут заставить пользователей отказаться от услуг. Если вы хотите уменьшить количество разбитых об стену телефонов после десятого подряд отказа в идентификации, стоит обратить внимание на FRR!</p><br/>
<h2 id="vidy-atak-obmanyvaem-sistemu">Виды атак. Обманываем систему</h2><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/dt/jx/lp/dtjxlptnxcphiicevd6lbhlzzpa.png"/></p><br/>
<p>Давайте, наконец, узнаем, как именно злоумышленники обманывают системы распознавания, а также как этому можно противопоставить.</p><br/>
<p>Самым популярным средством обмана являются маски. Нет ничего более очевидного, чем надеть маску другого человека и представить лицо системе идентификации (часто именуется Mask attack).</p><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/q2/tr/nl/q2trnl3zhhuftbggl3vx-sfqoyw.png"/></p><br/>
<p>Еще можно распечатать фото себя или кого-то еще на листе бумаге и поднести его к камере (условимся называть такой тип атаки Printed attack). </p><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/f9/g7/ds/f9g7dsc81eiffu9srctasf2udcm.png"/></p><br/>
<p>Чуть более сложной является Replay attack, когда системе предъявляют экран другого устройства, на котором воспроизводится заранее записанное видео с другим человеком. Сложность исполнения компенсируется высокой эффективностью такой атаки, поскольку системы контроля часто используют признаки, основанные на анализе временных последовательностей, например, отслеживание моргания, микродвижений головы, наличие мимики, дыхания и так далее. Все это можно легко воспроизвести на видео.</p><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/de/dd/vj/deddvj6xvnwbv6dpbryxphjwaa8.png"/></p><br/>
<p>Оба типа атак имеют ряд характерных признаков, позволяющих их обнаружить, и, таким образом, отличить экран планшета или лист бумаги от реального лица.</p><br/>
<p>Сведем характерные признаки, позволяющие определить эти два типа атак, в таблицу:</p><br/>
<div class="scrollable-table"><table>
<thead>
<tr>
<th><strong>Printed attack</strong></th>
<th><strong>Replay attack</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Снижение качества текстуры изображения при печати</td>
<td>Муар</td>
</tr>
<tr>
<td>Артефакты передачи полутонового изображения при печати на принтере</td>
<td>Отражения (блики)</td>
</tr>
<tr>
<td>Механические артефакты печати (горизонтальные линии)</td>
<td>Плоская картинка (отсутствие глубины)</td>
</tr>
<tr>
<td>Отсутствие локальных движений (например, морганий)</td>
<td>Могут быть видны границы изображения</td>
</tr>
<tr>
<td>Могут быть видны границы изображения</td>
<td></td>
</tr>
</tbody>
</table></div><br/>
<h2 id="algoritmy-obnaruzheniya-atak-staraya-dobraya-klassika">Алгоритмы обнаружения атак. Старая добрая классика</h2><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/1r/n8/-e/1rn8-elrfhm1q0ud9q4jbaukidq.png"/></p><br/>
<p>Один из самых старых подходов (работы 2007, 2008 годов) основан на обнаружении морганий человека путем анализа изображения по маске. Смысл заключается в построении какого-либо бинарного классификатора, позволяющего выделить изображения с открытыми и закрытыми глазами в последовательности кадров. Это может быть анализ видеопотока с помощью выделения частей лица (landmark detection), или же использование какой-то простой нейронной сети. И на сегодняшний день чаще всего используется этот метод; пользователю предлагают выполнить какую-то последовательность действий: покрутить головой, подмигнуть, улыбнуться и прочее. Если последовательность случайна, подготовиться к ней злоумышленнику заранее непросто. К сожалению, для честного пользователя этот квест тоже не всегда преодолим, и вовлеченность резко падает.</p><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/4s/fb/vv/4sfbvvforopuonls_ochr8dpzjg.png"/></p><br/>
<p>Еще можно использовать особенности ухудшения качества картинки при печати или воспроизведении на экране. Скорее всего, на изображении будут обнаружены даже какие-то локальные паттерны, пусть и неуловимые глазом. Это можно сделать, например, посчитав локальные бинарные паттерны (LBP, local binary pattern) для различных зон лица после выделения его из кадра (<a href="https://publications.idiap.ch/downloads/papers/2012/Chingovska_IEEEBIOSIG2012_2012.pdf">PDF</a>). Описанную систему можно считать основоположником всего направления алгоритмов face anti-spoofing на основе анализа изображения. В двух словах, при расчете LBP последовательно берется каждый пиксель изображения, восемь его соседей и сравнивается их интенсивность. Если интенсивность больше, чем на центральном пикселе, присваивается единица, если меньше – ноль. Таким образом, для каждого пикселя получается 8-битовая последовательность. По полученным последовательностям строится попиксельная гистограмма, которая подается на вход SVM-классификатора.</p><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/qu/x_/vv/qux_vvmfk7ikww7ojknhq_mpbhu.png"/></p><br/>
<p>Локальные бинарные паттерны, гистограммирование и SVM. Приобщиться к неустаревающей классике можно по <a href="https://publications.idiap.ch/downloads/papers/2012/Chingovska_IEEEBIOSIG2012_2012.pdf">ссылке</a></p><br/>
<p>Показатель эффективности HTER составляет «целых» 15%, и означает, что значительная часть злоумышленников преодолевает защиту без особых усилий, хотя и следует признать что множество и отсеивается. Алгоритм тестировался на наборе данных <a href="https://www.idiap.ch/datase">Replay-Attack</a> от IDIAP, который составлен из 1200 коротких видео 50 респондентов и трех видов атак – printed attack, mobile attack, high-definition attack.</p><br/>
<p>Идеи анализа текстуры изображения получили продолжение. В 2015 году Букинафит <a href="https://arxiv.org/abs/1511.06316">разработал</a> алгоритм альтернативного разбиения изображения на каналы, помимо традиционного RGB, для результатов которого снова подсчитывались локальные бинарные паттерны, которые, как и в предыдущем способе, подавались на вход SVN классификатора. Точность HTER, рассчитанная на датасетах CASIA и Replay-Attack, составила впечатляющие на тот момент 3%.</p><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/7j/vy/i7/7jvyi7inhsl5g2ulp05v4luido8.png"/></p><br/>
<p>В это же время появились работы по обнаружению муара. Пател <a href="http://biometrics.cse.msu.edu/Publications/Face/PatelHanJainOtt_LivevsSpoofFaceVideo_ICB15.pdf">опубликовал</a> статью, где предложил искать артефакты изображения в виде периодического узора, вызванные наложением двух разверток. Подход оказался работоспособным, показав HTER около 6% на наборах данных IDIAP, CASIA и RAFS. Это также было первой попыткой сравнить эффективность работы алгоритма на различных наборах данных.</p><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/98/m2/ls/98m2ls-svv7gxfaagripfrhzymo.png"/></p><br/>
<p>Периодический узор на изображении, вызванный наложением разверток</p><br/>
<p>Чтобы обнаружить попытки предъявления фото, логичным решением было попытаться анализировать не одно изображение, а их последовательность, взятую из видео потока. Например, Анжос с коллегами <a href="https://pdfs.semanticscholar.org/82fb/f7fe9b2114b87765e2aa62204b98a44dc89b.pdf">предложили</a> выделять признаки из оптического потока на соседних парах кадров, подавать на вход бинарного классификатора и усреднять результаты. Подход оказался достаточно эффективным, продемонстрировав HTER 1,52% на их собственном наборе данных.</p><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/ih/9e/vo/ih9evob9a5hmtjekxhx1iwsgxm8.png"/></p><br/>
<p>Интересным выглядит метод отслеживания движений, находящийся несколько в стороне от общепринятых подходов. Так как в 2013 году обычного для современных проектов в области глубокого обучения принципа «подать сырое изображение на вход сверточной сети и настраивать слои сетки до получения результата» не было, Бхарадваж последовательно <a href="http://iab-rubric.org/papers/PID2777141.pdf">применил</a> более сложные предварительные преобразования. В частности, он применил известный по работам ученых из MIT алгоритм эйлеровского усиления видео <a href="http://people.csail.mit.edu/mrub/evm/">Eulerian video magnification</a>, который с успехом применялся для анализа цветовых изменений кожного покрова в зависимости от пульса. Заменил LBP на HOOF (гистограммы направлений оптического потока), верно заметив, что коль скоро мы хотим отслеживать движения, и признаки нам нужны соответствующие, а не просто анализ текстур. В качестве классификатора использовался все тот же SVM, традиционный на тот момент. Алгоритм показал крайне впечатляющие результаты на датасетах Print Attack (0%) и Replay Attack (1,25%)</p><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/-i/1w/w2/-i1ww2rsu0kqmxglhh-on8lnr5g.png"/></p><br/>
<h2 id="davayte-uzhe-uchit-setki">Давайте уже учить сетки!</h2><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/7p/t-/yk/7pt-ykbzzuyb7bzpnd9qpjodfog.png"/></p><br/>
<p>С какого-то момента стало очевидно, что назрел переход к глубокому обучению. Пресловутая «революция глубокого обучения» настигла и face anti-spoofing.</p><br/>
<p>«Первой ласточкой» можно считать метод анализа карт глубины на отдельных участках («патчах») изображения. Очевидно, карта глубины является очень хорошим признаком для определения плоскости, в которой расположено изображение. Хотя бы потому что у изображения на листе бумаги «глубины» нет по определению. В работе <a href="http://cvlab.cse.msu.edu/pdfs/FaceAntiSpoofingUsingPatchandDepthBasedCNNs.pdf">Атаума</a> 2017 года из изображения извлекалось множество отдельных небольших участков, для них рассчитывались карты глубины, которые затем сливались с картой глубины основного изображения. При этом указывалось, что десяти случайных патчей изображения лица достаточно для надежного определения Printed Attack. Дополнительно авторы сливали вместе результаты работы двух сверточных нейросетей, первая из которых рассчитывала карты глубины для патчей, а вторая – для изображения в целом. При обучении на наборах данных с классом Printed Attack связывалась карта глубины, равная нулю, а с трехмерной моделью лица – серия случайно отбираемых участков. По большому счету, сама по себе карта глубины была не так важна, от нее использовалась лишь некоторая индикаторная функция, характеризующая «глубину участка». Алгоритм показал значение HTER 3,78%. Для обучения были использованы три публичных набора данных — CASIA-MFSD, MSU-USSA и Replay-Attack.</p><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/a7/bl/df/a7bldfybkgzeeptdtomphvq1kwk.png"/></p><br/>
<p>К сожалению, доступность большого количества прекрасных фреймворков для глубокого обучения привело к появлению огромного количества разработчиков, которые пытаются «в лоб» решить задачу face anti-spoofing хорошо знакомым способом ансамблирования нейросетей. Обычно это выглядит как стек карт признаков на выходах нескольких сетей, предобученных на каком-либо широко распространенном датасете, который подается на бинарный классификатор. </p><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/ek/8b/rc/ek8brc52akg2y3zfy6ejmbr-taq.png"/></p><br/>
<p>В целом стоит заключить, что к настоящему моменту опубликовано довольно много работ, которые в целом демонстрируют неплохие результаты, и которые объединяет всего одно небольшое «но». Все эти результаты продемонстрированы в рамках одного конкретного датасета! Ситуация усугубляется ограниченностью имеющихся наборов данных и, например, на пресловутом Replay-Attack уже никого не удивить HTER 0%. Все это приводит к появлению очень сложных архитектур, например, вот <a href="https://arxiv.org/abs/1808.08802">таких</a>, с использованием различных мудрёных признаков, вспомогательных алгоритмов, собранных в стек, с несколькими классификаторами, результаты которых усредняются и так далее… На выходе авторы получают HTER =0,04%!</p><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/aw/_k/s_/aw_ks_6-xp40fyjz97ue9mwzp8q.png"/></p><br/>
<p>Это наводит на мысль о том, что задача face anti-spoofing в рамках конкретного датасета решена. Сведем в таблицу различные современные методы на основе нейросетей. Как легко увидеть, «эталонных результатов» удалось достигнуть очень разнообразными методами, которые только возникли в пытливых умах разработчиков.</p><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/bb/7v/73/bb7v73awyzwhxeyyvtejgm3skh4.png"/></p><br/>
<p>Сравнительные результаты различных алгоритмов. Таблица взята <a href="https://arxiv.org/pdf/1710.09868v2.pdf">отсюда</a>.</p><br/>
<p>К сожалению, благостную картину борьбы за десятые доли процента нарушает все тот же «маленький» фактор. Если попытаться обучить нейросеть на одном наборе данных, а применить – на другом, то результаты окажутся… не столь оптимистичными. Хуже того, попытки применить классификаторы в реальной жизни не оставляют и вовсе никакой надежды.<br/>
Для примера, возьмем данные <a href="http://vipl.ict.ac.cn/uploadfile/upload/2017020711092984.pdf">работы</a> 2015 года, где для определения подлинности предъявленного изображения использовалась метрика его качества. Взгляните сами:</p><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/jw/b5/-n/jwb5-naohhjhek2tkmakbssjyik.png"/></p><br/>
<p>Иными словами, алгоритм, натренированный на данных Idiap, а примененный на MSU, даст коэффициент истинно положительных обнаружений 90,5%, а, если сделать наоборот (обучить на MSU, а проверить – на Idiap), то верно удастся определить только 47,2%(!) Для других сочетаний ситуация ухудшается еще больше, и, например, если натренировать алгоритм на MSU, а проверить – на CASIA, то TPR составит 10,8%! Это означает, что к атакующим было ошибочно причислено огромное количество честных пользователей, что не может не удручать. Ситуацию не смогло переломить даже cross-database обучение, что вроде бы кажется вполне разумным выходом из положения. </p><br/>
<p>Посмотрим еще. Результаты, приведенные в <a href="http://biometrics.cse.msu.edu/Publications/Face/PatelHanJain_FaceAntispoofing_CCBR2016.pdf">статье</a> Патела 2016 года, показывают, что даже при достаточно сложных конвейерах обработки и выделении таких надежных признаков, как моргание и текстура, результаты на незнакомых наборах данными не могут считаться удовлетворительными. Итак, в какой-то момент стало вполне очевидно, что предложенных способов отчаянно не хватает для обобщения результатов.</p><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/vf/lz/ol/vflzoljiplnkj_vgm-zth9a0sdc.png"/></p><br/>
<h2 id="a-esli-ustroit-sorevnovanie">А если устроить соревнование…</h2><br/>
<p>Конечно же, в области face anti-spoofing не обошлось без соревнований. В 2017 году в университете Оулу в Финляндии состоялся конкурс на собственном новом наборе данных с достаточно интересными протоколами, ориентированными, как раз, на использование в области мобильных приложений.</p><br/>
<p>-Протокол 1: Имеется разница в освещении и фоне. Наборы данных записаны в различных местах и отличаются фоном и освещением.</p><br/>
<p>-Протокол 2: Для атак использованы различные модели принтеров и экранов. Так, в проверочном наборе данных использована техника, которая не встречается в обучающем наборе</p><br/>
<p>-Протокол 3: Взаимозаменяемость датчиков. Видео настоящего пользователя и атак записываются на пять различных смартфонов и используются в наборе данных для обучения. Для проверки алгоритма используется видео с еще одного смартфона, который в обучающем наборе не включен.</p><br/>
<p>-Протокол 4: включает все вышеуказанные факторы.</p><br/>
<p>Результаты оказались достаточно неожиданными. Как и в любом соревновании, времени придумывать гениальные идеи не было, поэтому практически все участники взяли знакомые архитектуры и доработали их тонкой настройкой, работой с признаками и попытками как-то использовать для обучения другие наборы данных. Призовое решение показало ошибку на четвертом, самом сложном протоколе, около 10%. Краткое описание алгоритмов победителей в таблице чуть ниже:</p><br/>
<ol>
<li><p>GRADIENT</p><br/>
<ul>
<li>Выполняется слияние признаков по цвету (используя цветовые пространства HSV и YCbCr), текстуре и движению.</li>
<li>Информация о динамике извлекается по данной последовательности видео и картам изменений по времени в отдельном кадре.</li>
<li>Эта последовательность раздельно применяется по всем каналам в цветовых пространствах HSV и YCbCr, дающих вместе пару трехканальных изображений. Для каждого изображения ROI (region-of-interest) обрезается на основе положения глаз в последовательности кадров и масштабируется до 160×160 пикселей..</li>
<li>Каждая ROI делится на 3×3 и 5×5 прямоугольных областей, по которым извлекаются равномерные LBP гистограммы признаков, которые объединяются в два вектора признаков размерностью 6018.</li>
<li>С помощью рекурсивного удаления признаков (Recursive Feature Elimination) размерность уменьшается с 6018 до 1000. </li>
<li>Для каждого вектора признаков выполняется классификация на основе SVM с последующим усреднением.|</li>
</ul><br/>
</li>
<li><p>SZCVI</p><br/>
<ul>
<li>Из каждого видео извлекается выборка кадров, берется каждый шестой</li>
<li>Масштабирование кадров до 216×384</li>
<li>Пять VGG-подобных слоев</li>
<li>Результаты отдельных кадров внутри выборки усредняются</li>
</ul><br/>
</li>
<li><p>Recod</p><br/>
<ul>
<li>SqueezeNet обучается на Imagenet</li>
<li>Transfer learning на двух наборах данных: CASIA и UVAD</li>
<li>Сначала лицо обнаруживается и масштабируется до 224×224 pixels. Из каждого видео обучающего датасета извлекается, примерно, каждый седьмой кадр, который направляется на десять CNN.</li>
<li>Для получения итогового результата показатели отдельных кадров усредняются.</li>
<li>Для улучшения эффективности полученные показатели сводятся в обобщенный результат базового метода</li>
</ul><br/>
</li>
<li><p>CPqD</p><br/>
<ul>
<li>Сеть Inception-v3, обученная на ImageNet</li>
<li>Cигмоидная функция активации</li>
<li>На основании определения положения глаз выполняется обрезка участков изображения, содержащих лицо, которые затем масштабируются до кадров 224×224 RGB |</li>
</ul><br/>
</li>
</ol><br/>
<p>Хорошо видно, что новых идей появилось не так много. Все те же LBP, предобученные сетки, анализ текстуры и цвета, попарный анализ кадров и т.д. GRADIANT выглядит наиболее грамотно спроектированным с системной точки зрения, в нем смешиваются различные признаки, идет работа в различных цветовых пространствах, проводится чистка признаков. Он и победил в соревновании.</p><br/>
<p>Соревнование очень ярко показало существующие ограничения. В первую очередь, это ограниченность и несблансированность существующих датасетов для обучения. Во-первых, в них представлено довольно ограниченное количество людей (от 15 человек в NUAA до 1140 в MSU-USSA) и сессий, разнице внешнего освещения, выражениям лица, применяемым устройствам записи, углам съемки и видам атак. При этом в реальных условиях модель камеры, качество матрицы, условия съемки, фокусное расстояние и выдержка, фон и обстановка часто оказываются определяющими для анализа изображений. Во-вторых, сами методы анализа гораздо больше ориентированы на анализ отдельных участков изображения без существенной обработки самой обстановки сцены. Например, в наборе CASIA множество примеров атак представлены в виде изображения человека, который держит перед лицом фотографию. Очевидно, что видно характерное положение рук, границы листа с фото, могут быть видны волосы, шея и голова и так далее… Но решений, использующих анализ всей сцены и положения человека, представлено не было, все алгоритмы работали только с выделенным из всей сцены участком лица.</p><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/ur/yf/t8/uryft8082ylvw-1kbyvcxucjcem.png"/></p><br/>
<p>Недавно был предложен еще один многообещающий <a href="https://datasouls.com/c/idrnd-antispoof/description">конкурс</a> на новом наборе данных собственной разработки размером 30 Гб. Согласно условиям конкурса, должно быть выполнено обнаружение надетой на лицо маски, факта съемки распечатанной фотографии и предъявления видеозаписи на экране вместо настоящего лица. Вполне вероятно, что по его результатам мы и увидим концептуально новое решение.</p><br/>
<p>Конечно, есть решения, основанные на «нестандартных подходах». Перейдем к ним с надеждой на улучшение текущего положения дел. Например, было <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Siqi_Liu_Remote_Photoplethysmography_Correspondence_ECCV_2018_paper.pdf">предложено</a> воспользоваться методом дистанционной фотоплетизмографии (rPPG – remote photoplethysmography), позволяющим обнаружить биение пульса человека по видеоизображению. Идея состоит в том, что при попадании света на живое лицо человека часть света отразится, часть-рассеется, а часть – поглощается кожей и тканями лица. При этом картина будет разной в зависимости от степени наполненности тканей кровью. Таким образом, можно отследить пульсацию крови в сосудах лица и, соответственно, обнаружить пульс. Конечно, если закрыть лицо маской или предъявить экран телефона, никакой пульсации обнаружить не получится. На этом принципе Лю с соавторами предложили разбивать изображение лица на участки, детектировать пульс методом дистанционной фотоплетизмографии, попарно сравнивать различные участки для подсчета пульса и строить карты с целью обнаружения наличия или отсутствия маски, а также сравнения пульса на разных участках лица.</p><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/db/le/44/dble44fc8acyqdhzead3ndbkg3a.png"/></p><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/sy/u7/vu/syu7vucfvb5inhajqoilb6rqxu8.png"/></p><br/>
<p>Работа показала значение HTER около 10%, подтвердив принципиальную применимость метода. Имеется еще несколько работ, подтверждающих перспективность этого подхода<br/>
(CVPR 2018) J. H.-Ortega et al. <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w11/Hernandez-Ortega_Time_Analysis_of_CVPR_2018_paper.pdf">Time Analysis of Pulsebased Face Anti-Spoofing in Visible and NIR</a><br/>
(2016) X. Li. et al. <a href="https://projet.liris.cnrs.fr/imagine/pub/proceedings/ICPR-2016/media/files/1223.pdf">Generalized face anti-spoofing by detecting pulse from face videos</a><br/>
(2016) J. Chen et al. <a href="https://www.researchgate.net/publication/312566385_RealSense_real_heart_rate_Illumination_invariant_heart_rate_estimation_from_videos">Realsense = real heart rate: Illumination invariant heart rate estimation from videos</a><br/>
(2014) H. E. Tasli et al. <a href="http://www.vicarvision.nl/pub/Tasli_Gudi_denUyl_RemotePPG_2014.pdf">Remote PPG based vital sign measurement using adaptive facial regions</a></p><br/>
<p>В 2018 году Лю с коллегами из университета Мичигана предложили <a href="http://cvlab.cse.msu.edu/pdfs/Liu_Jourabloo_Liu_CVPR2018.pdf">отказаться от бинарной классификации</a> в пользу подхода, который они назвали “binary supervision” – то есть использование более сложной оценки на основе карты глубины и дистанционной фотоплетизмографии. Для каждого из настоящих изображений лица реконструировали трехмерную модель с помощью <a href="https://github.com/YadiraF/PRNet">нейросети</a> и назвали ее с картой глубины. Фальшивым изображениям была присвоена карта глубины, состоящая из нулей, в конце концов это ведь просто лист бумаги или экран устройства! Эти характеристики были приняты за «истину», нейросети обучались на собственном наборе данных SiW. Затем, на входное изображение накладывалась трехмерная маска лица, для нее высчитывались карта глубины и пульс, и все это связывалось вместе в довольно сложном конвейере. В итоге, метод показал точность около 10 процентов на конкурсном наборе данных OULU. Интересно, что победитель соревнования, организованного университетом Оулу, построил алгоритм на бинарных паттернах классификации, отслеживании морганий и прочих признаках «конструированных вручную», и его решение тоже имело точность около 10%. Выигрыш составил всего лишь около половины процента! В пользу новой комбинированной технологии говорит то, что алгоритм был обучен на собственном наборе данных, а проверен на OULU, улучшив результат победителя. Что говорит о некоторой переносимости результатов с датасета на датасет, и чем черт не шутит, возможно и на реальную жизнь. Однако, при попытке выполнить обучение на других датасетах – CASIA и ReplayAttack, снова был получен результат около 28%. Конечно, это превосходит показатели других алгоритмах при обучении на различных наборах данных, но при таких значениях точности ни о каком промышленном использовании речи быть не может!</p><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/qd/bu/fn/qdbufnt8yc9pghj4egkxqaku_3g.png"/></p><br/>
<p>Другой подход был предложен Вангом с коллегами в свежей <a href="https://arxiv.org/abs/1811.05118">работе</a> 2019 года. Было отмечено, что при анализе микродвижений лица заметны повороты и смещения головы, приводящие к характерному изменению углов и относительных расстояний между признаками на лице. Так при смещении лица в стороны по горизонтали угол между носом и ухом увеличивается. Но, если таким же образом сместить лист бумаги с картинкой, угол уменьшится! Для иллюстрации стоит процитировать рисунок из работы.</p><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/x9/lq/kz/x9lqkzuzmgz7gz5ejx1rsiom7wq.png"/></p><br/>
<p>На этом принципе авторы построили целый обучаемый блок для переноса данных между слоями нейронной сети. В нем учитывались «неправильные смещения» для каждого кадра в последовательности из двух кадров, и это позволило использовать результаты в следующем блоке анализа долговременных зависимостей на базе GRU <a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit">Gated Recurrent Unit</a>. Затем все признаки конкатенировались, подсчитывалась функция потерь и выполнялась итоговая классификация. Это позволило еще слегка улучшить результат на наборе данных OULU, но проблема зависимости от обучающего данных осталась, поскольку для пары CASIA-MFSD и Replay-Attack показатели составили 17,5 и 24 процента, соответственно.</p><br/>
<p>Под занавес стоит отметить <a href="https://arxiv.org/pdf/1902.10311.pdf">работу</a> специалистов Tencent, предложивших изменить сам способ получения исходного видеоизображения. Вместо пассивного наблюдения за сценой они предложили динамически освещать лицо и считывать отражения. Принцип активного облучения объекта уже давно применяется в локационных системах различного рода, поэтому, его использование для изучения лица выглядит весьма логичным. Очевидно, что для надежной идентификации в самом изображении не хватает признаков, и освещение экрана телефона или планшета последовательностью световых символов (light CAPTCHA по терминологии авторов), может сильно помочь. Далее определяется разница в рассеянии и отражении по паре кадров, и результаты подаются на многозадачную нейронную сеть для дальнейшей обработки по карте глубины и вычисления различных функций потерь. В конце выполняется регрессия нормализованных кадров освещенности. Авторы не анализировали обобщающую способность своего алгоритма на других наборах данных и обучали его на собственном закрытом датасете. Результат составляет порядка 1% и сообщается, что модель уже была развернута для реального использования. </p><br/>
<p><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/c-/ts/w8/c-tsw8vywrvsv3l58ow-axifkay.png"/></p><br/>
<p>До 2017 года область face anti-spoofing была не слишком активной. Зато 2019 уже подарил целую серию работ, что связано с агрессивным продвижением мобильных технологий идентификации по лицу, в первую очередь, компанией Apple. Кроме того, технологиями распознавания по лицу заинтересовались банки. В отрасль пришло много новых людей, что позволяет надеяться на быстрый прогресс. Но пока что, несмотря на красивые названия публикаций, обобщающая способность алгоритмов остается очень слабой и не позволяет говорить о какой-либо пригодности к практическому использованию.</p><br/>
<h2 id="zaklyuchenie-a-naposledok-ya-skazhu-chto">Заключение. А напоследок я скажу, что…</h2><br/>
<ul>
<li>Локальные бинарные паттерны, отслеживание моргания, дыхания, движений и прочие сконструированные вручную признаки совершенно не потеряли значимости. Это вызвано, прежде все тем, что глубокое обучение в области face anti-spoofing все еще весьма наивно.</li>
<li>Совершенно очевидно, что в «том самом» решении будет выполняться слияние нескольких методов. Анализ отражения, рассеяния, карты глубины должны использоваться вместе. Скорее всего, поможет добавление дополнительного канала данных, например, запись голоса и какие-то системные подходы, которые позволят собрать несколько технологий в единую систему</li>
<li>Практически все технологии, используемые для распознавания лица, находят применение в face anti-spoofing (кэп!) Все, что было разработано для распознавания лиц, в том или ином виде нашло применение и для анализа атак</li>
<li>Существующие датасеты достигли насыщения. Из десяти основных наборов данных в пяти удалось достичь нулевой ошибки. Это уже говорит, например, о работоспособности методов на основе карт глубины, но не позволяет улучшить обобщающую способность. Нужны новые данные и новые эксперименты на них</li>
<li>Есть явный дисбаланс между степенью развития распознавания лиц и face anti-spoofing. Технологии распознавания существенно опережают системы защиты. Более того, именно отсутствие надежных систем защиты тормозит практическое применение систем распознавания лиц. Так получилось, что основное внимание уделялось именно распознаванию лиц, а системы обнаружения атак остались несколько в стороне</li>
<li>Есть сильная потребность системного подхода в области face anti-spoofing. Прошедший конкурс университета Оулу показал, что при использовании нерепрезентативного набора данных вполне возможно победить простой грамотной настройкой устоявшихся решений, без разработки новых. Возможно, <a href="https://datasouls.com/c/idrnd-antispoof/description">новое соревнование</a> сможет переломить ситуацию</li>
<li>С возрастанием интереса к тематике и внедрением технологий распознавания по лицу крупными игроками появились «окна возможностей» для новых амбициозных команд, поскольку есть серьезная потребность в новом решении на уровне архитектуры</li>
</ul></div></div> <!----> <!----></div> <div class="tm-article-presenter__meta"><div class="tm-separated-list tm-article-presenter__meta-list"><span class="tm-separated-list__title">Теги:</span> <ul class="tm-separated-list__list"><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5BFace%20recognition%5D" class="tm-tags-list__link">Face recognition</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bface%20anti-spoofing%5D" class="tm-tags-list__link">face anti-spoofing</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bface%20biometrics%5D" class="tm-tags-list__link">face biometrics</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5BOpenDataScience%5D" class="tm-tags-list__link">OpenDataScience</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5BODS%5D" class="tm-tags-list__link">ODS</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bmachine%20learning%5D" class="tm-tags-list__link">machine learning</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bdeep%20learning%5D" class="tm-tags-list__link">deep learning</a></li></ul></div> <div class="tm-separated-list tm-article-presenter__meta-list"><span class="tm-separated-list__title">Хабы:</span> <ul class="tm-separated-list__list"><li class="tm-separated-list__item"><a href="/ru/company/ods/blog/" class="tm-hubs-list__link router-link-active">
    Блог компании Open Data Science
  </a></li><li class="tm-separated-list__item"><a href="/ru/hub/infosecurity/" class="tm-hubs-list__link">
    Информационная безопасность
  </a></li><li class="tm-separated-list__item"><a href="/ru/hub/machine_learning/" class="tm-hubs-list__link">
    Машинное обучение
  </a></li><li class="tm-separated-list__item"><a href="/ru/hub/popular_science/" class="tm-hubs-list__link">
    Научно-популярное
  </a></li><li class="tm-separated-list__item"><a href="/ru/hub/artificial_intelligence/" class="tm-hubs-list__link">
    Искусственный интеллект
  </a></li></ul></div></div></article></div> <!----></div> <div class="tm-article-sticky-panel"><div class="tm-data-icons tm-article-sticky-panel__icons"><div class="tm-article-rating tm-data-icons__item"><div class="tm-votes-meter tm-article-rating__votes-switcher"><svg height="16" width="16" class="tm-svg-img tm-votes-meter__icon tm-votes-meter__icon_medium"><title>Всего голосов 67: ↑66 и ↓1</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-rating"></use></svg> <span title="Всего голосов 67: ↑66 и ↓1" class="tm-votes-meter__value tm-votes-meter__value_positive tm-votes-meter__value_medium">+65</span></div> <DIV class="v-portal" style="display:none;"></DIV></div> <!----> <span title="Количество просмотров" class="tm-icon-counter tm-data-icons__item"><svg height="16" width="16" class="tm-svg-img tm-icon-counter__icon"><title>Просмотры</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-views"></use></svg> <span class="tm-icon-counter__value">21K</span></span> <button title="Добавить в закладки" type="button" class="bookmarks-button tm-data-icons__item"><span title="Добавить в закладки" class="tm-svg-icon__wrapper bookmarks-button__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Добавить в закладки</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-favorite"></use></svg></span> <span title="Количество пользователей, добавивших публикацию в закладки" class="bookmarks-button__counter">
    80
  </span></button> <!----> <div title="Поделиться" class="tm-sharing tm-data-icons__item"><button type="button" class="tm-sharing__button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="tm-sharing__icon"><path fill="currentColor" d="M10.33.275l9.047 7.572a.2.2 0 010 .306l-9.048 7.572a.2.2 0 01-.328-.153V11c-8 0-9.94 6-9.94 6S-1 5 10 5V.428a.2.2 0 01.328-.153z"></path></svg></button> <!----></div> <DIV class="v-portal" style="display:none;"></DIV></div> </div></div> <!----> <!----> <div class="tm-article-presenter__footer"><div class="tm-article-blocks"><!----> <section class="tm-block tm-block_spacing-bottom"><!----> <div class="tm-block__body tm-block__body_variant-balanced"><div class="tm-article-author"><div class="tm-article-author__company"><div class="tm-article-author__company-card"><div class="tm-company-snippet"><a href="/ru/company/ods/profile/" class="tm-company-snippet__logo-link"><div class="tm-entity-image"><img alt="" height="40" src="//habrastorage.org/getpro/habr/company/5d0/e1e/b70/5d0e1eb7036b66d4ff4a35ad6950cc3e.png" width="40" class="tm-entity-image__pic"></div></a> <div class="tm-company-snippet__info"><a href="/ru/company/ods/profile/" class="tm-company-snippet__title">Open Data Science</a> <div class="tm-company-snippet__description">Крупнейшее русскоязычное Data Science сообщество</div></div></div> <div class="tm-article-author__buttons"><!----> <!----></div></div> <div class="tm-article-author__company-contacts"><a href="http://ods.ai/" rel="noopener" target="_blank" class="tm-article-author__contact">
      Сайт
    </a></div> <div class="tm-article-author__separator"></div></div> <div class="tm-user-card tm-article-author__user-card tm-user-card_variant-article"><div class="tm-user-card__info-container"><div class="tm-user-card__header"><div class="tm-user-card__header-data"><a href="/ru/users/evgeniimakarov/" class="tm-user-card__userpic tm-user-card__userpic_size-40"><div class="tm-entity-image"><svg class="tm-svg-img tm-image-placeholder tm-image-placeholder_blue"><!----> <use xlink:href="/img/megazord-v24.ce74655c.svg#placeholder-user"></use></svg></div></a> <div class="tm-user-card__meta"><div title=" 27 голосов " class="tm-karma tm-user-card__karma"><div class="tm-karma__votes tm-karma__votes_positive">
    27
  </div> <div class="tm-karma__text">
    Карма
  </div></div> <div title="Рейтинг пользователя" class="tm-rating tm-user-card__rating"><div class="tm-rating__header"> <div class="tm-rating__counter">0</div></div> <div class="tm-rating__text">
    Рейтинг
  </div></div></div></div></div> <div class="tm-user-card__info tm-user-card__info_variant-article"><div class="tm-user-card__title tm-user-card__title_variant-article"><!----> <a href="/ru/users/evgeniimakarov/" class="tm-user-card__nickname tm-user-card__nickname_variant-article">
          @evgeniimakarov
        </a> <!----></div> <p class="tm-user-card__short-info tm-user-card__short-info_variant-article">Пользователь</p></div></div> <div class="tm-user-card__buttons tm-user-card__buttons_variant-article"><!----> <!----> <!----> <!----> <!----></div></div> <!----></div> <DIV class="v-portal" style="display:none;"></DIV></div> <!----></section> <div class="tm-article-blocks__comments"><div class="tm-article-page-comments"><div class="tm-article-comments-counter-link tm-article-comments-counter-button"><a href="/ru/company/ods/blog/452894/comments/" class="tm-article-comments-counter-link__link tm-article-comments-counter-link__link_button-style"><svg height="16" width="16" class="tm-svg-img tm-article-comments-counter-link__icon tm-article-comments-counter-link__icon_contrasted"><title>Комментарии</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-comments"></use></svg> <span class="tm-article-comments-counter-link__value tm-article-comments-counter-link__value_contrasted">
       Комментарии 18 
    </span></a> <!----></div></div></div>  <!---->  <!----> <!----></div></div></div></div></div> <div class="tm-page__sidebar"><div class="tm-layout-sidebar"><div class="tm-layout-sidebar__placeholder_initial"></div> <div class="tm-sexy-sidebar tm-sexy-sidebar_initial" style="margin-top:0px;"><!----> <section class="tm-block tm-block_spacing-bottom"><header class="tm-block__header"><h2 class="tm-block__title">Информация</h2> <!----></header> <div class="tm-block__body"><div class="tm-company-basic-info"><dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Дата основания</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap"><time datetime="2015-03-11T21:00:00.000Z" title="2015-03-12, 00:00">12  марта  2015</time></dd></dl> <dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Местоположение</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap">
    Россия
  </dd></dl> <dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Сайт</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap"><a href="http://ods.ai/" target="_blank" class="tm-company-basic-info__link">
      ods.ai
    </a></dd></dl> <dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Численность</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap">
    5 001–10 000 человек
  </dd></dl> <dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Дата регистрации</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap"><time datetime="2017-02-16T19:05:46.000Z" title="2017-02-16, 22:05">16  февраля  2017</time></dd></dl> <!----></div></div> <!----></section> <div class="tm-company-widgets"></div> <!----></div></div></div></div></div></div></main> <!----></div> <div class="tm-footer-menu"><div class="tm-page-width"><div class="tm-footer-menu__container"><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Ваш аккаунт
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="/kek/v1/auth/habrahabr/?back=/ru/company/ods/blog/452894/&amp;hl=ru" rel="nofollow" target="_self">
                Войти
              </a></li><li class="tm-footer-menu__list-item"><a href="/kek/v1/auth/habrahabr-register/?back=/ru/company/ods/blog/452894/&amp;hl=ru" rel="nofollow" target="_self">
                Регистрация
              </a></li></ul></div></div><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Разделы
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="/ru/" class="footer-menu__item-link router-link-active">
                Публикации
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/news/" class="footer-menu__item-link">
                Новости
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/hubs/" class="footer-menu__item-link">
                Хабы
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/companies/" class="footer-menu__item-link">
                Компании
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/users/" class="footer-menu__item-link">
                Авторы
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/sandbox/" class="footer-menu__item-link">
                Песочница
              </a></li></ul></div></div><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Информация
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="/ru/docs/help/" class="footer-menu__item-link">
                Устройство сайта
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/authors/codex/" class="footer-menu__item-link">
                Для авторов
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/companies/corpblogs/" class="footer-menu__item-link">
                Для компаний
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/docs/transparency/" class="footer-menu__item-link">
                Документы
              </a></li><li class="tm-footer-menu__list-item"><a href="https://account.habr.com/info/agreement" target="_blank">
                Соглашение
              </a></li><li class="tm-footer-menu__list-item"><a href="https://account.habr.com/info/confidential/" target="_blank">
                Конфиденциальность
              </a></li></ul></div></div><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Услуги
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="https://docs.google.com/presentation/d/e/2PACX-1vQLwRfQmXibiUlWaRg-BAc38s7oM3lJiaPju7qmdJsp8ysIvZ_G-Npem0njJLMozE2bPHMpDqiI5hhy/pub?start=false&amp;loop=false&amp;delayms=60000&amp;slide=id.g91a03369cd_4_297" target="_blank">
                Реклама
              </a></li><li class="tm-footer-menu__list-item"><a href="https://habrastorage.org/storage/stuff/habr/service_price.pdf" target="_blank">
                Тарифы
              </a></li><li class="tm-footer-menu__list-item"><a href="https://docs.google.com/presentation/d/e/2PACX-1vQJJds8-Di7BQSP_guHxICN7woVYoN5NP_22ra-BIo4bqnTT9FR6fB-Ku2P0AoRpX0Ds-LRkDeAoD8F/pub?start=false&amp;loop=false&amp;delayms=60000" target="_blank">
                Контент
              </a></li><li class="tm-footer-menu__list-item"><a href="https://tmtm.timepad.ru/" target="_blank">
                Семинары
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/megaprojects/" class="footer-menu__item-link">
                Мегапроекты
              </a></li></ul></div></div></div></div></div> <div class="tm-footer"><div class="tm-page-width"><div class="tm-footer__container"><!----> <div class="tm-footer__social"><a href="https://www.facebook.com/habrahabr.ru" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Facebook</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-facebook"></use></svg></a><a href="https://twitter.com/habr_com" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Twitter</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-twitter"></use></svg></a><a href="https://vk.com/habr" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>VK</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-vkontakte"></use></svg></a><a href="https://telegram.me/habr_com" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Telegram</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-telegram"></use></svg></a><a href="https://www.youtube.com/channel/UCd_sTwKqVrweTt4oAKY5y4w" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Youtube</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-youtube"></use></svg></a><a href="https://zen.yandex.ru/habr" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Яндекс Дзен</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-zen"></use></svg></a></div> <DIV class="v-portal" style="display:none;"></DIV> <button class="tm-footer__link"><!---->
        Настройка языка
      </button> <a href="/ru/about" class="tm-footer__link">
        О сайте
      </a> <a href="/ru/feedback/" class="tm-footer__link">
        Техническая поддержка
      </a> <!----> <a href="/berserk-mode-nope" class="tm-footer__link">
        Вернуться на старую версию
      </a> <div class="tm-footer-copyright"><span class="tm-copyright"><span class="tm-copyright__years">© 2006–2021 </span> <span class="tm-copyright__name">«<a href="https://company.habr.com/" rel="noopener" target="_blank" class="tm-copyright__link">Habr</a>»</span></span></div></div></div></div> <!----> <!----></div> <div class="vue-portal-target"></div></div>
<script>window.__INITIAL_STATE__={"adblock":{"hasAcceptableAdsFilter":false,"hasAdblock":false},"articlesList":{"articlesList":{"452894":{"id":"452894","timePublished":"2019-05-23T12:22:31+00:00","isCorporative":true,"lang":"ru","titleHtml":"Face Anti-Spoofing или технологично узнаём обманщика из тысячи по лицу","leadData":{"textHtml":"\u003Cp\u003EБиометрическая идентификация человека – это одна из самых старых идей для распознавания людей, которую вообще попытались технически осуществить. Пароли можно украсть, подсмотреть, забыть, ключи – подделать. А вот уникальные характеристики самого человека подделать и потерять намного труднее. Это могут быть отпечатки пальцев, голос, рисунок сосудов сетчатки глаза, походка и прочее.\u003C\u002Fp\u003E\u003Cbr\u003E\r\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fh4\u002Fwd\u002Fzr\u002Fh4wdzrqvlnvfy8da0k3cn5589hg.jpeg\"\u003E\u003C\u002Fp\u003E\u003Cbr\u003E\r\n\u003Cp\u003EКонечно же, системы биометрии пытаются обмануть! Вот об этом мы сегодня и поговорим. Как злоумышленники пытаются обойти системы распознавания лица, выдав себя за другого человека и каким образом это можно обнаружить.\u003C\u002Fp\u003E","imageUrl":null,"buttonTextHtml":"Читать дальше →","image":null},"editorVersion":"1.0","postType":"article","postLabels":[],"author":{"scoreStats":{"score":27,"votesCount":27},"rating":0,"relatedData":null,"contacts":[],"authorContacts":[],"paymentDetails":{"paymentYandexMoney":null,"paymentPayPalMe":null,"paymentWebmoney":null},"id":"1838051","alias":"evgeniimakarov","fullname":null,"avatarUrl":null,"speciality":null},"statistics":{"commentsCount":18,"favoritesCount":80,"readingCount":20728,"score":65,"votesCount":67},"hubs":[{"relatedData":null,"id":"20962","alias":"ods","type":"corporative","title":"Блог компании Open Data Science","titleHtml":"Блог компании Open Data Science","isProfiled":false},{"relatedData":null,"id":"50","alias":"infosecurity","type":"collective","title":"Информационная безопасность","titleHtml":"Информационная безопасность","isProfiled":true},{"relatedData":null,"id":"19439","alias":"machine_learning","type":"collective","title":"Машинное обучение","titleHtml":"Машинное обучение","isProfiled":true},{"relatedData":null,"id":"21910","alias":"popular_science","type":"collective","title":"Научно-популярное","titleHtml":"Научно-популярное","isProfiled":false},{"relatedData":null,"id":"21922","alias":"artificial_intelligence","type":"collective","title":"Искусственный интеллект","titleHtml":"Искусственный интеллект","isProfiled":false}],"flows":[{"id":"1","alias":"develop","title":"Разработка"},{"id":"7","alias":"popsci","title":"Научпоп"}],"relatedData":null,"textHtml":"\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\"\u003E\u003Cp\u003EБиометрическая идентификация человека – это одна из самых старых идей для распознавания людей, которую вообще попытались технически осуществить. Пароли можно украсть, подсмотреть, забыть, ключи – подделать. А вот уникальные характеристики самого человека подделать и потерять намного труднее. Это могут быть отпечатки пальцев, голос, рисунок сосудов сетчатки глаза, походка и прочее.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fwebt\u002Fh4\u002Fwd\u002Fzr\u002Fh4wdzrqvlnvfy8da0k3cn5589hg.jpeg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fh4\u002Fwd\u002Fzr\u002Fh4wdzrqvlnvfy8da0k3cn5589hg.jpeg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EКонечно же, системы биометрии пытаются обмануть! Вот об этом мы сегодня и поговорим. Как злоумышленники пытаются обойти системы распознавания лица, выдав себя за другого человека и каким образом это можно обнаружить.\u003C\u002Fp\u003E\u003Ca name=\"habracut\"\u003E\u003C\u002Fa\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EВидео-версию этого рассказа можно посмотреть \u003Ca href=\"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=Y4BMi38yRLo\"\u003Eтут\u003C\u002Fa\u003E, а тех, кто предпочитает чтение просмотру, приглашаю проследовать дальше\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EСогласно представлениям режиссеров Голливуда и писателей-фантастов, обмануть биометрическую идентификацию довольно просто. Нужно всего лишь предъявить системе «требуемые части» настоящего пользователя, как по отдельности, так и взяв его в заложники целиком. Или же можно “надеть личину” другого человека на себя, например, с помощью \u003Ca href=\"https:\u002F\u002Fwww.kinopoisk.ru\u002Ffilm\u002Fbez-litsa-1997-4606\"\u003Eфизической пересадки маски\u003C\u002Fa\u003E или вообще, \u003Ca href=\"https:\u002F\u002Fwww.kinopoisk.ru\u002Ffilm\u002Fgattaka-1997-5012\"\u003Eпредъявления фальшивых генетических признаков\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EВ реальной жизни злоумышленники тоже пытаются представиться кем-то другим. Например, ограбить банк, надев маску чернокожего мужчины, как на картинке ниже.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fpl\u002Fvz\u002F67\u002Fplvz67jfgazkigzqsdhkko62ego.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EРаспознавание по лицу выглядит очень перспективным направлением для использования в мобильном секторе. Если к использованию отпечатков пальцев все уже давно привыкли, а технологии работы с голосом постепенно и довольно предсказуемо развиваются, то с идентификацией по лицу ситуация сложилась довольно необычная и достойная небольшого экскурса в историю вопроса.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Ch2 id=\"kak-vse-nachinalos-ili-iz-fantastiki-v-realnost\"\u003EКак все начиналось или из фантастики в реальность\u003C\u002Fh2\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EСегодняшние системы распознавания демонстрируют огромную точность. С появлением больших наборов данных и сложных архитектур стало возможным добиться точности распознавания лица вплоть до 0,000001 (одна ошибка на миллион!) и они уже сейчас пригодны для переноса на мобильные платформы. Узким местом стала их уязвимость.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EДля того, чтобы выдать себя за другого человека в нашей технической реальности, а не в фильме, чаще всего используют маски. Компьютерную систему тоже пытаются одурачить, представив вместо своего лица чье-то еще. Маски бывают совершенно разного качества, от распечатанного на принтере фото другого человека, которое держат перед лицом, до очень сложных трехмерных масок с подогревом. Маски могут как предъявляться отдельно в виде листа или экрана, так и надеваться на голову. \u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EБольшое внимание к теме привлекла успешная попытка обмануть систему Face ID на iPhone X с помощью довольно сложной маски из каменного порошка со специальными вставками вокруг глаз, имитирующими тепло живого лица с помощью инфракрасного излучения.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fxl\u002Fkq\u002Fpr\u002Fxlkqprysss0ce2arbqm0sw_xbpo.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EУтверждается, что помощью такой маски удалось обмануть Face ID на iPhone X. Видео и немного текста можно найти \u003Ca href=\"http:\u002F\u002Fwww.bkav.com\u002Fd\u002Ftop-news\u002F-\u002Fview_content\u002Fcontent\u002F103968\u002Fbkav%92s-new-mask-beats-face-id-in-twin-way-severity-level-raised-do-not-use-face-id-in-business-transactions\"\u003Eздесь\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EНаличие таких уязвимостей очень опасно для банковских или государственных систем аутентификации пользователя по лицу, где проникновение злоумышленника влечет за собой значительные потери.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Ch2 id=\"terminologiya\"\u003EТерминология\u003C\u002Fh2\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EОбласть исследования face anti-spoofing довольно новая и пока еще не может похвастаться даже сложившейся терминологией.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EУсловимся называть попытку обмана системы идентификации путем предъявления ей поддельного биометрического параметра (в данном случае — лица) \u003Cstrong\u003Espoofing attack\u003C\u002Fstrong\u003E.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EСоответственно, комплекс защитных мер, чтобы противостоять такому обману, будем называть \u003Cstrong\u003Eanti-spoofing\u003C\u002Fstrong\u003E. Он может быть реализован в виде самых разных технологий и алгоритмов, встраиваемых в конвейер системы идентификации.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EВ \u003Ca href=\"https:\u002F\u002Fwww.iso.org\u002Fobp\u002Fui\u002F#iso:std:iso-iec:30107:-3:ed-1:v1:en\"\u003EISO\u003C\u002Fa\u003E предлагается несколько расширенный набор терминологии, с такими терминами, как \u003Cstrong\u003Epresentation attack\u003C\u002Fstrong\u003E — попытки заставить систему неверно идентифицировать пользователя или дать ему возможность избежать идентификации, с помощью демонстрации картинки, записанного видео и так далее. \u003Cstrong\u003ENormal (Bona Fide)\u003C\u002Fstrong\u003E – соответствует обычному алгоритму работы системы, то есть всему, что НЕ является атакой. \u003Cstrong\u003EPresentation attack instrument\u003C\u002Fstrong\u003E означает средство совершения атаки, например, искусственно изготовленную часть тела. И, наконец, \u003Cstrong\u003EPresentation attack detection\u003C\u002Fstrong\u003E — автоматизированные средства обнаружения таких атак. Впрочем, сами стандарты все еще находятся в разработке, поэтому говорить о каких-либо устоявшихся понятиях нельзя. Терминология на русском языке отсутствует почти полностью.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EДля определения качества работы системы часто пользуются метрикой \u003Cstrong\u003EHTER\u003C\u002Fstrong\u003E (Half-Total Error Rate – половина полной ошибки), которую вычисляют в виде суммы коэффициентов ошибочно разрешенных идентификаций (FAR – False Acceptance Rate) и ошибочно запрещенных идентификаций (FRR – False Rejection Rate), деленной пополам.\u003Cbr\u002F\u003E\r\nHTER=(FAR+FRR)\u002F2\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EСтоит сказать, что в системах биометрии обычно самое большое внимание уделяют FAR, с целью сделать всё возможное, чтобы не допустить злоумышленника в систему. И добиваются в этом неплохих успехов (помните одну миллионную из начала статьи?) Обратной стороной оказывается неизбежное возрастание FRR – количества обычных пользователей, ошибочно классифицированных как злоумышленников. Если для государственных, оборонных и прочих подобных систем этим можно пожертвовать, то мобильные технологии, работающие с их огромными масштабами, разнообразием абонентских устройств и, вообще, user-perspective ориентированные, очень чувствительны к любым факторам, которые могут заставить пользователей отказаться от услуг. Если вы хотите уменьшить количество разбитых об стену телефонов после десятого подряд отказа в идентификации, стоит обратить внимание на FRR!\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Ch2 id=\"vidy-atak-obmanyvaem-sistemu\"\u003EВиды атак. Обманываем систему\u003C\u002Fh2\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fdt\u002Fjx\u002Flp\u002Fdtjxlptnxcphiicevd6lbhlzzpa.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EДавайте, наконец, узнаем, как именно злоумышленники обманывают системы распознавания, а также как этому можно противопоставить.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EСамым популярным средством обмана являются маски. Нет ничего более очевидного, чем надеть маску другого человека и представить лицо системе идентификации (часто именуется Mask attack).\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fq2\u002Ftr\u002Fnl\u002Fq2trnl3zhhuftbggl3vx-sfqoyw.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EЕще можно распечатать фото себя или кого-то еще на листе бумаге и поднести его к камере (условимся называть такой тип атаки Printed attack). \u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Ff9\u002Fg7\u002Fds\u002Ff9g7dsc81eiffu9srctasf2udcm.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EЧуть более сложной является Replay attack, когда системе предъявляют экран другого устройства, на котором воспроизводится заранее записанное видео с другим человеком. Сложность исполнения компенсируется высокой эффективностью такой атаки, поскольку системы контроля часто используют признаки, основанные на анализе временных последовательностей, например, отслеживание моргания, микродвижений головы, наличие мимики, дыхания и так далее. Все это можно легко воспроизвести на видео.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fde\u002Fdd\u002Fvj\u002Fdeddvj6xvnwbv6dpbryxphjwaa8.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EОба типа атак имеют ряд характерных признаков, позволяющих их обнаружить, и, таким образом, отличить экран планшета или лист бумаги от реального лица.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EСведем характерные признаки, позволяющие определить эти два типа атак, в таблицу:\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cdiv class=\"scrollable-table\"\u003E\u003Ctable\u003E\r\n\u003Cthead\u003E\r\n\u003Ctr\u003E\r\n\u003Cth\u003E\u003Cstrong\u003EPrinted attack\u003C\u002Fstrong\u003E\u003C\u002Fth\u003E\r\n\u003Cth\u003E\u003Cstrong\u003EReplay attack\u003C\u002Fstrong\u003E\u003C\u002Fth\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003C\u002Fthead\u003E\r\n\u003Ctbody\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003EСнижение качества текстуры изображения при печати\u003C\u002Ftd\u003E\r\n\u003Ctd\u003EМуар\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003EАртефакты передачи полутонового изображения при печати на принтере\u003C\u002Ftd\u003E\r\n\u003Ctd\u003EОтражения (блики)\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003EМеханические артефакты печати (горизонтальные линии)\u003C\u002Ftd\u003E\r\n\u003Ctd\u003EПлоская картинка (отсутствие глубины)\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003EОтсутствие локальных движений (например, морганий)\u003C\u002Ftd\u003E\r\n\u003Ctd\u003EМогут быть видны границы изображения\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003EМогут быть видны границы изображения\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003C\u002Ftbody\u003E\r\n\u003C\u002Ftable\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Ch2 id=\"algoritmy-obnaruzheniya-atak-staraya-dobraya-klassika\"\u003EАлгоритмы обнаружения атак. Старая добрая классика\u003C\u002Fh2\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002F1r\u002Fn8\u002F-e\u002F1rn8-elrfhm1q0ud9q4jbaukidq.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EОдин из самых старых подходов (работы 2007, 2008 годов) основан на обнаружении морганий человека путем анализа изображения по маске. Смысл заключается в построении какого-либо бинарного классификатора, позволяющего выделить изображения с открытыми и закрытыми глазами в последовательности кадров. Это может быть анализ видеопотока с помощью выделения частей лица (landmark detection), или же использование какой-то простой нейронной сети. И на сегодняшний день чаще всего используется этот метод; пользователю предлагают выполнить какую-то последовательность действий: покрутить головой, подмигнуть, улыбнуться и прочее. Если последовательность случайна, подготовиться к ней злоумышленнику заранее непросто. К сожалению, для честного пользователя этот квест тоже не всегда преодолим, и вовлеченность резко падает.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002F4s\u002Ffb\u002Fvv\u002F4sfbvvforopuonls_ochr8dpzjg.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EЕще можно использовать особенности ухудшения качества картинки при печати или воспроизведении на экране. Скорее всего, на изображении будут обнаружены даже какие-то локальные паттерны, пусть и неуловимые глазом. Это можно сделать, например, посчитав локальные бинарные паттерны (LBP, local binary pattern) для различных зон лица после выделения его из кадра (\u003Ca href=\"https:\u002F\u002Fpublications.idiap.ch\u002Fdownloads\u002Fpapers\u002F2012\u002FChingovska_IEEEBIOSIG2012_2012.pdf\"\u003EPDF\u003C\u002Fa\u003E). Описанную систему можно считать основоположником всего направления алгоритмов face anti-spoofing на основе анализа изображения. В двух словах, при расчете LBP последовательно берется каждый пиксель изображения, восемь его соседей и сравнивается их интенсивность. Если интенсивность больше, чем на центральном пикселе, присваивается единица, если меньше – ноль. Таким образом, для каждого пикселя получается 8-битовая последовательность. По полученным последовательностям строится попиксельная гистограмма, которая подается на вход SVM-классификатора.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fqu\u002Fx_\u002Fvv\u002Fqux_vvmfk7ikww7ojknhq_mpbhu.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EЛокальные бинарные паттерны, гистограммирование и SVM. Приобщиться к неустаревающей классике можно по \u003Ca href=\"https:\u002F\u002Fpublications.idiap.ch\u002Fdownloads\u002Fpapers\u002F2012\u002FChingovska_IEEEBIOSIG2012_2012.pdf\"\u003Eссылке\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EПоказатель эффективности HTER составляет «целых» 15%, и означает, что значительная часть злоумышленников преодолевает защиту без особых усилий, хотя и следует признать что множество и отсеивается. Алгоритм тестировался на наборе данных \u003Ca href=\"https:\u002F\u002Fwww.idiap.ch\u002Fdatase\"\u003EReplay-Attack\u003C\u002Fa\u003E от IDIAP, который составлен из 1200 коротких видео 50 респондентов и трех видов атак – printed attack, mobile attack, high-definition attack.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EИдеи анализа текстуры изображения получили продолжение. В 2015 году Букинафит \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1511.06316\"\u003Eразработал\u003C\u002Fa\u003E алгоритм альтернативного разбиения изображения на каналы, помимо традиционного RGB, для результатов которого снова подсчитывались локальные бинарные паттерны, которые, как и в предыдущем способе, подавались на вход SVN классификатора. Точность HTER, рассчитанная на датасетах CASIA и Replay-Attack, составила впечатляющие на тот момент 3%.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002F7j\u002Fvy\u002Fi7\u002F7jvyi7inhsl5g2ulp05v4luido8.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EВ это же время появились работы по обнаружению муара. Пател \u003Ca href=\"http:\u002F\u002Fbiometrics.cse.msu.edu\u002FPublications\u002FFace\u002FPatelHanJainOtt_LivevsSpoofFaceVideo_ICB15.pdf\"\u003Eопубликовал\u003C\u002Fa\u003E статью, где предложил искать артефакты изображения в виде периодического узора, вызванные наложением двух разверток. Подход оказался работоспособным, показав HTER около 6% на наборах данных IDIAP, CASIA и RAFS. Это также было первой попыткой сравнить эффективность работы алгоритма на различных наборах данных.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002F98\u002Fm2\u002Fls\u002F98m2ls-svv7gxfaagripfrhzymo.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EПериодический узор на изображении, вызванный наложением разверток\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EЧтобы обнаружить попытки предъявления фото, логичным решением было попытаться анализировать не одно изображение, а их последовательность, взятую из видео потока. Например, Анжос с коллегами \u003Ca href=\"https:\u002F\u002Fpdfs.semanticscholar.org\u002F82fb\u002Ff7fe9b2114b87765e2aa62204b98a44dc89b.pdf\"\u003Eпредложили\u003C\u002Fa\u003E выделять признаки из оптического потока на соседних парах кадров, подавать на вход бинарного классификатора и усреднять результаты. Подход оказался достаточно эффективным, продемонстрировав HTER 1,52% на их собственном наборе данных.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fih\u002F9e\u002Fvo\u002Fih9evob9a5hmtjekxhx1iwsgxm8.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EИнтересным выглядит метод отслеживания движений, находящийся несколько в стороне от общепринятых подходов. Так как в 2013 году обычного для современных проектов в области глубокого обучения принципа «подать сырое изображение на вход сверточной сети и настраивать слои сетки до получения результата» не было, Бхарадваж последовательно \u003Ca href=\"http:\u002F\u002Fiab-rubric.org\u002Fpapers\u002FPID2777141.pdf\"\u003Eприменил\u003C\u002Fa\u003E более сложные предварительные преобразования. В частности, он применил известный по работам ученых из MIT алгоритм эйлеровского усиления видео \u003Ca href=\"http:\u002F\u002Fpeople.csail.mit.edu\u002Fmrub\u002Fevm\u002F\"\u003EEulerian video magnification\u003C\u002Fa\u003E, который с успехом применялся для анализа цветовых изменений кожного покрова в зависимости от пульса. Заменил LBP на HOOF (гистограммы направлений оптического потока), верно заметив, что коль скоро мы хотим отслеживать движения, и признаки нам нужны соответствующие, а не просто анализ текстур. В качестве классификатора использовался все тот же SVM, традиционный на тот момент. Алгоритм показал крайне впечатляющие результаты на датасетах Print Attack (0%) и Replay Attack (1,25%)\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002F-i\u002F1w\u002Fw2\u002F-i1ww2rsu0kqmxglhh-on8lnr5g.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Ch2 id=\"davayte-uzhe-uchit-setki\"\u003EДавайте уже учить сетки!\u003C\u002Fh2\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002F7p\u002Ft-\u002Fyk\u002F7pt-ykbzzuyb7bzpnd9qpjodfog.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EС какого-то момента стало очевидно, что назрел переход к глубокому обучению. Пресловутая «революция глубокого обучения» настигла и face anti-spoofing.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E«Первой ласточкой» можно считать метод анализа карт глубины на отдельных участках («патчах») изображения. Очевидно, карта глубины является очень хорошим признаком для определения плоскости, в которой расположено изображение. Хотя бы потому что у изображения на листе бумаги «глубины» нет по определению. В работе \u003Ca href=\"http:\u002F\u002Fcvlab.cse.msu.edu\u002Fpdfs\u002FFaceAntiSpoofingUsingPatchandDepthBasedCNNs.pdf\"\u003EАтаума\u003C\u002Fa\u003E 2017 года из изображения извлекалось множество отдельных небольших участков, для них рассчитывались карты глубины, которые затем сливались с картой глубины основного изображения. При этом указывалось, что десяти случайных патчей изображения лица достаточно для надежного определения Printed Attack. Дополнительно авторы сливали вместе результаты работы двух сверточных нейросетей, первая из которых рассчитывала карты глубины для патчей, а вторая – для изображения в целом. При обучении на наборах данных с классом Printed Attack связывалась карта глубины, равная нулю, а с трехмерной моделью лица – серия случайно отбираемых участков. По большому счету, сама по себе карта глубины была не так важна, от нее использовалась лишь некоторая индикаторная функция, характеризующая «глубину участка». Алгоритм показал значение HTER 3,78%. Для обучения были использованы три публичных набора данных — CASIA-MFSD, MSU-USSA и Replay-Attack.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fa7\u002Fbl\u002Fdf\u002Fa7bldfybkgzeeptdtomphvq1kwk.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EК сожалению, доступность большого количества прекрасных фреймворков для глубокого обучения привело к появлению огромного количества разработчиков, которые пытаются «в лоб» решить задачу face anti-spoofing хорошо знакомым способом ансамблирования нейросетей. Обычно это выглядит как стек карт признаков на выходах нескольких сетей, предобученных на каком-либо широко распространенном датасете, который подается на бинарный классификатор. \u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fek\u002F8b\u002Frc\u002Fek8brc52akg2y3zfy6ejmbr-taq.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EВ целом стоит заключить, что к настоящему моменту опубликовано довольно много работ, которые в целом демонстрируют неплохие результаты, и которые объединяет всего одно небольшое «но». Все эти результаты продемонстрированы в рамках одного конкретного датасета! Ситуация усугубляется ограниченностью имеющихся наборов данных и, например, на пресловутом Replay-Attack уже никого не удивить HTER 0%. Все это приводит к появлению очень сложных архитектур, например, вот \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1808.08802\"\u003Eтаких\u003C\u002Fa\u003E, с использованием различных мудрёных признаков, вспомогательных алгоритмов, собранных в стек, с несколькими классификаторами, результаты которых усредняются и так далее… На выходе авторы получают HTER =0,04%!\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Faw\u002F_k\u002Fs_\u002Faw_ks_6-xp40fyjz97ue9mwzp8q.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EЭто наводит на мысль о том, что задача face anti-spoofing в рамках конкретного датасета решена. Сведем в таблицу различные современные методы на основе нейросетей. Как легко увидеть, «эталонных результатов» удалось достигнуть очень разнообразными методами, которые только возникли в пытливых умах разработчиков.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fbb\u002F7v\u002F73\u002Fbb7v73awyzwhxeyyvtejgm3skh4.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EСравнительные результаты различных алгоритмов. Таблица взята \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1710.09868v2.pdf\"\u003Eотсюда\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EК сожалению, благостную картину борьбы за десятые доли процента нарушает все тот же «маленький» фактор. Если попытаться обучить нейросеть на одном наборе данных, а применить – на другом, то результаты окажутся… не столь оптимистичными. Хуже того, попытки применить классификаторы в реальной жизни не оставляют и вовсе никакой надежды.\u003Cbr\u002F\u003E\r\nДля примера, возьмем данные \u003Ca href=\"http:\u002F\u002Fvipl.ict.ac.cn\u002Fuploadfile\u002Fupload\u002F2017020711092984.pdf\"\u003Eработы\u003C\u002Fa\u003E 2015 года, где для определения подлинности предъявленного изображения использовалась метрика его качества. Взгляните сами:\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fjw\u002Fb5\u002F-n\u002Fjwb5-naohhjhek2tkmakbssjyik.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EИными словами, алгоритм, натренированный на данных Idiap, а примененный на MSU, даст коэффициент истинно положительных обнаружений 90,5%, а, если сделать наоборот (обучить на MSU, а проверить – на Idiap), то верно удастся определить только 47,2%(!) Для других сочетаний ситуация ухудшается еще больше, и, например, если натренировать алгоритм на MSU, а проверить – на CASIA, то TPR составит 10,8%! Это означает, что к атакующим было ошибочно причислено огромное количество честных пользователей, что не может не удручать. Ситуацию не смогло переломить даже cross-database обучение, что вроде бы кажется вполне разумным выходом из положения. \u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EПосмотрим еще. Результаты, приведенные в \u003Ca href=\"http:\u002F\u002Fbiometrics.cse.msu.edu\u002FPublications\u002FFace\u002FPatelHanJain_FaceAntispoofing_CCBR2016.pdf\"\u003Eстатье\u003C\u002Fa\u003E Патела 2016 года, показывают, что даже при достаточно сложных конвейерах обработки и выделении таких надежных признаков, как моргание и текстура, результаты на незнакомых наборах данными не могут считаться удовлетворительными. Итак, в какой-то момент стало вполне очевидно, что предложенных способов отчаянно не хватает для обобщения результатов.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fvf\u002Flz\u002Fol\u002Fvflzoljiplnkj_vgm-zth9a0sdc.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Ch2 id=\"a-esli-ustroit-sorevnovanie\"\u003EА если устроить соревнование…\u003C\u002Fh2\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EКонечно же, в области face anti-spoofing не обошлось без соревнований. В 2017 году в университете Оулу в Финляндии состоялся конкурс на собственном новом наборе данных с достаточно интересными протоколами, ориентированными, как раз, на использование в области мобильных приложений.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E-Протокол 1: Имеется разница в освещении и фоне. Наборы данных записаны в различных местах и отличаются фоном и освещением.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E-Протокол 2: Для атак использованы различные модели принтеров и экранов. Так, в проверочном наборе данных использована техника, которая не встречается в обучающем наборе\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E-Протокол 3: Взаимозаменяемость датчиков. Видео настоящего пользователя и атак записываются на пять различных смартфонов и используются в наборе данных для обучения. Для проверки алгоритма используется видео с еще одного смартфона, который в обучающем наборе не включен.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E-Протокол 4: включает все вышеуказанные факторы.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EРезультаты оказались достаточно неожиданными. Как и в любом соревновании, времени придумывать гениальные идеи не было, поэтому практически все участники взяли знакомые архитектуры и доработали их тонкой настройкой, работой с признаками и попытками как-то использовать для обучения другие наборы данных. Призовое решение показало ошибку на четвертом, самом сложном протоколе, около 10%. Краткое описание алгоритмов победителей в таблице чуть ниже:\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Col\u003E\r\n\u003Cli\u003E\u003Cp\u003EGRADIENT\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cul\u003E\r\n\u003Cli\u003EВыполняется слияние признаков по цвету (используя цветовые пространства HSV и YCbCr), текстуре и движению.\u003C\u002Fli\u003E\r\n\u003Cli\u003EИнформация о динамике извлекается по данной последовательности видео и картам изменений по времени в отдельном кадре.\u003C\u002Fli\u003E\r\n\u003Cli\u003EЭта последовательность раздельно применяется по всем каналам в цветовых пространствах HSV и YCbCr, дающих вместе пару трехканальных изображений. Для каждого изображения ROI (region-of-interest) обрезается на основе положения глаз в последовательности кадров и масштабируется до 160×160 пикселей..\u003C\u002Fli\u003E\r\n\u003Cli\u003EКаждая ROI делится на 3×3 и 5×5 прямоугольных областей, по которым извлекаются равномерные LBP гистограммы признаков, которые объединяются в два вектора признаков размерностью 6018.\u003C\u002Fli\u003E\r\n\u003Cli\u003EС помощью рекурсивного удаления признаков (Recursive Feature Elimination) размерность уменьшается с 6018 до 1000. \u003C\u002Fli\u003E\r\n\u003Cli\u003EДля каждого вектора признаков выполняется классификация на основе SVM с последующим усреднением.|\u003C\u002Fli\u003E\r\n\u003C\u002Ful\u003E\u003Cbr\u002F\u003E\r\n\u003C\u002Fli\u003E\r\n\u003Cli\u003E\u003Cp\u003ESZCVI\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cul\u003E\r\n\u003Cli\u003EИз каждого видео извлекается выборка кадров, берется каждый шестой\u003C\u002Fli\u003E\r\n\u003Cli\u003EМасштабирование кадров до 216×384\u003C\u002Fli\u003E\r\n\u003Cli\u003EПять VGG-подобных слоев\u003C\u002Fli\u003E\r\n\u003Cli\u003EРезультаты отдельных кадров внутри выборки усредняются\u003C\u002Fli\u003E\r\n\u003C\u002Ful\u003E\u003Cbr\u002F\u003E\r\n\u003C\u002Fli\u003E\r\n\u003Cli\u003E\u003Cp\u003ERecod\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cul\u003E\r\n\u003Cli\u003ESqueezeNet обучается на Imagenet\u003C\u002Fli\u003E\r\n\u003Cli\u003ETransfer learning на двух наборах данных: CASIA и UVAD\u003C\u002Fli\u003E\r\n\u003Cli\u003EСначала лицо обнаруживается и масштабируется до 224×224 pixels. Из каждого видео обучающего датасета извлекается, примерно, каждый седьмой кадр, который направляется на десять CNN.\u003C\u002Fli\u003E\r\n\u003Cli\u003EДля получения итогового результата показатели отдельных кадров усредняются.\u003C\u002Fli\u003E\r\n\u003Cli\u003EДля улучшения эффективности полученные показатели сводятся в обобщенный результат базового метода\u003C\u002Fli\u003E\r\n\u003C\u002Ful\u003E\u003Cbr\u002F\u003E\r\n\u003C\u002Fli\u003E\r\n\u003Cli\u003E\u003Cp\u003ECPqD\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cul\u003E\r\n\u003Cli\u003EСеть Inception-v3, обученная на ImageNet\u003C\u002Fli\u003E\r\n\u003Cli\u003ECигмоидная функция активации\u003C\u002Fli\u003E\r\n\u003Cli\u003EНа основании определения положения глаз выполняется обрезка участков изображения, содержащих лицо, которые затем масштабируются до кадров 224×224 RGB |\u003C\u002Fli\u003E\r\n\u003C\u002Ful\u003E\u003Cbr\u002F\u003E\r\n\u003C\u002Fli\u003E\r\n\u003C\u002Fol\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EХорошо видно, что новых идей появилось не так много. Все те же LBP, предобученные сетки, анализ текстуры и цвета, попарный анализ кадров и т.д. GRADIANT выглядит наиболее грамотно спроектированным с системной точки зрения, в нем смешиваются различные признаки, идет работа в различных цветовых пространствах, проводится чистка признаков. Он и победил в соревновании.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EСоревнование очень ярко показало существующие ограничения. В первую очередь, это ограниченность и несблансированность существующих датасетов для обучения. Во-первых, в них представлено довольно ограниченное количество людей (от 15 человек в NUAA до 1140 в MSU-USSA) и сессий, разнице внешнего освещения, выражениям лица, применяемым устройствам записи, углам съемки и видам атак. При этом в реальных условиях модель камеры, качество матрицы, условия съемки, фокусное расстояние и выдержка, фон и обстановка часто оказываются определяющими для анализа изображений. Во-вторых, сами методы анализа гораздо больше ориентированы на анализ отдельных участков изображения без существенной обработки самой обстановки сцены. Например, в наборе CASIA множество примеров атак представлены в виде изображения человека, который держит перед лицом фотографию. Очевидно, что видно характерное положение рук, границы листа с фото, могут быть видны волосы, шея и голова и так далее… Но решений, использующих анализ всей сцены и положения человека, представлено не было, все алгоритмы работали только с выделенным из всей сцены участком лица.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fur\u002Fyf\u002Ft8\u002Furyft8082ylvw-1kbyvcxucjcem.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EНедавно был предложен еще один многообещающий \u003Ca href=\"https:\u002F\u002Fdatasouls.com\u002Fc\u002Fidrnd-antispoof\u002Fdescription\"\u003Eконкурс\u003C\u002Fa\u003E на новом наборе данных собственной разработки размером 30 Гб. Согласно условиям конкурса, должно быть выполнено обнаружение надетой на лицо маски, факта съемки распечатанной фотографии и предъявления видеозаписи на экране вместо настоящего лица. Вполне вероятно, что по его результатам мы и увидим концептуально новое решение.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EКонечно, есть решения, основанные на «нестандартных подходах». Перейдем к ним с надеждой на улучшение текущего положения дел. Например, было \u003Ca href=\"http:\u002F\u002Fopenaccess.thecvf.com\u002Fcontent_ECCV_2018\u002Fpapers\u002FSiqi_Liu_Remote_Photoplethysmography_Correspondence_ECCV_2018_paper.pdf\"\u003Eпредложено\u003C\u002Fa\u003E воспользоваться методом дистанционной фотоплетизмографии (rPPG – remote photoplethysmography), позволяющим обнаружить биение пульса человека по видеоизображению. Идея состоит в том, что при попадании света на живое лицо человека часть света отразится, часть-рассеется, а часть – поглощается кожей и тканями лица. При этом картина будет разной в зависимости от степени наполненности тканей кровью. Таким образом, можно отследить пульсацию крови в сосудах лица и, соответственно, обнаружить пульс. Конечно, если закрыть лицо маской или предъявить экран телефона, никакой пульсации обнаружить не получится. На этом принципе Лю с соавторами предложили разбивать изображение лица на участки, детектировать пульс методом дистанционной фотоплетизмографии, попарно сравнивать различные участки для подсчета пульса и строить карты с целью обнаружения наличия или отсутствия маски, а также сравнения пульса на разных участках лица.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fdb\u002Fle\u002F44\u002Fdble44fc8acyqdhzead3ndbkg3a.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fsy\u002Fu7\u002Fvu\u002Fsyu7vucfvb5inhajqoilb6rqxu8.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EРабота показала значение HTER около 10%, подтвердив принципиальную применимость метода. Имеется еще несколько работ, подтверждающих перспективность этого подхода\u003Cbr\u002F\u003E\r\n(CVPR 2018) J. H.-Ortega et al. \u003Ca href=\"http:\u002F\u002Fopenaccess.thecvf.com\u002Fcontent_cvpr_2018_workshops\u002Fpapers\u002Fw11\u002FHernandez-Ortega_Time_Analysis_of_CVPR_2018_paper.pdf\"\u003ETime Analysis of Pulsebased Face Anti-Spoofing in Visible and NIR\u003C\u002Fa\u003E\u003Cbr\u002F\u003E\r\n(2016) X. Li. et al. \u003Ca href=\"https:\u002F\u002Fprojet.liris.cnrs.fr\u002Fimagine\u002Fpub\u002Fproceedings\u002FICPR-2016\u002Fmedia\u002Ffiles\u002F1223.pdf\"\u003EGeneralized face anti-spoofing by detecting pulse from face videos\u003C\u002Fa\u003E\u003Cbr\u002F\u003E\r\n(2016) J. Chen et al. \u003Ca href=\"https:\u002F\u002Fwww.researchgate.net\u002Fpublication\u002F312566385_RealSense_real_heart_rate_Illumination_invariant_heart_rate_estimation_from_videos\"\u003ERealsense = real heart rate: Illumination invariant heart rate estimation from videos\u003C\u002Fa\u003E\u003Cbr\u002F\u003E\r\n(2014) H. E. Tasli et al. \u003Ca href=\"http:\u002F\u002Fwww.vicarvision.nl\u002Fpub\u002FTasli_Gudi_denUyl_RemotePPG_2014.pdf\"\u003ERemote PPG based vital sign measurement using adaptive facial regions\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EВ 2018 году Лю с коллегами из университета Мичигана предложили \u003Ca href=\"http:\u002F\u002Fcvlab.cse.msu.edu\u002Fpdfs\u002FLiu_Jourabloo_Liu_CVPR2018.pdf\"\u003Eотказаться от бинарной классификации\u003C\u002Fa\u003E в пользу подхода, который они назвали “binary supervision” – то есть использование более сложной оценки на основе карты глубины и дистанционной фотоплетизмографии. Для каждого из настоящих изображений лица реконструировали трехмерную модель с помощью \u003Ca href=\"https:\u002F\u002Fgithub.com\u002FYadiraF\u002FPRNet\"\u003Eнейросети\u003C\u002Fa\u003E и назвали ее с картой глубины. Фальшивым изображениям была присвоена карта глубины, состоящая из нулей, в конце концов это ведь просто лист бумаги или экран устройства! Эти характеристики были приняты за «истину», нейросети обучались на собственном наборе данных SiW. Затем, на входное изображение накладывалась трехмерная маска лица, для нее высчитывались карта глубины и пульс, и все это связывалось вместе в довольно сложном конвейере. В итоге, метод показал точность около 10 процентов на конкурсном наборе данных OULU. Интересно, что победитель соревнования, организованного университетом Оулу, построил алгоритм на бинарных паттернах классификации, отслеживании морганий и прочих признаках «конструированных вручную», и его решение тоже имело точность около 10%. Выигрыш составил всего лишь около половины процента! В пользу новой комбинированной технологии говорит то, что алгоритм был обучен на собственном наборе данных, а проверен на OULU, улучшив результат победителя. Что говорит о некоторой переносимости результатов с датасета на датасет, и чем черт не шутит, возможно и на реальную жизнь. Однако, при попытке выполнить обучение на других датасетах – CASIA и ReplayAttack, снова был получен результат около 28%. Конечно, это превосходит показатели других алгоритмах при обучении на различных наборах данных, но при таких значениях точности ни о каком промышленном использовании речи быть не может!\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fqd\u002Fbu\u002Ffn\u002Fqdbufnt8yc9pghj4egkxqaku_3g.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EДругой подход был предложен Вангом с коллегами в свежей \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1811.05118\"\u003Eработе\u003C\u002Fa\u003E 2019 года. Было отмечено, что при анализе микродвижений лица заметны повороты и смещения головы, приводящие к характерному изменению углов и относительных расстояний между признаками на лице. Так при смещении лица в стороны по горизонтали угол между носом и ухом увеличивается. Но, если таким же образом сместить лист бумаги с картинкой, угол уменьшится! Для иллюстрации стоит процитировать рисунок из работы.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fx9\u002Flq\u002Fkz\u002Fx9lqkzuzmgz7gz5ejx1rsiom7wq.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EНа этом принципе авторы построили целый обучаемый блок для переноса данных между слоями нейронной сети. В нем учитывались «неправильные смещения» для каждого кадра в последовательности из двух кадров, и это позволило использовать результаты в следующем блоке анализа долговременных зависимостей на базе GRU \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FGated_recurrent_unit\"\u003EGated Recurrent Unit\u003C\u002Fa\u003E. Затем все признаки конкатенировались, подсчитывалась функция потерь и выполнялась итоговая классификация. Это позволило еще слегка улучшить результат на наборе данных OULU, но проблема зависимости от обучающего данных осталась, поскольку для пары CASIA-MFSD и Replay-Attack показатели составили 17,5 и 24 процента, соответственно.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EПод занавес стоит отметить \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1902.10311.pdf\"\u003Eработу\u003C\u002Fa\u003E специалистов Tencent, предложивших изменить сам способ получения исходного видеоизображения. Вместо пассивного наблюдения за сценой они предложили динамически освещать лицо и считывать отражения. Принцип активного облучения объекта уже давно применяется в локационных системах различного рода, поэтому, его использование для изучения лица выглядит весьма логичным. Очевидно, что для надежной идентификации в самом изображении не хватает признаков, и освещение экрана телефона или планшета последовательностью световых символов (light CAPTCHA по терминологии авторов), может сильно помочь. Далее определяется разница в рассеянии и отражении по паре кадров, и результаты подаются на многозадачную нейронную сеть для дальнейшей обработки по карте глубины и вычисления различных функций потерь. В конце выполняется регрессия нормализованных кадров освещенности. Авторы не анализировали обобщающую способность своего алгоритма на других наборах данных и обучали его на собственном закрытом датасете. Результат составляет порядка 1% и сообщается, что модель уже была развернута для реального использования. \u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fc-\u002Fts\u002Fw8\u002Fc-tsw8vywrvsv3l58ow-axifkay.png\"\u002F\u003E\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Cp\u003EДо 2017 года область face anti-spoofing была не слишком активной. Зато 2019 уже подарил целую серию работ, что связано с агрессивным продвижением мобильных технологий идентификации по лицу, в первую очередь, компанией Apple. Кроме того, технологиями распознавания по лицу заинтересовались банки. В отрасль пришло много новых людей, что позволяет надеяться на быстрый прогресс. Но пока что, несмотря на красивые названия публикаций, обобщающая способность алгоритмов остается очень слабой и не позволяет говорить о какой-либо пригодности к практическому использованию.\u003C\u002Fp\u003E\u003Cbr\u002F\u003E\r\n\u003Ch2 id=\"zaklyuchenie-a-naposledok-ya-skazhu-chto\"\u003EЗаключение. А напоследок я скажу, что…\u003C\u002Fh2\u003E\u003Cbr\u002F\u003E\r\n\u003Cul\u003E\r\n\u003Cli\u003EЛокальные бинарные паттерны, отслеживание моргания, дыхания, движений и прочие сконструированные вручную признаки совершенно не потеряли значимости. Это вызвано, прежде все тем, что глубокое обучение в области face anti-spoofing все еще весьма наивно.\u003C\u002Fli\u003E\r\n\u003Cli\u003EСовершенно очевидно, что в «том самом» решении будет выполняться слияние нескольких методов. Анализ отражения, рассеяния, карты глубины должны использоваться вместе. Скорее всего, поможет добавление дополнительного канала данных, например, запись голоса и какие-то системные подходы, которые позволят собрать несколько технологий в единую систему\u003C\u002Fli\u003E\r\n\u003Cli\u003EПрактически все технологии, используемые для распознавания лица, находят применение в face anti-spoofing (кэп!) Все, что было разработано для распознавания лиц, в том или ином виде нашло применение и для анализа атак\u003C\u002Fli\u003E\r\n\u003Cli\u003EСуществующие датасеты достигли насыщения. Из десяти основных наборов данных в пяти удалось достичь нулевой ошибки. Это уже говорит, например, о работоспособности методов на основе карт глубины, но не позволяет улучшить обобщающую способность. Нужны новые данные и новые эксперименты на них\u003C\u002Fli\u003E\r\n\u003Cli\u003EЕсть явный дисбаланс между степенью развития распознавания лиц и face anti-spoofing. Технологии распознавания существенно опережают системы защиты. Более того, именно отсутствие надежных систем защиты тормозит практическое применение систем распознавания лиц. Так получилось, что основное внимание уделялось именно распознаванию лиц, а системы обнаружения атак остались несколько в стороне\u003C\u002Fli\u003E\r\n\u003Cli\u003EЕсть сильная потребность системного подхода в области face anti-spoofing. Прошедший конкурс университета Оулу показал, что при использовании нерепрезентативного набора данных вполне возможно победить простой грамотной настройкой устоявшихся решений, без разработки новых. Возможно, \u003Ca href=\"https:\u002F\u002Fdatasouls.com\u002Fc\u002Fidrnd-antispoof\u002Fdescription\"\u003Eновое соревнование\u003C\u002Fa\u003E сможет переломить ситуацию\u003C\u002Fli\u003E\r\n\u003Cli\u003EС возрастанием интереса к тематике и внедрением технологий распознавания по лицу крупными игроками появились «окна возможностей» для новых амбициозных команд, поскольку есть серьезная потребность в новом решении на уровне архитектуры\u003C\u002Fli\u003E\r\n\u003C\u002Ful\u003E\u003C\u002Fdiv\u003E","tags":[{"titleHtml":"Face recognition"},{"titleHtml":"face anti-spoofing"},{"titleHtml":"face biometrics"},{"titleHtml":"OpenDataScience"},{"titleHtml":"ODS"},{"titleHtml":"machine learning"},{"titleHtml":"deep learning"}],"metadata":{"stylesUrls":[],"scriptUrls":[],"shareImageUrl":"https:\u002F\u002Fhabr.com\u002Fshare\u002Fpublication\u002F452894\u002Fce98f9b9d7867563422bf6824b86bc45\u002F","shareImageWidth":1200,"shareImageHeight":630,"vkShareImageUrl":"https:\u002F\u002Fhabr.com\u002Fshare\u002Fpublication\u002F452894\u002Fce98f9b9d7867563422bf6824b86bc45\u002F?format=vk","schemaJsonLd":"{\"@context\":\"http:\\\u002F\\\u002Fschema.org\",\"@type\":\"Article\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https:\\\u002F\\\u002Fhabr.com\\\u002Fru\\\u002Fcompany\\\u002Fods\\\u002Fblog\\\u002F452894\\\u002F\"},\"headline\":\"Face Anti-Spoofing или технологично узнаём обманщика из тысячи по лицу\",\"datePublished\":\"2019-05-23T15:22:31+03:00\",\"dateModified\":\"2019-10-20T23:46:21+03:00\",\"author\":{\"@type\":\"Person\",\"name\":\"evgeniimakarov\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Habr\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fa_\\\u002Flk\\\u002F9m\\\u002Fa_lk9mjkccjox-zccjrpfolmkmq.png\"}},\"description\":\"Биометрическая идентификация человека &ndash; это одна из самых старых идей для распознавания людей, которую вообще попытались технически осуществить. Пароли можно укр...\",\"url\":\"https:\\\u002F\\\u002Fhabr.com\\\u002Fru\\\u002Fcompany\\\u002Fods\\\u002Fblog\\\u002F452894\\\u002F#post-content-body\",\"about\":[\"c_ods\",\"h_infosecurity\",\"h_machine_learning\",\"h_popular_science\",\"h_artificial_intelligence\",\"f_develop\",\"f_popsci\"],\"image\":[\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fh4\\\u002Fwd\\\u002Fzr\\\u002Fh4wdzrqvlnvfy8da0k3cn5589hg.jpeg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fpl\\\u002Fvz\\\u002F67\\\u002Fplvz67jfgazkigzqsdhkko62ego.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fxl\\\u002Fkq\\\u002Fpr\\\u002Fxlkqprysss0ce2arbqm0sw_xbpo.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fdt\\\u002Fjx\\\u002Flp\\\u002Fdtjxlptnxcphiicevd6lbhlzzpa.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fq2\\\u002Ftr\\\u002Fnl\\\u002Fq2trnl3zhhuftbggl3vx-sfqoyw.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Ff9\\\u002Fg7\\\u002Fds\\\u002Ff9g7dsc81eiffu9srctasf2udcm.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fde\\\u002Fdd\\\u002Fvj\\\u002Fdeddvj6xvnwbv6dpbryxphjwaa8.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002F1r\\\u002Fn8\\\u002F-e\\\u002F1rn8-elrfhm1q0ud9q4jbaukidq.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002F4s\\\u002Ffb\\\u002Fvv\\\u002F4sfbvvforopuonls_ochr8dpzjg.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fqu\\\u002Fx_\\\u002Fvv\\\u002Fqux_vvmfk7ikww7ojknhq_mpbhu.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002F7j\\\u002Fvy\\\u002Fi7\\\u002F7jvyi7inhsl5g2ulp05v4luido8.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002F98\\\u002Fm2\\\u002Fls\\\u002F98m2ls-svv7gxfaagripfrhzymo.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fih\\\u002F9e\\\u002Fvo\\\u002Fih9evob9a5hmtjekxhx1iwsgxm8.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002F-i\\\u002F1w\\\u002Fw2\\\u002F-i1ww2rsu0kqmxglhh-on8lnr5g.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002F7p\\\u002Ft-\\\u002Fyk\\\u002F7pt-ykbzzuyb7bzpnd9qpjodfog.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fa7\\\u002Fbl\\\u002Fdf\\\u002Fa7bldfybkgzeeptdtomphvq1kwk.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fek\\\u002F8b\\\u002Frc\\\u002Fek8brc52akg2y3zfy6ejmbr-taq.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Faw\\\u002F_k\\\u002Fs_\\\u002Faw_ks_6-xp40fyjz97ue9mwzp8q.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fbb\\\u002F7v\\\u002F73\\\u002Fbb7v73awyzwhxeyyvtejgm3skh4.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fjw\\\u002Fb5\\\u002F-n\\\u002Fjwb5-naohhjhek2tkmakbssjyik.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fvf\\\u002Flz\\\u002Fol\\\u002Fvflzoljiplnkj_vgm-zth9a0sdc.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fur\\\u002Fyf\\\u002Ft8\\\u002Furyft8082ylvw-1kbyvcxucjcem.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fdb\\\u002Fle\\\u002F44\\\u002Fdble44fc8acyqdhzead3ndbkg3a.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fsy\\\u002Fu7\\\u002Fvu\\\u002Fsyu7vucfvb5inhajqoilb6rqxu8.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fqd\\\u002Fbu\\\u002Ffn\\\u002Fqdbufnt8yc9pghj4egkxqaku_3g.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fx9\\\u002Flq\\\u002Fkz\\\u002Fx9lqkzuzmgz7gz5ejx1rsiom7wq.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fc-\\\u002Fts\\\u002Fw8\\\u002Fc-tsw8vywrvsv3l58ow-axifkay.png\"]}","metaDescription":"Биометрическая идентификация человека – это одна из самых старых идей для распознавания людей, которую вообще попытались технически осуществить. Пароли можно украсть, подсмотреть, забыть, ключи –...","mainImageUrl":null,"amp":true},"polls":[],"commentsEnabled":true,"rulesRemindEnabled":false,"votesEnabled":true,"status":"published","plannedPublishTime":null,"checked":null,"isEditorial":false}},"articlesIds":{},"isLoading":false,"pagesCount":{},"route":{},"reasonsList":null,"view":"cards","lastVisitedRoute":{},"ssrCommentsArticleIds":[""],"karma":{}},"authorContribution":{"authors":{}},"betaTest":{"currentAnnouncement":null,"announcements":{},"announcementCards":null,"announcementComments":{},"announcementCommentThreads":{},"announcementCommentingStatuses":{},"archivedList":[]},"authorStatistics":{"articleRefs":{},"articleIds":{},"pagesCount":{},"route":{},"viewsCount":[],"maxStatsCount":{}},"career":{"seoLandings":[],"hubs":""},"comments":{"articleComments":{},"searchCommentsResults":null,"previewComment":null,"pagesCount":null,"commentAccess":{},"scrollParents":{},"pageArticleComments":{"lastViewedComment":0,"postId":null,"lastCommentTimestamp":"","moderated":[],"moderatedIds":[],"commentRoute":""}},"companies":{"companyRefs":{"ods":{"alias":"ods","imageUrl":"\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fcompany\u002F5d0\u002Fe1e\u002Fb70\u002F5d0e1eb7036b66d4ff4a35ad6950cc3e.png","titleHtml":"Open Data Science","descriptionHtml":"Крупнейшее русскоязычное Data Science сообщество","relatedData":null,"statistics":{"postsCount":132,"newsCount":0,"vacanciesCount":0,"employeesCount":113,"careerRating":null,"subscribersCount":7784,"rating":92.29,"invest":null},"foundationDate":{"year":"2015","month":"03","day":"12"},"location":{"city":{"id":"447159","title":"Москва"},"region":{"id":"1885","title":"Москва и Московская обл."},"country":{"id":"168","title":"Россия"}},"siteUrl":"http:\u002F\u002Fods.ai\u002F","staffNumber":"5 001–10 000 человек","registrationDate":"2017-02-16T19:05:46+00:00","representativeUser":null,"contacts":[{"title":"Сайт","url":"http:\u002F\u002Fods.ai\u002F"}],"settings":{"analyticsSettings":[],"branding":null,"status":"active"},"metadata":{"titleHtml":"Open Data Science, Москва - Крупнейшее русскоязычное Data Science сообщество с 12 марта 2015 г.","title":"Open Data Science, Москва - Крупнейшее русскоязычное Data Science сообщество с 12 марта 2015 г.","keywords":["Машинное обучение","Искусственный интеллект","Обработка изображений","Data Mining","Natural Language Processing"],"descriptionHtml":"132 статьи от авторов компании Open Data Science","description":"132 статьи от авторов компании Open Data Science"},"aDeskSettings":null,"careerAlias":null,"maxCustomTrackerLinks":0}},"companyIds":{},"companyTopIds":{},"pagesCount":{},"companyProfiles":{},"companiesCategories":[],"companiesCategoriesTotalCount":0,"companiesWidgets":{},"companiesWorkers":{},"companiesFans":{},"route":{},"isLoading":false,"companyWorkersLoading":false,"companyFansLoading":false,"vacancies":{}},"companiesContribution":{"hubs":{},"flows":{},"companyRefs":{}},"companyHubsContribution":{"contributionRefs":{"hubRefs":{},"hubIds":{}}},"conversation":{"messages":[],"respondent":null,"isLoadMore":false},"conversations":{"conversations":[],"unreadCount":0,"pagesCount":0,"isLoadMore":false},"desktopState":{"desktopFl":null,"desktopHl":null,"isChecked":false,"isLoginDemanded":false},"dfp":{"slotsDict":{}},"docs":{"menu":{},"articles":{},"mainMenu":[],"loading":{"main":false,"dropdown":false,"article":false}},"feature":{"isProbablyVisible":"true"},"flows":{"flows":[{"alias":"develop","id":1,"route":{"name":"FLOW_PAGE","params":{"flowName":"develop"}}},{"alias":"admin","id":6,"route":{"name":"FLOW_PAGE","params":{"flowName":"admin"}}},{"alias":"design","id":2,"route":{"name":"FLOW_PAGE","params":{"flowName":"design"}}},{"alias":"management","id":3,"route":{"name":"FLOW_PAGE","params":{"flowName":"management"}}},{"alias":"marketing","id":4,"route":{"name":"FLOW_PAGE","params":{"flowName":"marketing"}}},{"alias":"popsci","id":7,"route":{"name":"FLOW_PAGE","params":{"flowName":"popsci"}}}]},"global":{"isPwa":false,"device":"desktop","isHabrCom":true},"hubs":{"hubRefs":{},"hubIds":{},"pagesCount":{},"isLoading":false,"route":{}},"hubsBlock":{"hubRefs":{},"hubIds":{}},"i18n":{"fl":"ru","hl":"ru"},"info":{"infoPage":{},"isLoading":true},"location":{"urlStruct":{"protocol":null,"slashes":null,"auth":null,"host":null,"port":null,"hostname":null,"hash":null,"search":null,"query":{},"pathname":null,"path":null,"href":""},"searchQuery":null},"me":{"user":null,"ppgDemanded":false,"karmaResetInfo":{"canReincarnate":null,"wasReincarnated":null,"currentScore":null},"notes":null},"mostReadingList":{"mostReadingListIds":[],"mostReadingListRefs":null,"promoPost":null},"pinnedPost":{"pinnedPost":null},"ppa":{"articles":{},"card":null,"transactions":null,"totalTransactions":null,"isAccessible":null},"projectsBlocks":{"activeBlocks":{}},"pullRefresh":{"shouldRefresh":false},"sandbox":{"articleIds":[],"articleRefs":{},"pagesCount":null,"route":{},"lastVisitedRoute":{},"isLoading":false},"settingsOther":{"inputs":{"uiLang":{"errors":[],"ref":null,"value":""},"articlesLangEnglish":{"errors":[],"ref":null,"value":false},"articlesLangRussian":{"errors":[],"ref":null,"value":false},"agreement":{"errors":[],"ref":null,"value":false},"email":{"errors":[],"ref":null,"value":true},"digest":{"errors":[],"ref":null,"value":true}}},"similarList":{"similarListIds":[],"similarListRefs":null},"ssr":{"error":null,"isDataLoaded":false,"isDataLoading":false,"isHydrationFailed":false,"isServer":false},"userHubsContribution":{"contributionRefs":{"hubRefs":{},"hubIds":{}}},"userInvites":{"availableInvites":0,"usedInvitesIds":[],"usedInvitesRefs":{},"usedInvitesPagesCount":0,"unusedInvitesIds":[],"unusedInvitesRefs":{},"unusedInvitesPagesCount":0},"users":{"authorRefs":{},"authorIds":{},"pagesCount":{},"authorProfiles":{},"userHubs":{},"userInvitations":{},"authorFollowers":{},"authorFollowed":{},"karmaStats":[],"statistics":null,"isLoading":false,"authorFollowersLoading":false,"authorFollowedLoading":false,"userHubsLoading":false,"userInvitationsLoading":false,"route":{}},"viewport":{"prevScrollY":{},"scrollY":0,"width":0},"tracker":{"items":{},"pagesCache":{},"markedViewedSilently":{},"markedRead":{},"unreadCounters":{"applications":null,"system":null,"mentions":null,"subscribers":null,"posts_and_comments":null},"unviewedCounters":{"applications":null,"system":null,"mentions":null,"subscribers":null,"posts_and_comments":null}}};(function(){var s;(s=document.currentScript||document.scripts[document.scripts.length-1]).parentNode.removeChild(s);}());</script>
<script src="https://assets.habr.com/habr-web/js/chunk-vendors.c2c3fc9a.js" defer></script><script src="https://assets.habr.com/habr-web/js/app.c0af73e7.js" defer></script>



    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    </script>
  
  <script type="text/javascript" >
    (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
    m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
    (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

    ym(24049213, "init", {
      defer:true,
      trackLinks:true,
      accurateTrackBounce:true,
      webvisor:false,
    });
  </script>
  <noscript>
    <div>
      <img src="https://mc.yandex.ru/watch/24049213" style="position:absolute; left:-9999px;" alt="" />
    </div>
  </noscript>
  
    <script type="text/javascript">
      window.addEventListener('load', function () {
        setTimeout(() => {
          const img = new Image();
          img.src = 'https://vk.com/rtrg?p=VK-RTRG-421343-57vKE';
        }, 0);
      });
    </script>
  
</body>
</html>

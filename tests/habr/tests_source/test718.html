<!DOCTYPE html>
<html lang="ru" data-vue-meta="%7B%22lang%22:%7B%22ssr%22:%22ru%22%7D%7D">
<head >
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0,viewport-fit=cover">
  <title>Деплой приложений в VM, Nomad и Kubernetes / Хабр</title>
  <style>
    /* cyrillic-ext */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveSxf6TF0.woff2) format('woff2');
      unicode-range: U+0460-052F, U+1C80-1C88, U+20B4, U+2DE0-2DFF, U+A640-A69F, U+FE2E-FE2F;
    }

    /* cyrillic */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveQhf6TF0.woff2) format('woff2');
      unicode-range: U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;
    }

    /* latin-ext */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveSBf6TF0.woff2) format('woff2');
      unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
    }

    /* latin */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveRhf6.woff2) format('woff2');
      unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
    }
  </style>
  <link rel="preload" href="https://assets.habr.com/habr-web/css/chunk-vendors.e7843cc0.css" as="style"><link rel="preload" href="https://assets.habr.com/habr-web/js/chunk-vendors.c2c3fc9a.js" as="script"><link rel="preload" href="https://assets.habr.com/habr-web/css/app.02ee25a4.css" as="style"><link rel="preload" href="https://assets.habr.com/habr-web/js/app.c0af73e7.js" as="script">
  <link rel="stylesheet" href="https://assets.habr.com/habr-web/css/chunk-vendors.e7843cc0.css"><link rel="stylesheet" href="https://assets.habr.com/habr-web/css/app.02ee25a4.css">
  <script>window.i18nFetch = new Promise((res, rej) => {
          const xhr = new XMLHttpRequest();
          xhr.open('GET', '/js/i18n/ru-compiled.85eb77f0b17c8235e7b64b9f81ea5ec2.json');
          xhr.responseType = 'json';
          xhr.onload = function(e) {
            if (this.status === 200) {
              res({ru: xhr.response});
            } else {
              rej(e);
            }
          };
          xhr.send();
        });</script>
  
  <script data-vue-meta="ssr" src="/js/ads.js" onload="window['zhY4i4nJ9K'] = true" data-vmid="checkad"></script><script data-vue-meta="ssr" type="application/ld+json" data-vmid="ldjson-schema">{"@context":"http:\/\/schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/habr.com\/ru\/company\/lamoda\/blog\/451644\/"},"headline":"Деплой приложений в VM, Nomad и Kubernetes","datePublished":"2019-05-14T16:45:01+03:00","dateModified":"2019-05-14T18:22:23+03:00","author":{"@type":"Person","name":"Павел Агалецкий"},"publisher":{"@type":"Organization","name":"Habr","logo":{"@type":"ImageObject","url":"https:\/\/habrastorage.org\/webt\/a_\/lk\/9m\/a_lk9mjkccjox-zccjrpfolmkmq.png"}},"description":"Всем привет! Меня зовут Павел Агалецкий. Я работаю тимлидом в команде, которая разрабатывает систему доставки Lamoda. В 2018 году я выступал на конференции HighL...","url":"https:\/\/habr.com\/ru\/company\/lamoda\/blog\/451644\/#post-content-body","about":["c_lamoda","h_sys_admin","f_admin"],"image":["https:\/\/habrastorage.org\/webt\/4e\/vz\/4e\/4evz4entvdaauztnu558tb-2z9u.jpeg","https:\/\/habrastorage.org\/webt\/vg\/q9\/vb\/vgq9vb_izh4i890ro7giihibz6g.jpeg","https:\/\/habrastorage.org\/webt\/-g\/l7\/8h\/-gl78hl7nbmdbhbuacuntgxw4ow.jpeg","https:\/\/habrastorage.org\/webt\/9a\/ib\/zv\/9aibzvsme34ra2eg53bjfbqt1tw.jpeg","https:\/\/habrastorage.org\/webt\/nu\/au\/8d\/nuau8dqdcwft0boyw57x37wrkcm.jpeg","https:\/\/habrastorage.org\/webt\/pv\/eh\/va\/pvehvavusoxszoxum9bsuwyjqbc.jpeg","https:\/\/habrastorage.org\/webt\/2k\/ka\/53\/2kka53a1vp1lm0rnl4xbhmcpyu4.jpeg","https:\/\/habrastorage.org\/webt\/dc\/lr\/fh\/dclrfhbdr29ms0gouz_pvd8xz44.jpeg","https:\/\/habrastorage.org\/webt\/2r\/lo\/yt\/2rloytnvlrj6nri8vj7e9-hccg8.jpeg","https:\/\/habrastorage.org\/webt\/fe\/p6\/qi\/fep6qibp6hhnbsmqifk2jnmg20a.jpeg"]}</script>
  <script src="//www.googletagservices.com/tag/js/gpt.js" async></script>
  <style>.grecaptcha-badge{visibility: hidden;}</style>
  <meta name="habr-version" content="2.49.0">
  
  <meta data-vue-meta="ssr" property="fb:app_id" content="444736788986613"><meta data-vue-meta="ssr" property="fb:pages" content="472597926099084"><meta data-vue-meta="ssr" name="twitter:card" content="summary_large_image"><meta data-vue-meta="ssr" name="twitter:site" content="@habr_eng"><meta data-vue-meta="ssr" property="og:title" content="Деплой приложений в VM, Nomad и Kubernetes" data-vmid="og:title"><meta data-vue-meta="ssr" name="twitter:title" content="Деплой приложений в VM, Nomad и Kubernetes" data-vmid="twitter:title"><meta data-vue-meta="ssr" name="aiturec:title" content="Деплой приложений в VM, Nomad и Kubernetes" data-vmid="aiturec:title"><meta data-vue-meta="ssr" name="description" content="Всем привет! Меня зовут Павел Агалецкий. Я работаю тимлидом в команде, которая разрабатывает систему доставки Lamoda. В 2018 году я выступал на конференции HighLoad++, а сегодня хочу представить..." data-vmid="description"><meta data-vue-meta="ssr" itemprop="description" content="Всем привет! Меня зовут Павел Агалецкий. Я работаю тимлидом в команде, которая разрабатывает систему доставки Lamoda. В 2018 году я выступал на конференции HighLoad++, а сегодня хочу представить..." data-vmid="description:itemprop"><meta data-vue-meta="ssr" property="og:description" content="Всем привет! Меня зовут Павел Агалецкий. Я работаю тимлидом в команде, которая разрабатывает систему доставки Lamoda. В 2018 году я выступал на конференции HighLoad++, а сегодня хочу представить..." data-vmid="og:description"><meta data-vue-meta="ssr" name="twitter:description" content="Всем привет! Меня зовут Павел Агалецкий. Я работаю тимлидом в команде, которая разрабатывает систему доставки Lamoda. В 2018 году я выступал на конференции HighLoad++, а сегодня хочу представить..." data-vmid="twitter:description"><meta data-vue-meta="ssr" property="aiturec:description" content="Всем привет! Меня зовут Павел Агалецкий. Я работаю тимлидом в команде, которая разрабатывает систему доставки Lamoda. В 2018 году я выступал на конференции HighLoad++, а сегодня хочу представить..." data-vmid="aiturec:description"><meta data-vue-meta="ssr" itemprop="image" content="https://habr.com/share/publication/451644/d400912aad98631730692e19a5e658d9/" data-vmid="image:itemprop"><meta data-vue-meta="ssr" property="og:image" content="https://habr.com/share/publication/451644/d400912aad98631730692e19a5e658d9/" data-vmid="og:image"><meta data-vue-meta="ssr" property="aiturec:image" content="https://habr.com/share/publication/451644/d400912aad98631730692e19a5e658d9/" data-vmid="aiturec:image"><meta data-vue-meta="ssr" name="twitter:image" content="https://habr.com/share/publication/451644/d400912aad98631730692e19a5e658d9/" data-vmid="twitter:image"><meta data-vue-meta="ssr" property="vk:image" content="https://habr.com/share/publication/451644/d400912aad98631730692e19a5e658d9/" data-vmid="vk:image"><meta data-vue-meta="ssr" property="aiturec:item_id" content="451644" data-vmid="aiturec:item_id"><meta data-vue-meta="ssr" property="aiturec:datetime" content="2019-05-14T13:45:01.000Z" data-vmid="aiturec:datetime"><meta data-vue-meta="ssr" property="og:type" content="article" data-vmid="og:type"><meta data-vue-meta="ssr" property="og:locale" content="ru_RU" data-vmid="og:locale"><meta data-vue-meta="ssr" property="og:image:width" content="1200" data-vmid="og:image:width"><meta data-vue-meta="ssr" property="og:image:height" content="630" data-vmid="og:image:height">
  <link data-vue-meta="ssr" href="https://habr.com/ru/rss/post/451644/?fl=ru" type="application/rss+xml" title="" rel="alternate" name="rss"><link data-vue-meta="ssr" href="https://habr.com/ru/company/lamoda/blog/451644/" rel="canonical" data-vmid="canonical"><link data-vue-meta="ssr" data-vmid="hreflang"><link data-vue-meta="ssr" image_src="image" href="https://habr.com/share/publication/451644/d400912aad98631730692e19a5e658d9/" data-vmid="image:href">
  <meta name="apple-mobile-web-app-status-bar-style" content="#303b44">
  <meta name="msapplication-TileColor" content="#629FBC">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="mobile-web-app-capable" content="yes">
  <link
    rel="shortcut icon"
    type="image/png"
    sizes="16x16"
    href="https://assets.habr.com/habr-web/img/favicons/favicon-16.png"
  >
  <link
    rel="shortcut icon"
    type="image/png"
    sizes="32x32"
    href="https://assets.habr.com/habr-web/img/favicons/favicon-32.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="76x76"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-76.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="120x120"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-120.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="152x152"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-152.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="180x180"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-180.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="256x256"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-256.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1136x640.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2436x1125.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1792x828.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_828x1792.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1334x750.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1242x2668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2208x1242.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1125x2436.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1242x2208.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2732x2048.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2688x1242.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2224x1668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_750x1334.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2048x2732.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2388x1668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1668x2224.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_640x1136.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1668x2388.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2048x1536.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1536x2048.png"
  >
  <link
    rel="mask-icon"
    color="#77a2b6"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-120.svg"
  >
  <link
    crossorigin="use-credentials"
    href="/manifest.webmanifest"
    rel="manifest"
  >
</head>
<body>


<div id="app" data-server-rendered="true" data-async-called="true"><div class="tm-layout__wrapper"><!----> <div></div> <!----> <header class="tm-header"><div class="tm-page-width"><div class="tm-header__container"><!----> <span class="tm-header__logo-wrap"><a href="/ru/" class="tm-header__logo tm-header__logo_ru"><svg height="16" width="16" class="tm-svg-img tm-header__icon"><title>Хабр</title> <use xlink:href="/img/habr-logo-ru.svg#logo"></use></svg></a> <span class="tm-header__beta-sign" style="display:none;">β</span></span> <div class="tm-dropdown tm-header__projects"><div class="tm-dropdown__head"><button class="tm-header__dropdown-toggle"><svg height="16" width="16" class="tm-svg-img tm-header__icon tm-header__icon_dropdown"><title>Открыть список</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#arrow-down"></use></svg></button></div> <!----></div> <a href="/ru/sandbox/start/" class="tm-header__become-author-btn">
            Как стать автором
          </a> <div class="tm-feature tm-header__feature tm-feature_variant-inline"><!----></div> <!----> <!----></div></div></header> <div class="tm-layout"><div class="tm-page-progress-bar"></div> <div data-menu-sticky="true" class="tm-base-layout__header tm-base-layout__header_is-sticky"><div class="tm-page-width"><div class="tm-base-layout__header-wrapper"><div class="tm-main-menu"><div class="tm-main-menu__section"><nav class="tm-main-menu__section-content"><!----> <a href="/ru/all/" class="tm-main-menu__item">
        Все потоки
      </a> <a href="/ru/flows/develop/" class="tm-main-menu__item">
          Разработка
        </a><a href="/ru/flows/admin/" class="tm-main-menu__item">
          Администрирование
        </a><a href="/ru/flows/design/" class="tm-main-menu__item">
          Дизайн
        </a><a href="/ru/flows/management/" class="tm-main-menu__item">
          Менеджмент
        </a><a href="/ru/flows/marketing/" class="tm-main-menu__item">
          Маркетинг
        </a><a href="/ru/flows/popsci/" class="tm-main-menu__item">
          Научпоп
        </a></nav></div></div> <div class="tm-header-user-menu tm-base-layout__user-menu"><a href="/ru/search/" class="tm-header-user-menu__item tm-header-user-menu__search"><svg height="24" width="24" class="tm-svg-img tm-header-user-menu__icon tm-header-user-menu__icon_search tm-header-user-menu__icon_dark"><title>Поиск</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#search"></use></svg></a> <!----> <!----> <!----> <div class="tm-header-user-menu__item tm-header-user-menu__user_desktop"><div class="tm-dropdown"><div class="tm-dropdown__head"><svg height="24" width="24" data-test-id="menu-toggle-guest" class="tm-svg-img tm-header-user-menu__icon"><title>Профиль</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#header-user"></use></svg> <!----></div> <!----></div> <!----></div> <!----></div></div></div></div> <!----> <div class="tm-page-width"></div> <main class="tm-layout__container"><div hl="ru" companyName="lamoda" data-async-called="true" class="tm-page"><div class="tm-page-width"><div class="tm-page__header"><!----></div> <div class="tm-page__wrapper"><div class="tm-page__main tm-page__main_has-sidebar"><div class="pull-down"><div class="pull-down__header" style="height:0px;"><div class="pull-down__content" style="bottom:10px;"><svg height="24" width="24" class="tm-svg-img pull-down__arrow"><title>Обновить</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#pull-arrow"></use></svg></div></div> <div class="tm-article-presenter"><div class="tm-company-card tm-company-article__company-card"><div class="tm-company-card__info"><div class="tm-company-card__header"><a href="/ru/company/lamoda/profile/" class="tm-company-card__avatar"><div class="tm-entity-image"><img alt="" height="48" src="//habrastorage.org/getpro/habr/company/cdd/710/61a/cdd71061ae61f0c4f273ddcaaec533c7.png" width="48" class="tm-entity-image__pic"></div></a> <a href="https://career.habr.com/companies/latech" rel="noopener" target="_blank" class="tm-grade tm-company-card__rating"><div class="tm-rating"><div class="tm-rating__header"><svg height="24" width="24" class="tm-svg-img tm-svg-grade__icon"><title>Оценка компании на Хабр Карьере</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#grade"></use></svg> <div class="tm-rating__counter tm-rating__counter_variant-grade">4.58</div></div> <div class="tm-rating__text tm-rating__text_variant-grade">
    Оценка
  </div></div></a> <div class="tm-rating tm-company-card__rating"><div class="tm-rating__header"> <div class="tm-rating__counter">119.87</div></div> <div class="tm-rating__text">
    Рейтинг
  </div></div></div> <div class="tm-company-card__info"><a href="/ru/company/lamoda/profile/" class="tm-company-card__name">
        Lamoda
      </a> <div class="tm-company-card__description">Code the lifestyle</div></div></div> <div class="tm-company-card__buttons"><!----> <!----></div></div> <div class="tm-article-presenter__body"><div class="tm-misprint-area"><div class="tm-misprint-area__wrapper"><article class="tm-article-presenter__content tm-article-presenter__content_narrow"><div class="tm-article-presenter__header"> <div class="tm-article-snippet tm-article-presenter__snippet"><div class="tm-article-snippet__meta-container"><div class="tm-article-snippet__meta"><span class="tm-user-info tm-article-snippet__author"><a href="/ru/users/ewolf/" title="ewolf" class="tm-user-info__userpic"><div class="tm-entity-image"><img alt="" height="24" loading="lazy" src="//habrastorage.org/r/w32/getpro/habr/avatars/4c9/3da/2ed/4c93da2edb71072b412f00cbf40e9941.png" width="24" class="tm-entity-image__pic"></div></a> <span class="tm-user-info__user"><a href="/ru/users/ewolf/" class="tm-user-info__username">
      ewolf
    </a> </span></span> <span class="tm-article-snippet__datetime-published"><time datetime="2019-05-14T13:45:01.000Z" title="2019-05-14, 16:45">14  мая  2019 в 16:45</time></span></div> <!----></div> <h1 lang="ru" class="tm-article-snippet__title tm-article-snippet__title_h1"><span>Деплой приложений в VM, Nomad и Kubernetes</span></h1> <div class="tm-article-snippet__hubs"><span class="tm-article-snippet__hubs-item"><a href="/ru/company/lamoda/blog/" class="tm-article-snippet__hubs-item-link router-link-active"><span>Блог компании Lamoda</span> <!----></a></span><span class="tm-article-snippet__hubs-item"><a href="/ru/hub/sys_admin/" class="tm-article-snippet__hubs-item-link"><span>Системное администрирование</span> <span title="Профильный хаб" class="tm-article-snippet__profiled-hub">*</span></a></span></div> <!----> <!----> <!----></div></div> <!----> <div data-gallery-root="" lang="ru" class="tm-article-body"><div id="post-content-body" class="article-formatted-body article-formatted-body_version-1"><div xmlns="http://www.w3.org/1999/xhtml">Всем привет! Меня зовут Павел Агалецкий. Я работаю тимлидом в команде, которая разрабатывает систему доставки Lamoda. В 2018 году я выступал на конференции HighLoad++, а сегодня хочу представить расшифровку своего доклада.<br/>
<br/>
Моя тема посвящена опыту нашей компании по деплою систем и сервисов в разные среды. Начиная от наших доисторических времен, когда мы деплоили все системы в обычные виртуальные сервера, заканчивая постепенным переход от Nomad к деплою в Kubernetes. Расскажу о том, почему мы это сделали и какие у нас были в процессе проблемы.<br/>
<br/>
<div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.2493%;"><div class="tm-iframe_temp" data-src="https://www.youtube.com/embed/oqrb7dWECSo?rel=0&amp;showinfo=1&amp;hl=en-US" data-style="border: 0; top: 0; left: 0; width: 100%; height: 100%; position: absolute;" id="" width=""></div></div></div></div><a name="habracut"></a><br/>
 <h1>Деплой приложений на VM</h1><br/>
Начнем с того, что 3 года назад все системы и сервисы компании деплоились на обычные виртуальные сервера. Технически было организовано так, что весь код наших систем лежал и собирался за счет средств автоматической сборки, с помощью jenkins. При помощи Ansible он из нашей системы контроля версий раскатывался уже на виртуальные сервера. При этом каждая система, которая была в нашей компании, деплоилась, как минимум, на 2 сервера: одна из них — на head, вторая — на tail. Эти две системы были абсолютно идентичны между собой по всем своим настройкам, мощности, конфигурации и прочему. Отличие между ними было лишь в том, что head получал пользовательский трафик, а tail никогда трафик пользователей на себя не получал. <br/>
<br/>
Для чего это было сделано? <br/>
<br/>
Когда мы деплоили новые релизы нашего приложения, то хотели обеспечить возможность бесшовной раскатки, то есть без заметных последствий для пользователей. Достигалось это за счет того, что очередной собранный релиз с помощью Ansible выкатывался на tail. Там люди, которые занимались деплоем, могли проверить и убедиться, что все хорошо: все метрики, разделы и приложения работают; запускаются нужные скрипты. Только после того, как они убеждались, что все ок, трафик переключался. Он начинал идти на тот сервер, который до этого был tail. А тот, который до этого был head-ом, оставался без пользовательского трафика, при этом с имеющейся на нем предыдущей версией нашего приложения.<br/>
<br/>
Таким образом, для пользователей это было бесшовно. Потому что переключение одномоментное, так как это просто переключение балансировщика. Очень легко можно откатиться на предыдущую версию, просто переключив балансировщик назад. Также мы могли убедиться в способности приложения на продакшн еще до того, как на него пойдет пользовательский трафик, что было достаточно удобно. <br/>
<br/>
Какие мы видели во всем этом достоинства?<br/>
<br/>
<ol>
<li>Прежде всего, это достаточно <b>просто работает.</b> Всем понятно, как функционирует подобная схема деплоя, потому что большинство людей когда-либо деплоили на обычные виртуальные сервера.</li>
<li>Это достаточно <b>надежно</b>, так как технология деплоя простая, опробованная тысячами компаний. Миллионы серверов деплоятся именно так. Сложно что-то сломать. </li>
<li>И наконец, мы могли получить <b>атомарные деплои</b>. Деплои, которые для пользователей происходят одномоментно, без заметного этапа переключения между старой версией и новой. </li>
</ol><br/>
Но в этом всем мы также видели несколько недостатков: <br/>
<br/>
<ol>
<li>Помимо продакшн-среды, среды разработки, есть и другие среды. Например, qa и preproduction. На тот момент у нас было много серверов и около 60 сервисов. По этой причине приходилось <b>для каждого сервиса поддерживать актуальную для него версию </b>виртуальной машины. Причем, если вы хотите обновить библиотеки или поставить новые зависимости, вам необходимо это сделать во всех средах. Также нужно было синхронизировать время, когда вы собираетесь деплоить очередную новую версию вашего приложения, со временем, когда devops выполнит необходимые настройки окружения. В таком случае легко попасть в ситуацию, когда окружение у нас будет несколько отличаться сразу во всех средах подряд. Например, в QA-среде будут одни версии библиотек, а в продакшн — другие, что приведет к проблемам. </li>
<li><b>Сложность в обновлении зависимостей</b> вашего приложения. Это зависит не от вас, а от другой команды. А именно, от команды devops, которая поддерживает сервера. Вы должны поставить для них соответствующую задачу и дать описание того, что хотите сделать.</li>
<li>В то время мы также хотели разделить имевшиеся у нас большие крупные монолиты на отдельные маленькие сервисы, так как понимали, что их будет становиться все больше и больше. На тот момент у нас уже было их более 100. Необходимо было для каждого нового сервиса создавать отдельную новую виртуальную машину, которую также надо обслуживать и деплоить. Кроме этого, нужна не одна машина, а, как минимум, две. К этому всему еще добавляется QA-среда. Это вызывает проблемы и делает для вас создание и запуск новых систем более <b>сложным, дорогостоящим и долгим процессом.</b></li>
</ol><br/>
Поэтому мы приняли решение, что будет удобнее перейти от деплоя обычных виртуальных машин к деплою наших приложений в контейнере docker. При наличии docker нужна система, которая сможет запустить приложение в кластере, так как вы не сможете поднять просто так контейнер. Обычно хочется следить за тем, сколько контейнеров поднято, чтобы они поднимались автоматически. По этой причине нам нужно было выбрать систему управления. <br/>
<br/>
Мы долго размышляли над тем, какую из них можно взять. Дело в том, что на тот момент этот стек деплоя на обычные виртуальные сервера был у нас несколько устаревшим, так как там были не самые свежие версии операционных систем. В какой-то момент там даже FreeBSD стояла, которую было не очень удобно поддерживать. Мы понимали, что нужно как можно быстрее мигрировать в docker. Наши девопс посмотрели на свой имеющийся опыт работы с разными решениями и выбрали такую систему как Nomad. <br/>
<br/>
<h1>Переход на Nomad</h1><br/>
Nomad – это продукт компании “HashiCorp”. Также они известны другими своими решениями:<br/>
<br/>
<img src="https://habrastorage.org/r/w780q1/webt/4e/vz/4e/4evz4entvdaauztnu558tb-2z9u.jpeg" alt="image" data-src="https://habrastorage.org/webt/4e/vz/4e/4evz4entvdaauztnu558tb-2z9u.jpeg" data-blurred="true"/><br/>
<br/>
<b>«Consul»</b> — это средство для сервис-дискаверинга.<br/>
<br/>
<b>«Terraform»</b> — система для управления серверами, позволяющая вам настраивать их через конфигурацию, так называемую infrastructure-as-a-code.<br/>
<br/>
<b>«Vagrant»</b> позволяет вам разворачивать виртуальные машины локально или в облаке путем определенных файлов конфигурации. <br/>
<br/>
Nomad на тот момент показался достаточно простым решением, на который можно быстро перейти без изменения всей инфраструктуры. К тому же, он достаточно легко осваивается. Поэтому именно его мы и выбрали в качестве системы фильтрации нашего контейнера. <br/>
<br/>
Что нужно для того, чтобы вообще задеплоить вашу систему в Nomad? <br/>
<br/>
<ol>
<li>Прежде всего нужен <b>docker image</b> вашего приложения. Необходимо собрать его и поместить в хранилище образов docker. В нашем случае это artifactory — такая система, которая позволяет пушить в нее различные артефакты разного типа. Она умеет хранить архивы, образы docker, пакеты composer РНР, NРМ-пакеты и так далее. </li>
<li>Также необходим<b> конфигурационный файл</b>, который скажет Nomad, что, куда и в каком количестве вы хотите задеплоить. </li>
</ol><br/>
Когда мы говорим про Nomad, то в качестве формата информационного файла он использует язык HСL, что расшифровывается как <i>HashiCorp Configuration Language</i>. Это такое надмножество над Yaml, которое позволяет вам описать ваш сервис в терминах Nomad. <br/>
<br/>
<img src="https://habrastorage.org/r/w780q1/webt/vg/q9/vb/vgq9vb_izh4i890ro7giihibz6g.jpeg" alt="image" data-src="https://habrastorage.org/webt/vg/q9/vb/vgq9vb_izh4i890ro7giihibz6g.jpeg" data-blurred="true"/><br/>
<br/>
Он позволяет сказать, сколько контейнеров хотите задеплоить, из каких images передать им различные параметры при деплое. Таким образом, вы скармливаете этот файл Nomad, а он запускает в соответствии с ним контейнеры в продакшн. <br/>
<br/>
В нашем случае мы поняли, что просто писать под каждый сервис абсолютно одинаковые, идентичные HСL-файлы будет не очень удобно, потому что сервисов много и иногда хочется их обновить. Бывает такое, что один сервис задеплоен не в одном экземпляре, а в самых разных. Например, одна из систем, которая у нас находится в продакшн, имеет более 100 инстансов в проде. Они запускаются из одних и тех же образов, но отличаются конфигурационными настройками и конфигурационными файлами. <br/>
<br/>
Поэтому мы решили, что нам будет удобно хранить все наши конфигурационные файлы для деплоя в одном общем репозитории. Таким образом, они становились обозреваемы: их было легко поддерживать и можно было увидеть, какие системы у нас есть. В случае необходимости также несложно что-то обновить или поменять. Добавить новую систему тоже не составит труда — достаточно просто завести конфигурационный файл внутри новой директории. Внутри нее лежат файлы: service.hcl, который содержит описание нашего сервиса, и некоторые env-файлы, которые позволяют этот самый сервис, будучи задеплоенным в продакшн, настроить. <br/>
<br/>
<img src="https://habrastorage.org/r/w780q1/webt/-g/l7/8h/-gl78hl7nbmdbhbuacuntgxw4ow.jpeg" alt="image" data-src="https://habrastorage.org/webt/-g/l7/8h/-gl78hl7nbmdbhbuacuntgxw4ow.jpeg" data-blurred="true"/><br/>
<br/>
Однако некоторые наши системы задеплоены в проде не в одном экземпляре, а в нескольких сразу. Поэтому мы решили, что нам удобно будет хранить не конфиги в чистом виде, а их шаблонизированный вид. И в качестве языка шаблонизации мы выбрали <i>jinja 2</i>. В таком формате у нас хранятся как конфиги самого сервиса, так и env-файлы, нужные для него. <br/>
<br/>
Помимо этого, мы поместили в репозиторий общий для всех проектов скрипт-деплой, который позволяет запустить и задеплоить ваш сервис в продакшн, в нужный environment, в нужный таргет. В случае, когда мы превратили наш HCL-конфиг в шаблон, то тот HСL-файл, который до этого был обычным конфигом Nomad, в этом случае стал выглядеть несколько иначе.<br/>
<br/>
<img src="https://habrastorage.org/r/w780q1/webt/9a/ib/zv/9aibzvsme34ra2eg53bjfbqt1tw.jpeg" alt="image" data-src="https://habrastorage.org/webt/9a/ib/zv/9aibzvsme34ra2eg53bjfbqt1tw.jpeg" data-blurred="true"/><br/>
<br/>
То есть мы заменили какие-то переменные места конфига на вставки переменных, которые берутся из env-файлов или из других источников. Помимо этого, мы получили возможность собирать HСL-файлы динамически, то есть мы можем применять не только обычные вставки переменных. Так как jinja поддерживает циклы и условия, туда же можно делать конфигурационные файлы, которые меняются в зависимости от того, куда именно вы деплоите свои приложения. <br/>
<br/>
Например, вы хотите деплоить ваш сервис в препродакшн и в продакшн. Допустим, что в препродакшн вы не хотите запускать крон-cкрипты, а просто хотите видеть сервис на отдельном домене, чтобы убедиться, что он функционирует. Для любого, кто деплоит сервис, процесс выглядит очень просто и прозрачно. Достаточно выполнить файл deploy.sh, указать, какой сервис вы хотите задеплоить и в какой таргет. Например, вы хотите задеплоить некоторую систему в Россию, в Белоруссию или Казахстан. Для этого достаточно просто поменять один из параметров, и у вас соберется правильный конфигурационный файл. <br/>
<br/>
Когда сервис Nomad уже оказывается у вас задеплоенным в кластер, он выглядит следующим образом.<br/>
<br/>
<img src="https://habrastorage.org/r/w780q1/webt/nu/au/8d/nuau8dqdcwft0boyw57x37wrkcm.jpeg" alt="image" data-src="https://habrastorage.org/webt/nu/au/8d/nuau8dqdcwft0boyw57x37wrkcm.jpeg" data-blurred="true"/><br/>
<br/>
Для начала вам нужен некоторый балансировщик снаружи, который будет принимать весь пользовательский трафик в себя. Он будет работать вместе с Consul и узнавать у него, где, на какой ноде, по какому IP-адресу находится конкретный сервис, который соответствует тому или иному доменному имени. Сервисы в Consul появляются из самого Nomad. Поскольку это продукты одной и той же компании, они неплохо связаны между собой. Можно сказать, что Nomad из коробки умеет регистрировать все запускаемые в нем сервисы внутри Consul. <br/>
<br/>
После того как ваш внешний балансировщик узнает о том, в какой сервис необходимо отправить трафик, он перенаправляет его в соответствующий контейнер или в несколько контейнеров, которые соответствуют вашему приложению. Естественно, что при этом необходимо думать и о безопасности. Даже несмотря на то, что все сервисы запускаются на одних и тех же виртуальных машинах в контейнерах, обычно для этого требуется запретить свободный доступ из любого сервиса к любому другому. Мы достигали этого за счет сегментации. Каждый сервис запускался в своей виртуальной сети, на которой были прописаны правила маршрутизации и правила разрешения/запрета доступа к другим системами и сервисам. Они могли находиться как внутри этого кластера, так и вне него. Например, если вы хотите запретить сервису коннектиться к определённой базе данных, это можно сделать путем сегментации на уровне сети. То есть даже по ошибке вы не можете случайно подсоединяться из тестового окружения к вашей продакшн базе.<br/>
<br/>
Во что нам обошелся процесс перехода в плане человеческих ресурсов? <br/>
<br/>
Примерно 5-6 месяцев занял переход всей компании в Nomad. Мы переходили посервисно, но в достаточно быстром темпе. Каждая команда должна была создать свои собственные контейнеры для сервисов. <br/>
<br/>
У нас принят такой подход, что каждая команда отвечает за docker images своих систем самостоятельно. Девопс же предоставляют общую инфраструктуру, необходимую для деплоя, то есть поддержку самого кластера, поддержку CI-системы и так далее. И на тот момент у нас более 60 систем переехали в Nomad, получилось порядка 2 тысяч контейнеров. <br/>
<br/>
Девопс отвечает за общую инфраструктуру всего, что связано с деплоем, с серверами. А каждая команда разработки в свою очередь отвечает за реализацию контейнеров под свою конкретную систему, поскольку именно команда знает, что ей вообще нужно в том или ином контейнере.<br/>
<br/>
<h1>Причины отказа от Nomad</h1><br/>
Какие достоинства мы получили, перейдя на деплой с помощью Nomad и docker в том числе?<br/>
<br/>
<ol>
<li>Мы<b> обеспечили одинаковые условия</b> для всех сред. В девеломпенте, QA-среде, препродакшне, продакшне используются одни и те же образы контейнеров, с одними и теми же зависимостями. Соответственно у вас практически отсутствует шанс того, что в продакшн окажется не то, что вы до этого протестировали у себя локально или на тестовом окружении. </li>
<li>Также мы выяснили, что достаточно <b>легко добавить новый сервис</b>. Любые новые системы с точки зрения деплоя запускаются очень просто. Достаточно пойти в репозиторий, хранящий конфиги, добавить туда очередной конфиг для вашей системы, и у вас все готово. Вы можете деплоить вашу систему в продакшн без дополнительных усилий от девопс. </li>
<li>Все <b>конфигурационные файлы</b> в одном общем репозитории <b>оказались обозреваемые</b>. В тот момент, когда мы деплоили наши системы с помощью виртуальных серверов, мы использовали Ansible, в котором конфиги лежали в одном и том же репозитории. Однако для большинства разработчиков работать было с этим несколько сложнее. Тут объем конфигов и кода, который вам нужно добавить, чтобы задеплоить сервис, стал намного меньше. Плюс для девопс очень легко поправить его или поменять. В случае переходов, например, на новой версии Nomad, они могут взять и массово обновить все операционные файлы, лежащие в одном и том же месте.</li>
</ol><br/>
Но мы также столкнулись и с несколькими недостатками: <br/>
<br/>
Оказалось, что мы <b>не смогли достичь бесшовности деплоев </b>в случае Nomad. При выкатке контейнеров из разных условий могло получиться так, что он оказывался запущенным, и Nomad воспринимал его как готовый к принятию трафика контейнер. Это происходило еще до того, как приложение внутри него успело запуститься. По этой причине система на недолгий срок начинала выдавать 500-е ошибки, потому что трафик начинал идти на контейнер, который еще не готов его принять. <br/>
<br/>
Мы столкнулись с некоторыми <b>багами</b>. Самый существенный баг заключается в том, что Nomad не очень хорошо воспринимает большой кластер, если у вас много систем и контейнеров. Когда вы хотите вывезти в обслуживание один из серверов, который включен в кластер Nomad, есть достаточно большая вероятность того, что кластер почувствует себя не очень хорошо и развалится на части. Часть контейнеров может, например, упасть и не подняться — это вам впоследствии очень дорого обойдется, если все ваши системы в продакшн находятся именно в кластере, управляемом Nomad. <br/>
<br/>
Поэтому мы решили подумать, куда нам идти дальше. На тот момент мы стали намного лучше осознавать, чего хотим добиться. А именно: хотим надежности, немножко больше функций, чем дает Nomad, и более зрелую, более стабильную систему. <br/>
<br/>
В этом плане наш выбор пал на Kubernetes как на самую популярную платформу для запуска кластеров. Особенно при условии того, что размер и количество наших контейнеров был достаточно большой. Для таких целей Kubernetes показался самой подходящей системой из тех, что мы могли посмотреть. <br/>
<br/>
<h1>Переход в Kubernetes</h1><br/>
Немножко расскажу о том, какие есть основные понятия Kubernetes и чем они отличаются от Nomad. <br/>
<br/>
<img src="https://habrastorage.org/r/w780q1/webt/pv/eh/va/pvehvavusoxszoxum9bsuwyjqbc.jpeg" alt="image" data-src="https://habrastorage.org/webt/pv/eh/va/pvehvavusoxszoxum9bsuwyjqbc.jpeg" data-blurred="true"/><br/>
<br/>
В первую очередь самым базовым понятием в Kubernetes является понятие pod. <b>Pod</b> — это группа одного или нескольких контейнеров, которые всегда запускаются вместе. И они работают как будто всегда строго на одной виртуальной машине. Они доступны друг другу по айпишнику 127.0.0.1 на разных портах. <br/>
<br/>
Предположим, что у вас есть РНР-приложение, которое состоит из nginx и php-fpm – классическая схема. Скорее всего, вы захотите, чтобы и nginx и php-fpm контейнеры были всегда вместе. Kubernetes позволяет этого добиться, описав их как один общий pod. Как раз этого мы и не могли получить при помощи Nomad.<br/>
<br/>
Вторым понятием является <b>deployment</b>. Дело в том, что pod сам по себе – это эфемерная вещь, она запускается и исчезает. Хотите ли вы сначала убивать все ваши предыдущие контейнеры, а потом запускать сразу новые версии или же вы хотите выкатывать их постепенно – вот именно за этот процесс отвечает понятие deployment. Он описывает то, как вы деплоите ваши podы, в каком количестве и, как их обновлять. <br/>
<br/>
Третьим понятием является <b>service</b>. Ваш service – это фактически ваша система, которая принимает в себя некий трафик, а потом направляет его на один или несколько podов, соответствующих вашему сервису. То есть он позволяет сказать, что весь входящий трафик на такой-то сервис с таким-то именем необходимо отправлять на конкретно эти podы. И при этом он обеспечивает вам балансировку трафика. То есть вы можете запустить два podа вашего приложения, и весь входящий трафик будет равномерно балансироваться между относящимися к этому сервису podами.<br/>
<br/>
И четвертое основное понятие — <b>Ingress</b>. Это сервис, который запускается в кластере Kubernetes. Он выступает как внешний балансировщик нагрузки, который принимает на себя все запросы. За счет API Kubernetes Ingress может определять, куда эти запросы надо отправить. Причем делает он это очень гибко. Вы можете сказать, что все запросы на этот хост и такой-то URL отправляем на этот сервис. А эти запросы, приходящие на этот хост и на другой URL отправляем на другой сервис. <br/>
<br/>
Самое крутое с точки зрения того, кто разрабатывает приложение — это то, что вы способны этим всем управлять самостоятельно. Задав конфиг Ingress, можете весь трафик, приходящий на такой-то API, отправлять на отдельные контейнеры, прописанные, например, на Go. А вот этот трафик, приходящий на тот же домен, но на другой URL, отправлять на контейнеры, написанные на РНР, где много логики, но они не очень скоростные.<br/>
<br/>
Если сравнить все эти понятия с Nomad, то можно сказать, что первые три понятия – это все вместе Service. А последнее понятие в самом Nomad отсутствует. Мы в качестве него использовали внешний балансировщик: это может быть haproxy, nginx, nginx+ и так далее. В случае с кубом вам не надо вводить это дополнительное понятие отдельно. Однако, если посмотреть на Ingress внутри, то это либо nginx, либо haproxy, либо traefik, но как бы встроенный в Kubernetes. <br/>
<br/>
Все описанные мною понятия – это, по сути дела, ресурсы, которые существуют внутри кластера Kubernetes. Для их описания в кубе используется yaml-формат, более читаемый и привычный, чем НСL-файлы в случае с Nomad. Но структурно они описывают в случае, например, podа то же самое. Они говорят – хочу задеплоить такие-то podы туда-то, с такими-то имаджи, в таком-то количестве. <br/>
<br/>
<img src="https://habrastorage.org/r/w780q1/webt/2k/ka/53/2kka53a1vp1lm0rnl4xbhmcpyu4.jpeg" alt="image" data-src="https://habrastorage.org/webt/2k/ka/53/2kka53a1vp1lm0rnl4xbhmcpyu4.jpeg" data-blurred="true"/><br/>
<br/>
Помимо этого, мы поняли, что не хотим руками создавать каждый отдельный ресурс: deployment, сервисы, Ingress и прочее. Вместо этого мы хотели при деплое описывать нашу каждую систему, имеющуюся в терминах Kubernetes, чтобы не приходилось вручную пересоздавать в нужном порядке все необходимые зависимости ресурсов. В качестве такой системы, которая позволила нам это сделать, был выбран Helm. <br/>
<br/>
<h1>Основные понятия в Helm</h1><br/>
Helm – это <b>пакетный менеджер</b> для Kubernetes. Он очень похож на то, как работают пакетные менеджеры в языках программирования. Они позволяют вам хранить сервис, состоящий из, например, deployment nginx, deployment php-fpm, конфига для Ingress, configmaps (это сущность, которая позволяет вам задать env и другие параметры для вашей системы) в виде так называемых чартов. При этом Helm <b>работает поверх Kubernetes</b>. То есть это не какая-то система, стоящая в стороне, а просто ещё один сервис, запускаемый внутри куба. Вы взаимодействуете с ним по его API через консольную команду. Его удобство и прелесть в том, что даже если helm сломается или вы его удалите из кластера, то ваши сервисы не исчезнут, так как helm служит по сути только для запуска системы. За работоспособность и состояние сервисов дальше отвечает сам Kubernetes. <br/>
<br/>
Также мы поняли, что <b>шаблонизация</b>, которую до этого были вынуждены делать самостоятельно через внедрение jinja в наши конфиги, является одной из основных возможностей helm. Все конфиги, которые вы создаете для ваших систем, хранятся в helm в виде шаблонов, похожих немножко на jinja, но, на самом деле, использующих шаблонизацию языка Go, на котором helm написан, как и Kubernetes. <br/>
<br/>
Helm добавляет нам еще несколько дополнительных понятий. <br/>
<br/>
<b>Chart</b> — это описание вашего сервиса. В других пакетных менеджерах его назвали бы пакет, bundle или что-то подобное. Здесь это называется chart. <br/>
<br/>
<b>Values </b>– это переменные, которые вы хотите использовать для сборки ваших конфигов из шаблонов. <br/>
<br/>
<b>Release</b>. Каждый раз сервис, который деплоится с помощью helm, получает инкрементальную версию релиза. Helm помнит, какой был конфиг сервиса при предыдущем, позапрошлом релизе и так далее. Поэтому, если нужно откатиться, достаточно выполнить команду helm callback, указав ему предыдущую версию релиза. Даже если в момент отката соответствующая конфигурация в вашем репозитории не будет доступна, helm все равно помнит, какой она была, и откатит вашу систему на то состояние, в котором она была в предыдущем релизе. <br/>
<br/>
В случае, когда мы используем helm, обычные конфиги для Kubernetes тоже превращаются в шаблоны, в которых есть возможность использовать переменные, функции, применять условные операторы. Таким образом, вы можете собирать конфиг вашего сервиса в зависимости от среды.<br/>
<br/>
<img src="https://habrastorage.org/r/w780q1/webt/dc/lr/fh/dclrfhbdr29ms0gouz_pvd8xz44.jpeg" alt="image" data-src="https://habrastorage.org/webt/dc/lr/fh/dclrfhbdr29ms0gouz_pvd8xz44.jpeg" data-blurred="true"/> <br/>
<br/>
На практике мы решили поступить чуть-чуть иначе, чем мы делали в случае с Nomad. Если в Nomad в одном репозитории хранились и конфиги для деплоя, и n-переменные, которые нужны для того, чтобы задеплоить наш сервис, то здесь мы решили их разделить на два отдельных репозитория. В репозитории «deploy» хранятся только n-переменные, нужные для деплоя, а в репозитории «helm» хранятся конфиги или чарты.<br/>
<br/>
<img src="https://habrastorage.org/r/w780q1/webt/2r/lo/yt/2rloytnvlrj6nri8vj7e9-hccg8.jpeg" alt="image" data-src="https://habrastorage.org/webt/2r/lo/yt/2rloytnvlrj6nri8vj7e9-hccg8.jpeg" data-blurred="true"/><br/>
<br/>
Что это нам дало? <br/>
<br/>
Несмотря на то, что в самих конфигурационных файлах мы не храним какие-то действительно чувствительные данные. Например, пароли к базам данных. Они хранятся в виде secrets в Kubernetes, но тем не менее, там все равно есть отдельные вещи, к которым мы не хотим давать доступ всем подряд. Поэтому доступ к репозиторию «deploy» более ограничен, а репозиторий «helm» содержит просто описание сервиса. По этой причине в него можно дать доступ безопасно большему кругу лиц. <br/>
<br/>
Поскольку у нас есть не только продакшн, но и другие среды, то благодаря такому разделению мы можем переиспользовать наши helm-чарты, чтобы деплоить сервисы не только в продакшн, но и, например, в QA-среду. Даже для того, чтобы разворачивать их локально, используя <i>Minikube</i> — это такая штука для локального запуска Kubernetes. <br/>
<br/>
Внутри каждого репозитория мы оставили разделение на отдельные директории для каждого сервиса. То есть внутри каждой директории находятся шаблоны, относящиеся к соответствующему чарту и описывающие те ресурсы, которые необходимо задеплоить для запуска нашей системы. В репозитории «deploy» мы оставили только энвы. В этом случае мы не стали использовать шаблонизацию с помощью jinja, потому что helm сам дает шаблонизацию из коробки – это одна из его основных функций. <br/>
<br/>
Мы оставили скрипт для деплоя – deploy.sh, который упрощает и стандартизирует запуск для деплоя с помощью helm. Таким образом, для любого, кто хочет задеплоить, интерфейс деплоя выглядит точно так же, как он был в случае деплоя через Nomad. Такой же deploy.sh, название вашего сервиса, и то, куда вы хотите его задеплоить. Это приводит к тому, что внутри запускается helm. Он в свою очередь собирает конфиги из шаблонов, подставляет в них необходимые values-файлы, затем деплоит, пуская их в Kubernetes. <br/>
<br/>
<h1>Выводы</h1><br/>
Сервис Kubernetes выглядит более сложным, чем Nomad. <br/>
<br/>
<img src="https://habrastorage.org/r/w780q1/webt/fe/p6/qi/fep6qibp6hhnbsmqifk2jnmg20a.jpeg" alt="image" data-src="https://habrastorage.org/webt/fe/p6/qi/fep6qibp6hhnbsmqifk2jnmg20a.jpeg" data-blurred="true"/><br/>
<br/>
Здесь исходящий трафик приходит в Ingress. Это как раз фронт-контроллер, который принимает на себя все запросы и впоследствии отправляет их в соответствующие данным запроса сервисы. Определяет он их на основе конфигов, которые являются частью описания вашего приложения в helm и которые разработчики задают самостоятельно. Сервис же отправляет запросы на свои podы, то есть конкретные контейнеры, балансируя входящий трафик между всеми контейнерами, которые относятся к данному сервису. Ну и, конечно, не стоит забывать про то, что от безопасности на уровне сети, мы никуда не должны уходить. Поэтому в кластере Kubernetes работает сегментация, которая основана на тегировании. Все сервисы имеют определенные теги, к которым привязаны права доступов сервисов к тем или иным внешним/внутренним ресурсам внутри или вне кластера. <br/>
<br/>
Выполняя переход, мы увидели, что Kubernetes обладает всеми возможностями Nomad, который до этого мы использовали, а также добавляет много нового. Его можно расширять через плагины, а по факту через кастомные типы ресурсов. То есть у вас есть возможность не просто использовать что-то, что идет в Kubernetes из коробки, а создать свой собственный ресурс и сервис, который будет ваш ресурс читать. Это дает дополнительные возможности расширения вашей системы без необходимости переустановки Kubernetes и без необходимости изменений. <br/>
<br/>
Примером такого использования является Prometheus, который запускается у нас внутри кластера Kubernetes. Для того, чтобы он начал собирать метрики с того или иного сервиса, нам нужно добавить в описание сервиса дополнительный тип ресурса, так называемый сервис-монитор. Prometheus за счет того, что умеет вычитывать, будучи запущенным в Kubernetes, кастомный тип ресурсов, автоматически начинает собирать метрики с новой системы. Это достаточно удобно. <br/>
<br/>
Первый деплой, который мы сделали в Kubernetes был в марте 2018 года. И за это время мы никогда не испытывали с ним никаких проблем. Он достаточно стабильно работает без существенных багов. К тому же, мы можем его дальше расширять. На сегодняшний день нам хватает тех возможностей, которые в нем есть, а темпы развития Kubernetes нам очень нравятся. На данный момент больше 3000 контейнеров находятся в Kubernetes. Кластер занимает несколько Node. При этом он обслуживаемый, стабильный и очень контролируемый.</div></div> <!----> <!----></div> <div class="tm-article-presenter__meta"><div class="tm-separated-list tm-article-presenter__meta-list"><span class="tm-separated-list__title">Теги:</span> <ul class="tm-separated-list__list"><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bkubernetes%5D" class="tm-tags-list__link">kubernetes</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bnomad%5D" class="tm-tags-list__link">nomad</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bdeploy%5D" class="tm-tags-list__link">deploy</a></li></ul></div> <div class="tm-separated-list tm-article-presenter__meta-list"><span class="tm-separated-list__title">Хабы:</span> <ul class="tm-separated-list__list"><li class="tm-separated-list__item"><a href="/ru/company/lamoda/blog/" class="tm-hubs-list__link router-link-active">
    Блог компании Lamoda
  </a></li><li class="tm-separated-list__item"><a href="/ru/hub/sys_admin/" class="tm-hubs-list__link">
    Системное администрирование
  </a></li></ul></div></div></article></div> <!----></div> <div class="tm-article-sticky-panel"><div class="tm-data-icons tm-article-sticky-panel__icons"><div class="tm-article-rating tm-data-icons__item"><div class="tm-votes-meter tm-article-rating__votes-switcher"><svg height="16" width="16" class="tm-svg-img tm-votes-meter__icon tm-votes-meter__icon_medium"><title>Всего голосов 16: ↑13 и ↓3</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-rating"></use></svg> <span title="Всего голосов 16: ↑13 и ↓3" class="tm-votes-meter__value tm-votes-meter__value_positive tm-votes-meter__value_medium">+10</span></div> <DIV class="v-portal" style="display:none;"></DIV></div> <!----> <span title="Количество просмотров" class="tm-icon-counter tm-data-icons__item"><svg height="16" width="16" class="tm-svg-img tm-icon-counter__icon"><title>Просмотры</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-views"></use></svg> <span class="tm-icon-counter__value">6.4K</span></span> <button title="Добавить в закладки" type="button" class="bookmarks-button tm-data-icons__item"><span title="Добавить в закладки" class="tm-svg-icon__wrapper bookmarks-button__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Добавить в закладки</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-favorite"></use></svg></span> <span title="Количество пользователей, добавивших публикацию в закладки" class="bookmarks-button__counter">
    44
  </span></button> <!----> <div title="Поделиться" class="tm-sharing tm-data-icons__item"><button type="button" class="tm-sharing__button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="tm-sharing__icon"><path fill="currentColor" d="M10.33.275l9.047 7.572a.2.2 0 010 .306l-9.048 7.572a.2.2 0 01-.328-.153V11c-8 0-9.94 6-9.94 6S-1 5 10 5V.428a.2.2 0 01.328-.153z"></path></svg></button> <!----></div> <DIV class="v-portal" style="display:none;"></DIV></div> </div></div> <!----> <!----> <div class="tm-article-presenter__footer"><div class="tm-article-blocks"><!----> <section class="tm-block tm-block_spacing-bottom"><!----> <div class="tm-block__body tm-block__body_variant-balanced"><div class="tm-article-author"><div class="tm-article-author__company"><div class="tm-article-author__company-card"><div class="tm-company-snippet"><a href="/ru/company/lamoda/profile/" class="tm-company-snippet__logo-link"><div class="tm-entity-image"><img alt="" height="40" src="//habrastorage.org/getpro/habr/company/cdd/710/61a/cdd71061ae61f0c4f273ddcaaec533c7.png" width="40" class="tm-entity-image__pic"></div></a> <div class="tm-company-snippet__info"><a href="/ru/company/lamoda/profile/" class="tm-company-snippet__title">Lamoda</a> <div class="tm-company-snippet__description">Code the lifestyle</div></div></div> <div class="tm-article-author__buttons"><!----> <!----></div></div> <div class="tm-article-author__company-contacts"><a href="http://tech.lamoda.ru" rel="noopener" target="_blank" class="tm-article-author__contact">
      Сайт
    </a><a href="http://lamoda.ru" rel="noopener" target="_blank" class="tm-article-author__contact">
      Сайт
    </a></div> <div class="tm-article-author__separator"></div></div> <div class="tm-user-card tm-article-author__user-card tm-user-card_variant-article"><div class="tm-user-card__info-container"><div class="tm-user-card__header"><div class="tm-user-card__header-data"><a href="/ru/users/ewolf/" class="tm-user-card__userpic tm-user-card__userpic_size-40"><div class="tm-entity-image"><img alt="" src="//habrastorage.org/getpro/habr/avatars/4c9/3da/2ed/4c93da2edb71072b412f00cbf40e9941.png" class="tm-entity-image__pic"></div></a> <div class="tm-user-card__meta"><div title=" 28 голосов " class="tm-karma tm-user-card__karma"><div class="tm-karma__votes tm-karma__votes_positive">
    28
  </div> <div class="tm-karma__text">
    Карма
  </div></div> <div title="Рейтинг пользователя" class="tm-rating tm-user-card__rating"><div class="tm-rating__header"> <div class="tm-rating__counter">0</div></div> <div class="tm-rating__text">
    Рейтинг
  </div></div></div></div></div> <div class="tm-user-card__info tm-user-card__info_variant-article"><div class="tm-user-card__title tm-user-card__title_variant-article"><span class="tm-user-card__name tm-user-card__name_variant-article">Павел Агалецкий</span> <a href="/ru/users/ewolf/" class="tm-user-card__nickname tm-user-card__nickname_variant-article">
          @ewolf
        </a> <!----></div> <p class="tm-user-card__short-info tm-user-card__short-info_variant-article">Пользователь</p></div></div> <div class="tm-user-card__buttons tm-user-card__buttons_variant-article"><!----> <!----> <!----> <!----> <!----></div></div> <!----></div> <DIV class="v-portal" style="display:none;"></DIV></div> <!----></section> <div class="tm-article-blocks__comments"><div class="tm-article-page-comments"><div class="tm-article-comments-counter-link tm-article-comments-counter-button"><a href="/ru/company/lamoda/blog/451644/comments/" class="tm-article-comments-counter-link__link tm-article-comments-counter-link__link_button-style"><svg height="16" width="16" class="tm-svg-img tm-article-comments-counter-link__icon tm-article-comments-counter-link__icon_contrasted"><title>Комментарии</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-comments"></use></svg> <span class="tm-article-comments-counter-link__value tm-article-comments-counter-link__value_contrasted">
       Комментарии 10 
    </span></a> <!----></div></div></div>  <!---->  <!----> <!----></div></div></div></div></div> <div class="tm-page__sidebar"><div class="tm-layout-sidebar"><div class="tm-layout-sidebar__placeholder_initial"></div> <div class="tm-sexy-sidebar tm-sexy-sidebar_initial" style="margin-top:0px;"><!----> <section class="tm-block tm-block_spacing-bottom"><header class="tm-block__header"><h2 class="tm-block__title">Информация</h2> <!----></header> <div class="tm-block__body"><div class="tm-company-basic-info"><dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Дата основания</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap"><time datetime="2011-03-04T21:00:00.000Z" title="2011-03-05, 00:00">5  марта  2011</time></dd></dl> <dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Местоположение</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap">
    Россия
  </dd></dl> <dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Сайт</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap"><a href="http://tech.lamoda.ru" target="_blank" class="tm-company-basic-info__link">
      tech.lamoda.ru
    </a></dd></dl> <dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Численность</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap">
    5 001–10 000 человек
  </dd></dl> <dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Дата регистрации</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap"><time datetime="2018-09-20T09:17:42.000Z" title="2018-09-20, 12:17">20  сентября  2018</time></dd></dl> <!----></div></div> <!----></section> <div class="tm-company-widgets"></div> <!----></div></div></div></div></div></div></main> <!----></div> <div class="tm-footer-menu"><div class="tm-page-width"><div class="tm-footer-menu__container"><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Ваш аккаунт
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="/kek/v1/auth/habrahabr/?back=/ru/company/lamoda/blog/451644/&amp;hl=ru" rel="nofollow" target="_self">
                Войти
              </a></li><li class="tm-footer-menu__list-item"><a href="/kek/v1/auth/habrahabr-register/?back=/ru/company/lamoda/blog/451644/&amp;hl=ru" rel="nofollow" target="_self">
                Регистрация
              </a></li></ul></div></div><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Разделы
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="/ru/" class="footer-menu__item-link router-link-active">
                Публикации
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/news/" class="footer-menu__item-link">
                Новости
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/hubs/" class="footer-menu__item-link">
                Хабы
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/companies/" class="footer-menu__item-link">
                Компании
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/users/" class="footer-menu__item-link">
                Авторы
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/sandbox/" class="footer-menu__item-link">
                Песочница
              </a></li></ul></div></div><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Информация
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="/ru/docs/help/" class="footer-menu__item-link">
                Устройство сайта
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/authors/codex/" class="footer-menu__item-link">
                Для авторов
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/companies/corpblogs/" class="footer-menu__item-link">
                Для компаний
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/docs/transparency/" class="footer-menu__item-link">
                Документы
              </a></li><li class="tm-footer-menu__list-item"><a href="https://account.habr.com/info/agreement" target="_blank">
                Соглашение
              </a></li><li class="tm-footer-menu__list-item"><a href="https://account.habr.com/info/confidential/" target="_blank">
                Конфиденциальность
              </a></li></ul></div></div><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Услуги
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="https://docs.google.com/presentation/d/e/2PACX-1vQLwRfQmXibiUlWaRg-BAc38s7oM3lJiaPju7qmdJsp8ysIvZ_G-Npem0njJLMozE2bPHMpDqiI5hhy/pub?start=false&amp;loop=false&amp;delayms=60000&amp;slide=id.g91a03369cd_4_297" target="_blank">
                Реклама
              </a></li><li class="tm-footer-menu__list-item"><a href="https://habrastorage.org/storage/stuff/habr/service_price.pdf" target="_blank">
                Тарифы
              </a></li><li class="tm-footer-menu__list-item"><a href="https://docs.google.com/presentation/d/e/2PACX-1vQJJds8-Di7BQSP_guHxICN7woVYoN5NP_22ra-BIo4bqnTT9FR6fB-Ku2P0AoRpX0Ds-LRkDeAoD8F/pub?start=false&amp;loop=false&amp;delayms=60000" target="_blank">
                Контент
              </a></li><li class="tm-footer-menu__list-item"><a href="https://tmtm.timepad.ru/" target="_blank">
                Семинары
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/megaprojects/" class="footer-menu__item-link">
                Мегапроекты
              </a></li></ul></div></div></div></div></div> <div class="tm-footer"><div class="tm-page-width"><div class="tm-footer__container"><!----> <div class="tm-footer__social"><a href="https://www.facebook.com/habrahabr.ru" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Facebook</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-facebook"></use></svg></a><a href="https://twitter.com/habr_com" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Twitter</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-twitter"></use></svg></a><a href="https://vk.com/habr" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>VK</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-vkontakte"></use></svg></a><a href="https://telegram.me/habr_com" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Telegram</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-telegram"></use></svg></a><a href="https://www.youtube.com/channel/UCd_sTwKqVrweTt4oAKY5y4w" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Youtube</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-youtube"></use></svg></a><a href="https://zen.yandex.ru/habr" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Яндекс Дзен</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-zen"></use></svg></a></div> <DIV class="v-portal" style="display:none;"></DIV> <button class="tm-footer__link"><!---->
        Настройка языка
      </button> <a href="/ru/about" class="tm-footer__link">
        О сайте
      </a> <a href="/ru/feedback/" class="tm-footer__link">
        Техническая поддержка
      </a> <!----> <a href="/berserk-mode-nope" class="tm-footer__link">
        Вернуться на старую версию
      </a> <div class="tm-footer-copyright"><span class="tm-copyright"><span class="tm-copyright__years">© 2006–2021 </span> <span class="tm-copyright__name">«<a href="https://company.habr.com/" rel="noopener" target="_blank" class="tm-copyright__link">Habr</a>»</span></span></div></div></div></div> <!----> <!----></div> <div class="vue-portal-target"></div></div>
<script>window.__INITIAL_STATE__={"adblock":{"hasAcceptableAdsFilter":false,"hasAdblock":false},"articlesList":{"articlesList":{"451644":{"id":"451644","timePublished":"2019-05-14T13:45:01+00:00","isCorporative":true,"lang":"ru","titleHtml":"Деплой приложений в VM, Nomad и Kubernetes","leadData":{"textHtml":"Всем привет! Меня зовут Павел Агалецкий. Я работаю тимлидом в команде, которая разрабатывает систему доставки Lamoda. В 2018 году я выступал на конференции HighLoad++, а сегодня хочу представить расшифровку своего доклада.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nМоя тема посвящена опыту нашей компании по деплою систем и сервисов в разные среды. Начиная от наших доисторических времен, когда мы деплоили все системы в обычные виртуальные сервера, заканчивая постепенным переход от Nomad к деплою в Kubernetes. Расскажу о том, почему мы это сделали и какие у нас были в процессе проблемы.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv class=\"oembed\"\u003E\u003Cdiv\u003E\u003Cdiv style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.2493%;\"\u003E\u003Ciframe src=\"https:\u002F\u002Fwww.youtube.com\u002Fembed\u002Foqrb7dWECSo?rel=0&amp;showinfo=1&amp;hl=en-US\" style=\"border: 0; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" allowfullscreen scrolling=\"no\"\u003E\u003C\u002Fiframe\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E","imageUrl":null,"buttonTextHtml":null,"image":null},"editorVersion":"1.0","postType":"article","postLabels":[],"author":{"scoreStats":{"score":28,"votesCount":28},"rating":0,"relatedData":null,"contacts":[],"authorContacts":[],"paymentDetails":{"paymentYandexMoney":null,"paymentPayPalMe":null,"paymentWebmoney":null},"id":"19465","alias":"ewolf","fullname":"Павел Агалецкий","avatarUrl":"\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Favatars\u002F4c9\u002F3da\u002F2ed\u002F4c93da2edb71072b412f00cbf40e9941.png","speciality":"Пользователь"},"statistics":{"commentsCount":10,"favoritesCount":44,"readingCount":6406,"score":10,"votesCount":16},"hubs":[{"relatedData":null,"id":"22117","alias":"lamoda","type":"corporative","title":"Блог компании Lamoda","titleHtml":"Блог компании Lamoda","isProfiled":false},{"relatedData":null,"id":"221","alias":"sys_admin","type":"collective","title":"Системное администрирование","titleHtml":"Системное администрирование","isProfiled":true}],"flows":[{"id":"6","alias":"admin","title":"Администрирование"}],"relatedData":null,"textHtml":"\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\"\u003EВсем привет! Меня зовут Павел Агалецкий. Я работаю тимлидом в команде, которая разрабатывает систему доставки Lamoda. В 2018 году я выступал на конференции HighLoad++, а сегодня хочу представить расшифровку своего доклада.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nМоя тема посвящена опыту нашей компании по деплою систем и сервисов в разные среды. Начиная от наших доисторических времен, когда мы деплоили все системы в обычные виртуальные сервера, заканчивая постепенным переход от Nomad к деплою в Kubernetes. Расскажу о том, почему мы это сделали и какие у нас были в процессе проблемы.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv class=\"oembed\"\u003E\u003Cdiv\u003E\u003Cdiv style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.2493%;\"\u003E\u003Cdiv class=\"tm-iframe_temp\" data-src=\"https:\u002F\u002Fwww.youtube.com\u002Fembed\u002Foqrb7dWECSo?rel=0&amp;showinfo=1&amp;hl=en-US\" data-style=\"border: 0; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" id=\"\" width=\"\"\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Ca name=\"habracut\"\u003E\u003C\u002Fa\u003E\u003Cbr\u002F\u003E\r\n \u003Ch1\u003EДеплой приложений на VM\u003C\u002Fh1\u003E\u003Cbr\u002F\u003E\r\nНачнем с того, что 3 года назад все системы и сервисы компании деплоились на обычные виртуальные сервера. Технически было организовано так, что весь код наших систем лежал и собирался за счет средств автоматической сборки, с помощью jenkins. При помощи Ansible он из нашей системы контроля версий раскатывался уже на виртуальные сервера. При этом каждая система, которая была в нашей компании, деплоилась, как минимум, на 2 сервера: одна из них — на head, вторая — на tail. Эти две системы были абсолютно идентичны между собой по всем своим настройкам, мощности, конфигурации и прочему. Отличие между ними было лишь в том, что head получал пользовательский трафик, а tail никогда трафик пользователей на себя не получал. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nДля чего это было сделано? \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nКогда мы деплоили новые релизы нашего приложения, то хотели обеспечить возможность бесшовной раскатки, то есть без заметных последствий для пользователей. Достигалось это за счет того, что очередной собранный релиз с помощью Ansible выкатывался на tail. Там люди, которые занимались деплоем, могли проверить и убедиться, что все хорошо: все метрики, разделы и приложения работают; запускаются нужные скрипты. Только после того, как они убеждались, что все ок, трафик переключался. Он начинал идти на тот сервер, который до этого был tail. А тот, который до этого был head-ом, оставался без пользовательского трафика, при этом с имеющейся на нем предыдущей версией нашего приложения.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nТаким образом, для пользователей это было бесшовно. Потому что переключение одномоментное, так как это просто переключение балансировщика. Очень легко можно откатиться на предыдущую версию, просто переключив балансировщик назад. Также мы могли убедиться в способности приложения на продакшн еще до того, как на него пойдет пользовательский трафик, что было достаточно удобно. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nКакие мы видели во всем этом достоинства?\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Col\u003E\r\n\u003Cli\u003EПрежде всего, это достаточно \u003Cb\u003Eпросто работает.\u003C\u002Fb\u003E Всем понятно, как функционирует подобная схема деплоя, потому что большинство людей когда-либо деплоили на обычные виртуальные сервера.\u003C\u002Fli\u003E\r\n\u003Cli\u003EЭто достаточно \u003Cb\u003Eнадежно\u003C\u002Fb\u003E, так как технология деплоя простая, опробованная тысячами компаний. Миллионы серверов деплоятся именно так. Сложно что-то сломать. \u003C\u002Fli\u003E\r\n\u003Cli\u003EИ наконец, мы могли получить \u003Cb\u003Eатомарные деплои\u003C\u002Fb\u003E. Деплои, которые для пользователей происходят одномоментно, без заметного этапа переключения между старой версией и новой. \u003C\u002Fli\u003E\r\n\u003C\u002Fol\u003E\u003Cbr\u002F\u003E\r\nНо в этом всем мы также видели несколько недостатков: \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Col\u003E\r\n\u003Cli\u003EПомимо продакшн-среды, среды разработки, есть и другие среды. Например, qa и preproduction. На тот момент у нас было много серверов и около 60 сервисов. По этой причине приходилось \u003Cb\u003Eдля каждого сервиса поддерживать актуальную для него версию \u003C\u002Fb\u003Eвиртуальной машины. Причем, если вы хотите обновить библиотеки или поставить новые зависимости, вам необходимо это сделать во всех средах. Также нужно было синхронизировать время, когда вы собираетесь деплоить очередную новую версию вашего приложения, со временем, когда devops выполнит необходимые настройки окружения. В таком случае легко попасть в ситуацию, когда окружение у нас будет несколько отличаться сразу во всех средах подряд. Например, в QA-среде будут одни версии библиотек, а в продакшн — другие, что приведет к проблемам. \u003C\u002Fli\u003E\r\n\u003Cli\u003E\u003Cb\u003EСложность в обновлении зависимостей\u003C\u002Fb\u003E вашего приложения. Это зависит не от вас, а от другой команды. А именно, от команды devops, которая поддерживает сервера. Вы должны поставить для них соответствующую задачу и дать описание того, что хотите сделать.\u003C\u002Fli\u003E\r\n\u003Cli\u003EВ то время мы также хотели разделить имевшиеся у нас большие крупные монолиты на отдельные маленькие сервисы, так как понимали, что их будет становиться все больше и больше. На тот момент у нас уже было их более 100. Необходимо было для каждого нового сервиса создавать отдельную новую виртуальную машину, которую также надо обслуживать и деплоить. Кроме этого, нужна не одна машина, а, как минимум, две. К этому всему еще добавляется QA-среда. Это вызывает проблемы и делает для вас создание и запуск новых систем более \u003Cb\u003Eсложным, дорогостоящим и долгим процессом.\u003C\u002Fb\u003E\u003C\u002Fli\u003E\r\n\u003C\u002Fol\u003E\u003Cbr\u002F\u003E\r\nПоэтому мы приняли решение, что будет удобнее перейти от деплоя обычных виртуальных машин к деплою наших приложений в контейнере docker. При наличии docker нужна система, которая сможет запустить приложение в кластере, так как вы не сможете поднять просто так контейнер. Обычно хочется следить за тем, сколько контейнеров поднято, чтобы они поднимались автоматически. По этой причине нам нужно было выбрать систему управления. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nМы долго размышляли над тем, какую из них можно взять. Дело в том, что на тот момент этот стек деплоя на обычные виртуальные сервера был у нас несколько устаревшим, так как там были не самые свежие версии операционных систем. В какой-то момент там даже FreeBSD стояла, которую было не очень удобно поддерживать. Мы понимали, что нужно как можно быстрее мигрировать в docker. Наши девопс посмотрели на свой имеющийся опыт работы с разными решениями и выбрали такую систему как Nomad. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ch1\u003EПереход на Nomad\u003C\u002Fh1\u003E\u003Cbr\u002F\u003E\r\nNomad – это продукт компании “HashiCorp”. Также они известны другими своими решениями:\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fwebt\u002F4e\u002Fvz\u002F4e\u002F4evz4entvdaauztnu558tb-2z9u.jpeg\" alt=\"image\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002F4e\u002Fvz\u002F4e\u002F4evz4entvdaauztnu558tb-2z9u.jpeg\" data-blurred=\"true\"\u002F\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cb\u003E«Consul»\u003C\u002Fb\u003E — это средство для сервис-дискаверинга.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cb\u003E«Terraform»\u003C\u002Fb\u003E — система для управления серверами, позволяющая вам настраивать их через конфигурацию, так называемую infrastructure-as-a-code.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cb\u003E«Vagrant»\u003C\u002Fb\u003E позволяет вам разворачивать виртуальные машины локально или в облаке путем определенных файлов конфигурации. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nNomad на тот момент показался достаточно простым решением, на который можно быстро перейти без изменения всей инфраструктуры. К тому же, он достаточно легко осваивается. Поэтому именно его мы и выбрали в качестве системы фильтрации нашего контейнера. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nЧто нужно для того, чтобы вообще задеплоить вашу систему в Nomad? \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Col\u003E\r\n\u003Cli\u003EПрежде всего нужен \u003Cb\u003Edocker image\u003C\u002Fb\u003E вашего приложения. Необходимо собрать его и поместить в хранилище образов docker. В нашем случае это artifactory — такая система, которая позволяет пушить в нее различные артефакты разного типа. Она умеет хранить архивы, образы docker, пакеты composer РНР, NРМ-пакеты и так далее. \u003C\u002Fli\u003E\r\n\u003Cli\u003EТакже необходим\u003Cb\u003E конфигурационный файл\u003C\u002Fb\u003E, который скажет Nomad, что, куда и в каком количестве вы хотите задеплоить. \u003C\u002Fli\u003E\r\n\u003C\u002Fol\u003E\u003Cbr\u002F\u003E\r\nКогда мы говорим про Nomad, то в качестве формата информационного файла он использует язык HСL, что расшифровывается как \u003Ci\u003EHashiCorp Configuration Language\u003C\u002Fi\u003E. Это такое надмножество над Yaml, которое позволяет вам описать ваш сервис в терминах Nomad. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fwebt\u002Fvg\u002Fq9\u002Fvb\u002Fvgq9vb_izh4i890ro7giihibz6g.jpeg\" alt=\"image\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fvg\u002Fq9\u002Fvb\u002Fvgq9vb_izh4i890ro7giihibz6g.jpeg\" data-blurred=\"true\"\u002F\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nОн позволяет сказать, сколько контейнеров хотите задеплоить, из каких images передать им различные параметры при деплое. Таким образом, вы скармливаете этот файл Nomad, а он запускает в соответствии с ним контейнеры в продакшн. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nВ нашем случае мы поняли, что просто писать под каждый сервис абсолютно одинаковые, идентичные HСL-файлы будет не очень удобно, потому что сервисов много и иногда хочется их обновить. Бывает такое, что один сервис задеплоен не в одном экземпляре, а в самых разных. Например, одна из систем, которая у нас находится в продакшн, имеет более 100 инстансов в проде. Они запускаются из одних и тех же образов, но отличаются конфигурационными настройками и конфигурационными файлами. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nПоэтому мы решили, что нам будет удобно хранить все наши конфигурационные файлы для деплоя в одном общем репозитории. Таким образом, они становились обозреваемы: их было легко поддерживать и можно было увидеть, какие системы у нас есть. В случае необходимости также несложно что-то обновить или поменять. Добавить новую систему тоже не составит труда — достаточно просто завести конфигурационный файл внутри новой директории. Внутри нее лежат файлы: service.hcl, который содержит описание нашего сервиса, и некоторые env-файлы, которые позволяют этот самый сервис, будучи задеплоенным в продакшн, настроить. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fwebt\u002F-g\u002Fl7\u002F8h\u002F-gl78hl7nbmdbhbuacuntgxw4ow.jpeg\" alt=\"image\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002F-g\u002Fl7\u002F8h\u002F-gl78hl7nbmdbhbuacuntgxw4ow.jpeg\" data-blurred=\"true\"\u002F\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nОднако некоторые наши системы задеплоены в проде не в одном экземпляре, а в нескольких сразу. Поэтому мы решили, что нам удобно будет хранить не конфиги в чистом виде, а их шаблонизированный вид. И в качестве языка шаблонизации мы выбрали \u003Ci\u003Ejinja 2\u003C\u002Fi\u003E. В таком формате у нас хранятся как конфиги самого сервиса, так и env-файлы, нужные для него. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nПомимо этого, мы поместили в репозиторий общий для всех проектов скрипт-деплой, который позволяет запустить и задеплоить ваш сервис в продакшн, в нужный environment, в нужный таргет. В случае, когда мы превратили наш HCL-конфиг в шаблон, то тот HСL-файл, который до этого был обычным конфигом Nomad, в этом случае стал выглядеть несколько иначе.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fwebt\u002F9a\u002Fib\u002Fzv\u002F9aibzvsme34ra2eg53bjfbqt1tw.jpeg\" alt=\"image\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002F9a\u002Fib\u002Fzv\u002F9aibzvsme34ra2eg53bjfbqt1tw.jpeg\" data-blurred=\"true\"\u002F\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nТо есть мы заменили какие-то переменные места конфига на вставки переменных, которые берутся из env-файлов или из других источников. Помимо этого, мы получили возможность собирать HСL-файлы динамически, то есть мы можем применять не только обычные вставки переменных. Так как jinja поддерживает циклы и условия, туда же можно делать конфигурационные файлы, которые меняются в зависимости от того, куда именно вы деплоите свои приложения. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nНапример, вы хотите деплоить ваш сервис в препродакшн и в продакшн. Допустим, что в препродакшн вы не хотите запускать крон-cкрипты, а просто хотите видеть сервис на отдельном домене, чтобы убедиться, что он функционирует. Для любого, кто деплоит сервис, процесс выглядит очень просто и прозрачно. Достаточно выполнить файл deploy.sh, указать, какой сервис вы хотите задеплоить и в какой таргет. Например, вы хотите задеплоить некоторую систему в Россию, в Белоруссию или Казахстан. Для этого достаточно просто поменять один из параметров, и у вас соберется правильный конфигурационный файл. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nКогда сервис Nomad уже оказывается у вас задеплоенным в кластер, он выглядит следующим образом.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fwebt\u002Fnu\u002Fau\u002F8d\u002Fnuau8dqdcwft0boyw57x37wrkcm.jpeg\" alt=\"image\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fnu\u002Fau\u002F8d\u002Fnuau8dqdcwft0boyw57x37wrkcm.jpeg\" data-blurred=\"true\"\u002F\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nДля начала вам нужен некоторый балансировщик снаружи, который будет принимать весь пользовательский трафик в себя. Он будет работать вместе с Consul и узнавать у него, где, на какой ноде, по какому IP-адресу находится конкретный сервис, который соответствует тому или иному доменному имени. Сервисы в Consul появляются из самого Nomad. Поскольку это продукты одной и той же компании, они неплохо связаны между собой. Можно сказать, что Nomad из коробки умеет регистрировать все запускаемые в нем сервисы внутри Consul. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nПосле того как ваш внешний балансировщик узнает о том, в какой сервис необходимо отправить трафик, он перенаправляет его в соответствующий контейнер или в несколько контейнеров, которые соответствуют вашему приложению. Естественно, что при этом необходимо думать и о безопасности. Даже несмотря на то, что все сервисы запускаются на одних и тех же виртуальных машинах в контейнерах, обычно для этого требуется запретить свободный доступ из любого сервиса к любому другому. Мы достигали этого за счет сегментации. Каждый сервис запускался в своей виртуальной сети, на которой были прописаны правила маршрутизации и правила разрешения\u002Fзапрета доступа к другим системами и сервисам. Они могли находиться как внутри этого кластера, так и вне него. Например, если вы хотите запретить сервису коннектиться к определённой базе данных, это можно сделать путем сегментации на уровне сети. То есть даже по ошибке вы не можете случайно подсоединяться из тестового окружения к вашей продакшн базе.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nВо что нам обошелся процесс перехода в плане человеческих ресурсов? \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nПримерно 5-6 месяцев занял переход всей компании в Nomad. Мы переходили посервисно, но в достаточно быстром темпе. Каждая команда должна была создать свои собственные контейнеры для сервисов. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nУ нас принят такой подход, что каждая команда отвечает за docker images своих систем самостоятельно. Девопс же предоставляют общую инфраструктуру, необходимую для деплоя, то есть поддержку самого кластера, поддержку CI-системы и так далее. И на тот момент у нас более 60 систем переехали в Nomad, получилось порядка 2 тысяч контейнеров. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nДевопс отвечает за общую инфраструктуру всего, что связано с деплоем, с серверами. А каждая команда разработки в свою очередь отвечает за реализацию контейнеров под свою конкретную систему, поскольку именно команда знает, что ей вообще нужно в том или ином контейнере.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ch1\u003EПричины отказа от Nomad\u003C\u002Fh1\u003E\u003Cbr\u002F\u003E\r\nКакие достоинства мы получили, перейдя на деплой с помощью Nomad и docker в том числе?\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Col\u003E\r\n\u003Cli\u003EМы\u003Cb\u003E обеспечили одинаковые условия\u003C\u002Fb\u003E для всех сред. В девеломпенте, QA-среде, препродакшне, продакшне используются одни и те же образы контейнеров, с одними и теми же зависимостями. Соответственно у вас практически отсутствует шанс того, что в продакшн окажется не то, что вы до этого протестировали у себя локально или на тестовом окружении. \u003C\u002Fli\u003E\r\n\u003Cli\u003EТакже мы выяснили, что достаточно \u003Cb\u003Eлегко добавить новый сервис\u003C\u002Fb\u003E. Любые новые системы с точки зрения деплоя запускаются очень просто. Достаточно пойти в репозиторий, хранящий конфиги, добавить туда очередной конфиг для вашей системы, и у вас все готово. Вы можете деплоить вашу систему в продакшн без дополнительных усилий от девопс. \u003C\u002Fli\u003E\r\n\u003Cli\u003EВсе \u003Cb\u003Eконфигурационные файлы\u003C\u002Fb\u003E в одном общем репозитории \u003Cb\u003Eоказались обозреваемые\u003C\u002Fb\u003E. В тот момент, когда мы деплоили наши системы с помощью виртуальных серверов, мы использовали Ansible, в котором конфиги лежали в одном и том же репозитории. Однако для большинства разработчиков работать было с этим несколько сложнее. Тут объем конфигов и кода, который вам нужно добавить, чтобы задеплоить сервис, стал намного меньше. Плюс для девопс очень легко поправить его или поменять. В случае переходов, например, на новой версии Nomad, они могут взять и массово обновить все операционные файлы, лежащие в одном и том же месте.\u003C\u002Fli\u003E\r\n\u003C\u002Fol\u003E\u003Cbr\u002F\u003E\r\nНо мы также столкнулись и с несколькими недостатками: \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nОказалось, что мы \u003Cb\u003Eне смогли достичь бесшовности деплоев \u003C\u002Fb\u003Eв случае Nomad. При выкатке контейнеров из разных условий могло получиться так, что он оказывался запущенным, и Nomad воспринимал его как готовый к принятию трафика контейнер. Это происходило еще до того, как приложение внутри него успело запуститься. По этой причине система на недолгий срок начинала выдавать 500-е ошибки, потому что трафик начинал идти на контейнер, который еще не готов его принять. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nМы столкнулись с некоторыми \u003Cb\u003Eбагами\u003C\u002Fb\u003E. Самый существенный баг заключается в том, что Nomad не очень хорошо воспринимает большой кластер, если у вас много систем и контейнеров. Когда вы хотите вывезти в обслуживание один из серверов, который включен в кластер Nomad, есть достаточно большая вероятность того, что кластер почувствует себя не очень хорошо и развалится на части. Часть контейнеров может, например, упасть и не подняться — это вам впоследствии очень дорого обойдется, если все ваши системы в продакшн находятся именно в кластере, управляемом Nomad. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nПоэтому мы решили подумать, куда нам идти дальше. На тот момент мы стали намного лучше осознавать, чего хотим добиться. А именно: хотим надежности, немножко больше функций, чем дает Nomad, и более зрелую, более стабильную систему. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nВ этом плане наш выбор пал на Kubernetes как на самую популярную платформу для запуска кластеров. Особенно при условии того, что размер и количество наших контейнеров был достаточно большой. Для таких целей Kubernetes показался самой подходящей системой из тех, что мы могли посмотреть. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ch1\u003EПереход в Kubernetes\u003C\u002Fh1\u003E\u003Cbr\u002F\u003E\r\nНемножко расскажу о том, какие есть основные понятия Kubernetes и чем они отличаются от Nomad. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fwebt\u002Fpv\u002Feh\u002Fva\u002Fpvehvavusoxszoxum9bsuwyjqbc.jpeg\" alt=\"image\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fpv\u002Feh\u002Fva\u002Fpvehvavusoxszoxum9bsuwyjqbc.jpeg\" data-blurred=\"true\"\u002F\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nВ первую очередь самым базовым понятием в Kubernetes является понятие pod. \u003Cb\u003EPod\u003C\u002Fb\u003E — это группа одного или нескольких контейнеров, которые всегда запускаются вместе. И они работают как будто всегда строго на одной виртуальной машине. Они доступны друг другу по айпишнику 127.0.0.1 на разных портах. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nПредположим, что у вас есть РНР-приложение, которое состоит из nginx и php-fpm – классическая схема. Скорее всего, вы захотите, чтобы и nginx и php-fpm контейнеры были всегда вместе. Kubernetes позволяет этого добиться, описав их как один общий pod. Как раз этого мы и не могли получить при помощи Nomad.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nВторым понятием является \u003Cb\u003Edeployment\u003C\u002Fb\u003E. Дело в том, что pod сам по себе – это эфемерная вещь, она запускается и исчезает. Хотите ли вы сначала убивать все ваши предыдущие контейнеры, а потом запускать сразу новые версии или же вы хотите выкатывать их постепенно – вот именно за этот процесс отвечает понятие deployment. Он описывает то, как вы деплоите ваши podы, в каком количестве и, как их обновлять. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nТретьим понятием является \u003Cb\u003Eservice\u003C\u002Fb\u003E. Ваш service – это фактически ваша система, которая принимает в себя некий трафик, а потом направляет его на один или несколько podов, соответствующих вашему сервису. То есть он позволяет сказать, что весь входящий трафик на такой-то сервис с таким-то именем необходимо отправлять на конкретно эти podы. И при этом он обеспечивает вам балансировку трафика. То есть вы можете запустить два podа вашего приложения, и весь входящий трафик будет равномерно балансироваться между относящимися к этому сервису podами.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nИ четвертое основное понятие — \u003Cb\u003EIngress\u003C\u002Fb\u003E. Это сервис, который запускается в кластере Kubernetes. Он выступает как внешний балансировщик нагрузки, который принимает на себя все запросы. За счет API Kubernetes Ingress может определять, куда эти запросы надо отправить. Причем делает он это очень гибко. Вы можете сказать, что все запросы на этот хост и такой-то URL отправляем на этот сервис. А эти запросы, приходящие на этот хост и на другой URL отправляем на другой сервис. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nСамое крутое с точки зрения того, кто разрабатывает приложение — это то, что вы способны этим всем управлять самостоятельно. Задав конфиг Ingress, можете весь трафик, приходящий на такой-то API, отправлять на отдельные контейнеры, прописанные, например, на Go. А вот этот трафик, приходящий на тот же домен, но на другой URL, отправлять на контейнеры, написанные на РНР, где много логики, но они не очень скоростные.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nЕсли сравнить все эти понятия с Nomad, то можно сказать, что первые три понятия – это все вместе Service. А последнее понятие в самом Nomad отсутствует. Мы в качестве него использовали внешний балансировщик: это может быть haproxy, nginx, nginx+ и так далее. В случае с кубом вам не надо вводить это дополнительное понятие отдельно. Однако, если посмотреть на Ingress внутри, то это либо nginx, либо haproxy, либо traefik, но как бы встроенный в Kubernetes. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nВсе описанные мною понятия – это, по сути дела, ресурсы, которые существуют внутри кластера Kubernetes. Для их описания в кубе используется yaml-формат, более читаемый и привычный, чем НСL-файлы в случае с Nomad. Но структурно они описывают в случае, например, podа то же самое. Они говорят – хочу задеплоить такие-то podы туда-то, с такими-то имаджи, в таком-то количестве. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fwebt\u002F2k\u002Fka\u002F53\u002F2kka53a1vp1lm0rnl4xbhmcpyu4.jpeg\" alt=\"image\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002F2k\u002Fka\u002F53\u002F2kka53a1vp1lm0rnl4xbhmcpyu4.jpeg\" data-blurred=\"true\"\u002F\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nПомимо этого, мы поняли, что не хотим руками создавать каждый отдельный ресурс: deployment, сервисы, Ingress и прочее. Вместо этого мы хотели при деплое описывать нашу каждую систему, имеющуюся в терминах Kubernetes, чтобы не приходилось вручную пересоздавать в нужном порядке все необходимые зависимости ресурсов. В качестве такой системы, которая позволила нам это сделать, был выбран Helm. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ch1\u003EОсновные понятия в Helm\u003C\u002Fh1\u003E\u003Cbr\u002F\u003E\r\nHelm – это \u003Cb\u003Eпакетный менеджер\u003C\u002Fb\u003E для Kubernetes. Он очень похож на то, как работают пакетные менеджеры в языках программирования. Они позволяют вам хранить сервис, состоящий из, например, deployment nginx, deployment php-fpm, конфига для Ingress, configmaps (это сущность, которая позволяет вам задать env и другие параметры для вашей системы) в виде так называемых чартов. При этом Helm \u003Cb\u003Eработает поверх Kubernetes\u003C\u002Fb\u003E. То есть это не какая-то система, стоящая в стороне, а просто ещё один сервис, запускаемый внутри куба. Вы взаимодействуете с ним по его API через консольную команду. Его удобство и прелесть в том, что даже если helm сломается или вы его удалите из кластера, то ваши сервисы не исчезнут, так как helm служит по сути только для запуска системы. За работоспособность и состояние сервисов дальше отвечает сам Kubernetes. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nТакже мы поняли, что \u003Cb\u003Eшаблонизация\u003C\u002Fb\u003E, которую до этого были вынуждены делать самостоятельно через внедрение jinja в наши конфиги, является одной из основных возможностей helm. Все конфиги, которые вы создаете для ваших систем, хранятся в helm в виде шаблонов, похожих немножко на jinja, но, на самом деле, использующих шаблонизацию языка Go, на котором helm написан, как и Kubernetes. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nHelm добавляет нам еще несколько дополнительных понятий. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cb\u003EChart\u003C\u002Fb\u003E — это описание вашего сервиса. В других пакетных менеджерах его назвали бы пакет, bundle или что-то подобное. Здесь это называется chart. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cb\u003EValues \u003C\u002Fb\u003E– это переменные, которые вы хотите использовать для сборки ваших конфигов из шаблонов. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cb\u003ERelease\u003C\u002Fb\u003E. Каждый раз сервис, который деплоится с помощью helm, получает инкрементальную версию релиза. Helm помнит, какой был конфиг сервиса при предыдущем, позапрошлом релизе и так далее. Поэтому, если нужно откатиться, достаточно выполнить команду helm callback, указав ему предыдущую версию релиза. Даже если в момент отката соответствующая конфигурация в вашем репозитории не будет доступна, helm все равно помнит, какой она была, и откатит вашу систему на то состояние, в котором она была в предыдущем релизе. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nВ случае, когда мы используем helm, обычные конфиги для Kubernetes тоже превращаются в шаблоны, в которых есть возможность использовать переменные, функции, применять условные операторы. Таким образом, вы можете собирать конфиг вашего сервиса в зависимости от среды.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fwebt\u002Fdc\u002Flr\u002Ffh\u002Fdclrfhbdr29ms0gouz_pvd8xz44.jpeg\" alt=\"image\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fdc\u002Flr\u002Ffh\u002Fdclrfhbdr29ms0gouz_pvd8xz44.jpeg\" data-blurred=\"true\"\u002F\u003E \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nНа практике мы решили поступить чуть-чуть иначе, чем мы делали в случае с Nomad. Если в Nomad в одном репозитории хранились и конфиги для деплоя, и n-переменные, которые нужны для того, чтобы задеплоить наш сервис, то здесь мы решили их разделить на два отдельных репозитория. В репозитории «deploy» хранятся только n-переменные, нужные для деплоя, а в репозитории «helm» хранятся конфиги или чарты.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fwebt\u002F2r\u002Flo\u002Fyt\u002F2rloytnvlrj6nri8vj7e9-hccg8.jpeg\" alt=\"image\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002F2r\u002Flo\u002Fyt\u002F2rloytnvlrj6nri8vj7e9-hccg8.jpeg\" data-blurred=\"true\"\u002F\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nЧто это нам дало? \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nНесмотря на то, что в самих конфигурационных файлах мы не храним какие-то действительно чувствительные данные. Например, пароли к базам данных. Они хранятся в виде secrets в Kubernetes, но тем не менее, там все равно есть отдельные вещи, к которым мы не хотим давать доступ всем подряд. Поэтому доступ к репозиторию «deploy» более ограничен, а репозиторий «helm» содержит просто описание сервиса. По этой причине в него можно дать доступ безопасно большему кругу лиц. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nПоскольку у нас есть не только продакшн, но и другие среды, то благодаря такому разделению мы можем переиспользовать наши helm-чарты, чтобы деплоить сервисы не только в продакшн, но и, например, в QA-среду. Даже для того, чтобы разворачивать их локально, используя \u003Ci\u003EMinikube\u003C\u002Fi\u003E — это такая штука для локального запуска Kubernetes. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nВнутри каждого репозитория мы оставили разделение на отдельные директории для каждого сервиса. То есть внутри каждой директории находятся шаблоны, относящиеся к соответствующему чарту и описывающие те ресурсы, которые необходимо задеплоить для запуска нашей системы. В репозитории «deploy» мы оставили только энвы. В этом случае мы не стали использовать шаблонизацию с помощью jinja, потому что helm сам дает шаблонизацию из коробки – это одна из его основных функций. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nМы оставили скрипт для деплоя – deploy.sh, который упрощает и стандартизирует запуск для деплоя с помощью helm. Таким образом, для любого, кто хочет задеплоить, интерфейс деплоя выглядит точно так же, как он был в случае деплоя через Nomad. Такой же deploy.sh, название вашего сервиса, и то, куда вы хотите его задеплоить. Это приводит к тому, что внутри запускается helm. Он в свою очередь собирает конфиги из шаблонов, подставляет в них необходимые values-файлы, затем деплоит, пуская их в Kubernetes. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ch1\u003EВыводы\u003C\u002Fh1\u003E\u003Cbr\u002F\u003E\r\nСервис Kubernetes выглядит более сложным, чем Nomad. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fwebt\u002Ffe\u002Fp6\u002Fqi\u002Ffep6qibp6hhnbsmqifk2jnmg20a.jpeg\" alt=\"image\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Ffe\u002Fp6\u002Fqi\u002Ffep6qibp6hhnbsmqifk2jnmg20a.jpeg\" data-blurred=\"true\"\u002F\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nЗдесь исходящий трафик приходит в Ingress. Это как раз фронт-контроллер, который принимает на себя все запросы и впоследствии отправляет их в соответствующие данным запроса сервисы. Определяет он их на основе конфигов, которые являются частью описания вашего приложения в helm и которые разработчики задают самостоятельно. Сервис же отправляет запросы на свои podы, то есть конкретные контейнеры, балансируя входящий трафик между всеми контейнерами, которые относятся к данному сервису. Ну и, конечно, не стоит забывать про то, что от безопасности на уровне сети, мы никуда не должны уходить. Поэтому в кластере Kubernetes работает сегментация, которая основана на тегировании. Все сервисы имеют определенные теги, к которым привязаны права доступов сервисов к тем или иным внешним\u002Fвнутренним ресурсам внутри или вне кластера. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nВыполняя переход, мы увидели, что Kubernetes обладает всеми возможностями Nomad, который до этого мы использовали, а также добавляет много нового. Его можно расширять через плагины, а по факту через кастомные типы ресурсов. То есть у вас есть возможность не просто использовать что-то, что идет в Kubernetes из коробки, а создать свой собственный ресурс и сервис, который будет ваш ресурс читать. Это дает дополнительные возможности расширения вашей системы без необходимости переустановки Kubernetes и без необходимости изменений. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nПримером такого использования является Prometheus, который запускается у нас внутри кластера Kubernetes. Для того, чтобы он начал собирать метрики с того или иного сервиса, нам нужно добавить в описание сервиса дополнительный тип ресурса, так называемый сервис-монитор. Prometheus за счет того, что умеет вычитывать, будучи запущенным в Kubernetes, кастомный тип ресурсов, автоматически начинает собирать метрики с новой системы. Это достаточно удобно. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nПервый деплой, который мы сделали в Kubernetes был в марте 2018 года. И за это время мы никогда не испытывали с ним никаких проблем. Он достаточно стабильно работает без существенных багов. К тому же, мы можем его дальше расширять. На сегодняшний день нам хватает тех возможностей, которые в нем есть, а темпы развития Kubernetes нам очень нравятся. На данный момент больше 3000 контейнеров находятся в Kubernetes. Кластер занимает несколько Node. При этом он обслуживаемый, стабильный и очень контролируемый.\u003C\u002Fdiv\u003E","tags":[{"titleHtml":"kubernetes"},{"titleHtml":"nomad"},{"titleHtml":"deploy"}],"metadata":{"stylesUrls":[],"scriptUrls":[],"shareImageUrl":"https:\u002F\u002Fhabr.com\u002Fshare\u002Fpublication\u002F451644\u002Fd400912aad98631730692e19a5e658d9\u002F","shareImageWidth":1200,"shareImageHeight":630,"vkShareImageUrl":"https:\u002F\u002Fhabr.com\u002Fshare\u002Fpublication\u002F451644\u002Fd400912aad98631730692e19a5e658d9\u002F?format=vk","schemaJsonLd":"{\"@context\":\"http:\\\u002F\\\u002Fschema.org\",\"@type\":\"Article\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https:\\\u002F\\\u002Fhabr.com\\\u002Fru\\\u002Fcompany\\\u002Flamoda\\\u002Fblog\\\u002F451644\\\u002F\"},\"headline\":\"Деплой приложений в VM, Nomad и Kubernetes\",\"datePublished\":\"2019-05-14T16:45:01+03:00\",\"dateModified\":\"2019-05-14T18:22:23+03:00\",\"author\":{\"@type\":\"Person\",\"name\":\"Павел Агалецкий\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Habr\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fa_\\\u002Flk\\\u002F9m\\\u002Fa_lk9mjkccjox-zccjrpfolmkmq.png\"}},\"description\":\"Всем привет! Меня зовут Павел Агалецкий. Я работаю тимлидом в команде, которая разрабатывает систему доставки Lamoda. В 2018 году я выступал на конференции HighL...\",\"url\":\"https:\\\u002F\\\u002Fhabr.com\\\u002Fru\\\u002Fcompany\\\u002Flamoda\\\u002Fblog\\\u002F451644\\\u002F#post-content-body\",\"about\":[\"c_lamoda\",\"h_sys_admin\",\"f_admin\"],\"image\":[\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002F4e\\\u002Fvz\\\u002F4e\\\u002F4evz4entvdaauztnu558tb-2z9u.jpeg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fvg\\\u002Fq9\\\u002Fvb\\\u002Fvgq9vb_izh4i890ro7giihibz6g.jpeg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002F-g\\\u002Fl7\\\u002F8h\\\u002F-gl78hl7nbmdbhbuacuntgxw4ow.jpeg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002F9a\\\u002Fib\\\u002Fzv\\\u002F9aibzvsme34ra2eg53bjfbqt1tw.jpeg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fnu\\\u002Fau\\\u002F8d\\\u002Fnuau8dqdcwft0boyw57x37wrkcm.jpeg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fpv\\\u002Feh\\\u002Fva\\\u002Fpvehvavusoxszoxum9bsuwyjqbc.jpeg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002F2k\\\u002Fka\\\u002F53\\\u002F2kka53a1vp1lm0rnl4xbhmcpyu4.jpeg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fdc\\\u002Flr\\\u002Ffh\\\u002Fdclrfhbdr29ms0gouz_pvd8xz44.jpeg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002F2r\\\u002Flo\\\u002Fyt\\\u002F2rloytnvlrj6nri8vj7e9-hccg8.jpeg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Ffe\\\u002Fp6\\\u002Fqi\\\u002Ffep6qibp6hhnbsmqifk2jnmg20a.jpeg\"]}","metaDescription":"Всем привет! Меня зовут Павел Агалецкий. Я работаю тимлидом в команде, которая разрабатывает систему доставки Lamoda. В 2018 году я выступал на конференции HighLoad++, а сегодня хочу представить...","mainImageUrl":null,"amp":false},"polls":[],"commentsEnabled":true,"rulesRemindEnabled":false,"votesEnabled":true,"status":"published","plannedPublishTime":null,"checked":null,"isEditorial":false}},"articlesIds":{},"isLoading":false,"pagesCount":{},"route":{},"reasonsList":null,"view":"cards","lastVisitedRoute":{},"ssrCommentsArticleIds":[""],"karma":{}},"authorContribution":{"authors":{}},"betaTest":{"currentAnnouncement":null,"announcements":{},"announcementCards":null,"announcementComments":{},"announcementCommentThreads":{},"announcementCommentingStatuses":{},"archivedList":[]},"authorStatistics":{"articleRefs":{},"articleIds":{},"pagesCount":{},"route":{},"viewsCount":[],"maxStatsCount":{}},"career":{"seoLandings":[],"hubs":""},"comments":{"articleComments":{},"searchCommentsResults":null,"previewComment":null,"pagesCount":null,"commentAccess":{},"scrollParents":{},"pageArticleComments":{"lastViewedComment":0,"postId":null,"lastCommentTimestamp":"","moderated":[],"moderatedIds":[],"commentRoute":""}},"companies":{"companyRefs":{"lamoda":{"alias":"lamoda","imageUrl":"\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fcompany\u002Fcdd\u002F710\u002F61a\u002Fcdd71061ae61f0c4f273ddcaaec533c7.png","titleHtml":"Lamoda","descriptionHtml":"Code the lifestyle","relatedData":null,"statistics":{"postsCount":71,"newsCount":0,"vacanciesCount":7,"employeesCount":49,"careerRating":4.58,"subscribersCount":2072,"rating":119.87,"invest":null},"foundationDate":{"year":"2011","month":"03","day":"05"},"location":{"city":{"id":"447159","title":"Москва"},"region":{"id":"1885","title":"Москва и Московская обл."},"country":{"id":"168","title":"Россия"}},"siteUrl":"http:\u002F\u002Ftech.lamoda.ru","staffNumber":"5 001–10 000 человек","registrationDate":"2018-09-20T09:17:42+00:00","representativeUser":null,"contacts":[{"title":"Сайт","url":"http:\u002F\u002Ftech.lamoda.ru"},{"title":"Сайт","url":"http:\u002F\u002Flamoda.ru"}],"settings":{"analyticsSettings":[{"type":"ym","trackingId":"56240926"}],"branding":null,"status":"active"},"metadata":{"titleHtml":"Lamoda, Москва - Code the lifestyle с 5 марта 2011 г.","title":"Lamoda, Москва - Code the lifestyle с 5 марта 2011 г.","keywords":["Управление разработкой","Управление проектами","Big Data","IT-инфраструктура","Go"],"descriptionHtml":"71 статья от авторов компании Lamoda","description":"71 статья от авторов компании Lamoda"},"aDeskSettings":null,"careerAlias":"latech","maxCustomTrackerLinks":0}},"companyIds":{},"companyTopIds":{},"pagesCount":{},"companyProfiles":{},"companiesCategories":[],"companiesCategoriesTotalCount":0,"companiesWidgets":{},"companiesWorkers":{},"companiesFans":{},"route":{},"isLoading":false,"companyWorkersLoading":false,"companyFansLoading":false,"vacancies":{}},"companiesContribution":{"hubs":{},"flows":{},"companyRefs":{}},"companyHubsContribution":{"contributionRefs":{"hubRefs":{},"hubIds":{}}},"conversation":{"messages":[],"respondent":null,"isLoadMore":false},"conversations":{"conversations":[],"unreadCount":0,"pagesCount":0,"isLoadMore":false},"desktopState":{"desktopFl":null,"desktopHl":null,"isChecked":false,"isLoginDemanded":false},"dfp":{"slotsDict":{}},"docs":{"menu":{},"articles":{},"mainMenu":[],"loading":{"main":false,"dropdown":false,"article":false}},"feature":{"isProbablyVisible":"true"},"flows":{"flows":[{"alias":"develop","id":1,"route":{"name":"FLOW_PAGE","params":{"flowName":"develop"}}},{"alias":"admin","id":6,"route":{"name":"FLOW_PAGE","params":{"flowName":"admin"}}},{"alias":"design","id":2,"route":{"name":"FLOW_PAGE","params":{"flowName":"design"}}},{"alias":"management","id":3,"route":{"name":"FLOW_PAGE","params":{"flowName":"management"}}},{"alias":"marketing","id":4,"route":{"name":"FLOW_PAGE","params":{"flowName":"marketing"}}},{"alias":"popsci","id":7,"route":{"name":"FLOW_PAGE","params":{"flowName":"popsci"}}}]},"global":{"isPwa":false,"device":"desktop","isHabrCom":true},"hubs":{"hubRefs":{},"hubIds":{},"pagesCount":{},"isLoading":false,"route":{}},"hubsBlock":{"hubRefs":{},"hubIds":{}},"i18n":{"fl":"ru","hl":"ru"},"info":{"infoPage":{},"isLoading":true},"location":{"urlStruct":{"protocol":null,"slashes":null,"auth":null,"host":null,"port":null,"hostname":null,"hash":null,"search":null,"query":{},"pathname":null,"path":null,"href":""},"searchQuery":null},"me":{"user":null,"ppgDemanded":false,"karmaResetInfo":{"canReincarnate":null,"wasReincarnated":null,"currentScore":null},"notes":null},"mostReadingList":{"mostReadingListIds":[],"mostReadingListRefs":null,"promoPost":null},"pinnedPost":{"pinnedPost":null},"ppa":{"articles":{},"card":null,"transactions":null,"totalTransactions":null,"isAccessible":null},"projectsBlocks":{"activeBlocks":{}},"pullRefresh":{"shouldRefresh":false},"sandbox":{"articleIds":[],"articleRefs":{},"pagesCount":null,"route":{},"lastVisitedRoute":{},"isLoading":false},"settingsOther":{"inputs":{"uiLang":{"errors":[],"ref":null,"value":""},"articlesLangEnglish":{"errors":[],"ref":null,"value":false},"articlesLangRussian":{"errors":[],"ref":null,"value":false},"agreement":{"errors":[],"ref":null,"value":false},"email":{"errors":[],"ref":null,"value":true},"digest":{"errors":[],"ref":null,"value":true}}},"similarList":{"similarListIds":[],"similarListRefs":null},"ssr":{"error":null,"isDataLoaded":false,"isDataLoading":false,"isHydrationFailed":false,"isServer":false},"userHubsContribution":{"contributionRefs":{"hubRefs":{},"hubIds":{}}},"userInvites":{"availableInvites":0,"usedInvitesIds":[],"usedInvitesRefs":{},"usedInvitesPagesCount":0,"unusedInvitesIds":[],"unusedInvitesRefs":{},"unusedInvitesPagesCount":0},"users":{"authorRefs":{},"authorIds":{},"pagesCount":{},"authorProfiles":{},"userHubs":{},"userInvitations":{},"authorFollowers":{},"authorFollowed":{},"karmaStats":[],"statistics":null,"isLoading":false,"authorFollowersLoading":false,"authorFollowedLoading":false,"userHubsLoading":false,"userInvitationsLoading":false,"route":{}},"viewport":{"prevScrollY":{},"scrollY":0,"width":0},"tracker":{"items":{},"pagesCache":{},"markedViewedSilently":{},"markedRead":{},"unreadCounters":{"applications":null,"system":null,"mentions":null,"subscribers":null,"posts_and_comments":null},"unviewedCounters":{"applications":null,"system":null,"mentions":null,"subscribers":null,"posts_and_comments":null}}};(function(){var s;(s=document.currentScript||document.scripts[document.scripts.length-1]).parentNode.removeChild(s);}());</script>
<script src="https://assets.habr.com/habr-web/js/chunk-vendors.c2c3fc9a.js" defer></script><script src="https://assets.habr.com/habr-web/js/app.c0af73e7.js" defer></script>



    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    </script>
  
  <script type="text/javascript" >
    (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
    m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
    (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

    ym(24049213, "init", {
      defer:true,
      trackLinks:true,
      accurateTrackBounce:true,
      webvisor:false,
    });
  </script>
  <noscript>
    <div>
      <img src="https://mc.yandex.ru/watch/24049213" style="position:absolute; left:-9999px;" alt="" />
    </div>
  </noscript>
  
    <script type="text/javascript">
      window.addEventListener('load', function () {
        setTimeout(() => {
          const img = new Image();
          img.src = 'https://vk.com/rtrg?p=VK-RTRG-421343-57vKE';
        }, 0);
      });
    </script>
  
</body>
</html>
